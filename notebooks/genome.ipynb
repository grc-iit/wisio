{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f28a99d-17b4-418f-9369-3b381455ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.9/site-packages/dask_jobqueue/core.py:251: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n",
      "/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.9/site-packages/distributed/node.py:183: UserWarning: Port 8788 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 36973 instead\n",
      "  warnings.warn(\n",
      "/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.9/site-packages/dask_jobqueue/core.py:251: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%run init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21796ca6-ccb6-471c-8410-a1caef730151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import functools\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import psutil\n",
    "import socket\n",
    "from anytree import PostOrderIter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dask.distributed import Client, LocalCluster, get_task_stream, progress, wait\n",
    "from distributed.diagnostics.plugin import SchedulerPlugin\n",
    "from dask_jobqueue import LSFCluster\n",
    "from time import perf_counter, sleep\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada62f1c-470e-4452-96cf-396945e92c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vani.utils.logger import ElapsedTimeLogger, create_logger, format_log\n",
    "# filter_group_index='tmid'\n",
    "# wait_until_workers_alive(filter_group_index, vn.n_workers_per_node / 2)\n",
    "# #wait_workers = vn.clients[filter_group_index].submit()\n",
    "# #wait_workers.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af43a5cc-6496-4337-a00b-d3c39bc614b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(future):\n",
    "    dask_graph = future.__dask_graph__()\n",
    "    edges = []\n",
    "    edge_count = {}\n",
    "    for key, value in dask_graph.dependencies.items():\n",
    "        vals = list(value)\n",
    "        destination = key.split(\"-\")[0]\n",
    "        for val in vals:\n",
    "            source = val.split(\"-\")[0]\n",
    "            str_edge = str(source) + \"-\" + str(destination)\n",
    "            if str_edge not in edge_count:\n",
    "                edge_count[str_edge] = 0\n",
    "            edge_count[str_edge] = edge_count[str_edge] + 1\n",
    "            edges.append((source, destination))\n",
    "    #print(edges[:5])\n",
    "    import networkx as nx\n",
    "    dag = nx.DiGraph()\n",
    "    dag.add_edges_from(edges)\n",
    "    import graphviz\n",
    "    dot = graphviz.Digraph()\n",
    "    for node in dag.nodes:\n",
    "        dot.node(node)\n",
    "        #print(node)\n",
    "    for edge_str, count in edge_count.items():\n",
    "        source, destination  = edge_str.split(\"-\")\n",
    "        dot.edge(source, destination, label=str(count))\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7ce75b-fcf8-475b-8f5a-1a4c5dba9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = '-'\n",
    "def merge(x, y):\n",
    "    import numpy as np\n",
    "    return {\n",
    "        'read': {\n",
    "            'uniq_ranks': np.union1d(x['read']['uniq_ranks'],y['read']['uniq_ranks']),\n",
    "            'agg_dur': x['read']['agg_dur'] + y['read']['agg_dur'],\n",
    "            'total_io_size': x['read']['total_io_size'] + y['read']['total_io_size'],\n",
    "            'uniq_filenames': np.union1d(x['read']['uniq_filenames'],y['read']['uniq_filenames']),\n",
    "            'bw_sum': x['read']['bw_sum'] + y['read']['bw_sum'],\n",
    "            'ops': x['read']['ops'] + y['read']['ops'],\n",
    "        },\n",
    "        'write': {\n",
    "            'uniq_ranks': np.union1d(x['write']['uniq_ranks'],y['write']['uniq_ranks']),\n",
    "            'agg_dur': x['write']['agg_dur'] + y['write']['agg_dur'],\n",
    "            'total_io_size': x['write']['total_io_size'] + y['write']['total_io_size'],\n",
    "            'uniq_filenames': np.union1d(x['write']['uniq_filenames'],y['write']['uniq_filenames']),\n",
    "            'bw_sum': x['write']['bw_sum'] + y['write']['bw_sum'],\n",
    "            'ops': x['write']['ops'] + y['write']['ops'],\n",
    "        },\n",
    "        'metadata': {\n",
    "            'uniq_ranks': np.union1d(x['metadata']['uniq_ranks'],y['metadata']['uniq_ranks']),\n",
    "            'agg_dur': x['metadata']['agg_dur'] + y['metadata']['agg_dur'],\n",
    "            'uniq_filenames': np.union1d(x['metadata']['uniq_filenames'],y['metadata']['uniq_filenames']),\n",
    "            'ops': x['metadata']['ops'] + y['metadata']['ops'],\n",
    "        }\n",
    "    }\n",
    "\n",
    "def filter(target_ddf, filter_group_index: str):\n",
    "    import numpy as np\n",
    "    #target_ddf = ddf.loc[start:stop]\n",
    "    # Select dataframes\n",
    "    #result = wait(target_ddf)\n",
    "    import intervals\n",
    "    empty = {\n",
    "            'uniq_ranks': [],\n",
    "            'agg_dur': 0.0,\n",
    "            'total_io_size': 0,\n",
    "            'uniq_filenames': [],\n",
    "            'bw_sum': 0.0,\n",
    "            'ops': 0,\n",
    "        }\n",
    "    if len(target_ddf.index) == 0:\n",
    "        return {\n",
    "        'read': empty,\n",
    "        'write': empty,\n",
    "        'metadata': empty\n",
    "    }\n",
    "    #target_ddf = target_ddf.compute()\n",
    "    import pandas as pd\n",
    "    def f(x):\n",
    "        d = {}\n",
    "        d['duration'] = x['duration'].sum()\n",
    "        d['size'] = x['size'].sum()\n",
    "        d['bandwidth'] = x['bandwidth'].sum()\n",
    "        d['index'] = x['index'].count()\n",
    "        d['proc_id'] = x['proc_id'].nunique()\n",
    "        d['filename'] = x['filename'].min()\n",
    "        return pd.Series(d, index=['duration', 'size', 'bandwidth', 'index', 'proc_id', 'filename'])\n",
    "    \n",
    "    aggregated_values = target_ddf.groupby(['file_id','io_cat']).apply(f)#.compute()\n",
    "    \n",
    "    # Clear dataframes\n",
    "    #del read_ddf\n",
    "    del target_ddf\n",
    "    return aggregated_values.compute()\n",
    "    # Arrange results\n",
    "    #read_start, read_end = 0, len(read_tasks)\n",
    "    #write_start, write_end = len(read_tasks), len(read_tasks) + len(write_tasks)\n",
    "    #metadata_start, metadata_end = len(read_tasks) + len(write_tasks), 0\n",
    "    index_values = aggregated_values.index.unique()\n",
    "    read_values = empty\n",
    "    write_values = empty\n",
    "    metadata_values = empty\n",
    "    if 1 in index_values:\n",
    "        read_values = {\n",
    "            'uniq_ranks': aggregated_values.loc[1]['proc_id'],\n",
    "            'agg_dur': aggregated_values.loc[1]['duration'],\n",
    "            'total_io_size': aggregated_values.loc[1]['size'],\n",
    "            'uniq_filenames': aggregated_values.loc[1]['file_id'],\n",
    "            'bw_sum': aggregated_values.loc[1]['bandwidth'],\n",
    "            'ops': aggregated_values.loc[1]['index'],\n",
    "        }\n",
    "    if 2 in index_values:\n",
    "        write_values = {\n",
    "            'uniq_ranks': aggregated_values.loc[2]['proc_id'],\n",
    "            'agg_dur': aggregated_values.loc[2]['duration'],\n",
    "            'total_io_size': aggregated_values.loc[2]['size'],\n",
    "            'uniq_filenames': aggregated_values.loc[2]['file_id'],\n",
    "            'bw_sum': aggregated_values.loc[2]['bandwidth'],\n",
    "            'ops': aggregated_values.loc[2]['index'],\n",
    "        }\n",
    "    if 3 in index_values:\n",
    "        metadata_values = {\n",
    "            'uniq_ranks': aggregated_values.loc[3]['proc_id'],\n",
    "            'agg_dur': aggregated_values.loc[3]['duration'],\n",
    "            'total_io_size': aggregated_values.loc[3]['size'],\n",
    "            'uniq_filenames': aggregated_values.loc[3]['file_id'],\n",
    "            'bw_sum': aggregated_values.loc[3]['bandwidth'],\n",
    "            'ops': aggregated_values.loc[3]['index'],\n",
    "        }\n",
    "    filter_result = {\n",
    "        'read': read_values,\n",
    "        'write': write_values,\n",
    "        'metadata': metadata_values\n",
    "    }\n",
    "    # Return results\n",
    "    return filter_result\n",
    "def cal_len(x):\n",
    "    return {\n",
    "        'read': {\n",
    "            'uniq_ranks': len(x['read']['uniq_ranks']),\n",
    "            'agg_dur': x['read']['agg_dur'],\n",
    "            'total_io_size': x['read']['total_io_size'],\n",
    "            'uniq_filenames': len(x['read']['uniq_filenames']),\n",
    "            'bw_sum': x['read']['bw_sum'],\n",
    "            'ops': x['read']['ops'],\n",
    "        },\n",
    "        'write': {\n",
    "            'uniq_ranks': len(x['write']['uniq_ranks']),\n",
    "            'agg_dur': x['write']['agg_dur'],\n",
    "            'total_io_size': x['write']['total_io_size'],\n",
    "            'uniq_filenames': len(x['write']['uniq_filenames']),\n",
    "            'bw_sum': x['write']['bw_sum'],\n",
    "            'ops': x['write']['ops'],\n",
    "        },\n",
    "        'metadata': {\n",
    "            'uniq_ranks': len(x['metadata']['uniq_ranks']),\n",
    "            'agg_dur': x['metadata']['agg_dur'],\n",
    "            'uniq_filenames': len(x['metadata']['uniq_filenames']),\n",
    "            'ops': x['metadata']['ops']\n",
    "        }\n",
    "    }\n",
    "\n",
    "from dask.graph_manipulation import bind\n",
    "def compute_min_max(log_dir: str, filter_group_index: str, depth):\n",
    "    with open(f\"{log_dir}/global.json\") as file:\n",
    "        global_metrics = json.load(file)\n",
    "        min_val, max_val = global_metrics[filter_group_index][0], global_metrics[filter_group_index][1]\n",
    "        next_tasks = 2 ** depth\n",
    "        interval = math.ceil((max_val - min_val) * 1.0 / next_tasks)\n",
    "        time_range = range(min_val, max_val, interval)\n",
    "        return interval, time_range\n",
    "\n",
    "def splice_ddf(ddf, file_id):\n",
    "    return ddf.loc[[file_id]].reset_index().compute()\n",
    "def splice_ddf_list(ddf, list_vals):\n",
    "    start = 0\n",
    "    stop = 0\n",
    "    if len(list_vals) > 0:\n",
    "        start = list_vals[0]\n",
    "    if len(list_vals) > 1:\n",
    "        stop = list_vals[-1] - 1\n",
    "    return ddf.loc[start:stop].reset_index()#.compute()\n",
    "def filter_map(ddf, file_id, filter_group_index):\n",
    "    #print(index,\"depth\", depth, end = '\\r')\n",
    "    target_ddf = delayed_func(splice_ddf, f\"splice_ddf{delimiter}{file_id}\", [ddf, file_id])\n",
    "    return delayed_func(filter, f\"filter{delimiter}{file_id}\", [target_ddf, filter_group_index])\n",
    "def merge_map(index, depth,task_1, task_2):\n",
    "    #print(index,\"depth\", depth, end = '\\r')\n",
    "    return delayed_func(merge, f\"merge{delimiter}{depth}_{task_1[0]}_{task_2[0]}\", [task_1[1], task_2[1]])\n",
    "def compute_metrics(file_id_list, ddf, filter_group_index: str, wait_persist_delayed):\n",
    "    num_nodes = len(file_id_list) * 2 - 1\n",
    "    MAX_DEPTH = math.ceil(math.log(len(file_id_list),2))\n",
    "    less_pieces = False\n",
    "    print(len(file_id_list), MAX_DEPTH, less_pieces)\n",
    "    iterations = list(range(0, MAX_DEPTH + 1))\n",
    "    iterations.reverse()\n",
    "    all_tasks = [0] * (MAX_DEPTH + 1)\n",
    "    all_intervals = [0] * (MAX_DEPTH + 1)\n",
    "    from tqdm.notebook import trange, tqdm\n",
    "    nodes_done = 0\n",
    "    NUM_THREADS = 128\n",
    "    executor = concurrent.futures.ThreadPoolExecutor(NUM_THREADS)\n",
    "    for i in tqdm(iterations):\n",
    "        print(f\"\\ndepth {i}\")\n",
    "        tasks = []\n",
    "        current_intervals = []\n",
    "        if i == MAX_DEPTH:\n",
    "            tasks = [0]*len(file_id_list)\n",
    "            print(f\"submitting {i}\")\n",
    "            index_futures = {executor.submit(filter_map, i,index, ddf, file_id, filter_group_index): index for index,file_id in enumerate(file_id_list)}\n",
    "            print(f\"waiting {i}\")\n",
    "            for future in concurrent.futures.as_completed(index_futures):\n",
    "                index = index_futures[future]\n",
    "                print(f\"{index} depth {i}           \", end = '\\r')\n",
    "                tasks[index] = future.result()\n",
    "            print(f\"done {i}\")\n",
    "            #futures = [executor.submit(filter_map, index, ddf, file_id, filter_group_index) for index,file_id in enumerate(file_id_list)]\n",
    "            #concurrent.futures.wait(futures)\n",
    "            #for future in futures:\n",
    "            #    tasks.append(future.result())\n",
    "            #for file_id in file_id_list:\n",
    "            #    print(nodes_done,\" of \", num_nodes, \" \" ,i, end = '\\r')\n",
    "            #    target_ddf = delayed_func(splice_ddf, f\"splice_ddf{delimiter}{file_id}\", [ddf, file_id])\n",
    "            #    tasks.append(delayed_func(filter, f\"filter{delimiter}{file_id}\", [target_ddf, filter_group_index]))\n",
    "            #    nodes_done = nodes_done +1\n",
    "            #    index = index +1\n",
    "                #current_intervals.append((start, stop))\n",
    "        else:\n",
    "            next_tasks = len(all_tasks[i + 1])\n",
    "            num_tasks = math.floor(next_tasks /2 )\n",
    "            if next_tasks % 2 == 1:\n",
    "                next_tasks = next_tasks - 1#3\n",
    "                num_tasks = num_tasks + 1\n",
    "            tasks = [0]*num_tasks\n",
    "            index_futures = {executor.submit(merge_map,index, i,[node_id, all_tasks[i + 1][node_id]],[node_id+1, all_tasks[i + 1][node_id+1]]):index for index,node_id in enumerate(range(0, next_tasks, 2))}\n",
    "            for future in concurrent.futures.as_completed(index_futures):\n",
    "                index = index_futures[future]\n",
    "                print(f\"{index} depth {i}           \", end = '\\r')\n",
    "                tasks[index] = future.result()\n",
    "            if next_tasks % 2 == 1:\n",
    "                print(f\"{next_tasks/2} depth {i}           \", end = '\\r')\n",
    "                tasks.append(all_tasks[i + 1][next_tasks-1])\n",
    "            \n",
    "            index_futures = {executor.submit(delayed_func,cal_len, f\"cal_len{delimiter}{i+1}_{t}\", [next_tasks]):t for t, next_tasks in enumerate(all_tasks[i + 1])}\n",
    "            for future in concurrent.futures.as_completed(index_futures):\n",
    "                index = index_futures[future]\n",
    "                print(f\"{index} depth {i}           \", end = '\\r')\n",
    "                all_tasks[i + 1][index] = future.result()\n",
    "#             executor = concurrent.futures.ThreadPoolExecutor(32)\n",
    "#             futures = [executor.submit(delayed_func,cal_len, f\"cal_len{delimiter}{i+1}_{t}\", [next_tasks]) for t, next_tasks in enumerate(all_tasks[i + 1])]\n",
    "#             concurrent.futures.wait(futures)\n",
    "#             for t,future in enumerate(futures):\n",
    "#                 all_tasks[i + 1][t] = future.result()\n",
    "                #tasks.append(future.result())\n",
    "            #next_tasks = len(all_tasks[i + 1])\n",
    "            #if next_tasks % 2 == 1:\n",
    "            #    next_tasks = next_tasks - 1#3\n",
    "            #index = 0\n",
    "            #if next_tasks > 1:\n",
    "            #    for t in range(0, next_tasks, 2):\n",
    "            #        print(nodes_done,\" of \", num_nodes, \" \" ,i, end = '\\r')\n",
    "            #        #start, stop = all_intervals[i+1][t][0], all_intervals[i+1][t+1][1]\n",
    "            #        #target_ddf = delayed_func(splice_ddf, f\"splice_ddf{delimiter}{start}_{stop}\", [ddf, start, stop])\n",
    "            #        #tasks.append(delayed_func(filter, f\"filter{delimiter}{start}_{stop}\", [target_ddf, filter_group_index]))\n",
    "            #        #current_intervals.append((start, stop))\n",
    "            #        tasks.append(delayed_func(merge, f\"merge{delimiter}{i}_{t}_{t+1}\", [all_tasks[i + 1][t], all_tasks[i + 1][t + 1]]))\n",
    "            #        nodes_done = nodes_done +1\n",
    "            #        index = index +1\n",
    "            #next_tasks = len(all_tasks[i + 1])\n",
    "            #if next_tasks % 2 == 1:\n",
    "            #    print(nodes_done,\" of \", num_nodes, \" \" ,i, end = '\\r')\n",
    "            #    tasks.append(all_tasks[i + 1][next_tasks-1])\n",
    "            #    nodes_done = nodes_done +1\n",
    "            #    index = index +1\n",
    "                #start, stop = all_intervals[i+1][next_tasks - 3][0], all_intervals[i+1][next_tasks - 1][1]\n",
    "                #target_ddf = delayed_func(splice_ddf, f\"splice_ddf{delimiter}{start}_{stop}\", [ddf, start, stop])\n",
    "                #tasks.append(delayed_func(filter, f\"filter{delimiter}{start}_{stop}\", [target_ddf, filter_group_index]))\n",
    "                #current_intervals.append((start, stop))\n",
    "            # TODO why are we calling len on everything?\n",
    "            #for t, next_tasks in enumerate(all_tasks[i + 1]):\n",
    "            #    all_tasks[i + 1][t] = delayed_func(cal_len, f\"cal_len{delimiter}{i+1}_{t}\", [next_tasks])\n",
    "        all_tasks[i] = tasks\n",
    "        #all_intervals[i] = current_intervals\n",
    "    if len(all_tasks[0]) == 2:\n",
    "        all_tasks[0] = [delayed_func(merge, f\"merge{delimiter}{0}_{0}_{1}\", [all_tasks[0][0], all_tasks[0][1]])]\n",
    "        #all_tasks[0] = [delayed_func(cal_len,f\"cal_len{delimiter}0_{t}\",all_tasks[0][0])]\n",
    "    for t, next_tasks in enumerate(all_tasks[0]):\n",
    "        all_tasks[0][t] = delayed_func(cal_len,f\"cal_len{delimiter}0_{t}\",[next_tasks])\n",
    "    #metrics = dask.compute(all_tasks)\n",
    "    return all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e383bd8-a1b3-4027-a41d-d57eb8677dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.9/site-packages/dask_jobqueue/core.py:251: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "total_workers = n_workers_per_node*2\n",
    "cluster.scale(total_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ee5b93-a7bd-4413-9d21-749e668703e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "CPU times: user 5.96 s, sys: 336 ms, total: 6.3 s\n",
      "Wall time: 6.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import dask\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def delayed_func(func, name, args, nout=1):\n",
    "    #if name:\n",
    "    #    func.__name__ = name\n",
    "    obj = dask.delayed(func, nout=nout)(*args, dask_key_name=name)\n",
    "    return obj\n",
    "def wait_delayed(ddf):\n",
    "    result = wait(ddf)\n",
    "    return 1\n",
    "def dummy(x):\n",
    "    return x\n",
    "import numpy as np\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "def group_filter_map(ddf, file_id_list, filter_group_index):\n",
    "    delayed_list = []\n",
    "    for file_id in file_id_list:\n",
    "        delayed_list.append(filter_map(ddf, file_id, filter_group_index))\n",
    "    return delayed_list\n",
    "def cal_range_filenames(file_id_list):\n",
    "    #     #file_id_list = df.index.unique()\n",
    "    #     mask = mask =  (2**32 -1) << 32\n",
    "    #     dir1_ids = set()\n",
    "    #     for file in file_id_list:\n",
    "    #         dir1_ids.add(file & mask)\n",
    "    s = list(file_id_list)\n",
    "    s.sort()\n",
    "    with open(f\"{log_dir}/file.json\", \"w\") as file:\n",
    "        json.dump(s, file, cls=NpEncoder)\n",
    "    return s\n",
    "#def transform(ddf):\n",
    "#    import dask.array as da\n",
    "#     mask =  (2**32 -1) << 32\n",
    "#     ddf['dir_1'] = ddf.index.apply(lambda val, mask: val & mask, axis=1, meta=ddf, args=(mask))\n",
    "#     mask =  (2**16 -1) << 48\n",
    "#     ddf['dir_2'] = ddf.index.apply(lambda val, mask: val & mask, axis=1, meta=ddf, args=(mask))\n",
    "#     mask =  (2**8 -1) << 56\n",
    "#     ddf['dir_3'] = ddf.index.apply(lambda val, mask: val & mask, axis=1, meta=ddf, args=(mask))\n",
    "#     return ddf\n",
    "import numpy as np\n",
    "MAX_DEPTH = 10\n",
    "log_dir = \"/p/gpfs1/iopp/recorder_app_logs/genome_pegasus/nodes-32/_parquet\"\n",
    "filter_group_index='file_id'\n",
    "delayed_list =[]\n",
    "PARTITION_FOLDER=\"partitoned\"\n",
    "is_partitioned = os.path.exists(f\"{log_dir}/{PARTITION_FOLDER}/{filter_group_index}/_common_metadata\")\n",
    "json_exists = os.path.exists(f\"{log_dir}/file.json\")\n",
    "if not is_partitioned:\n",
    "    interval, time_range = compute_min_max(log_dir, filter_group_index, MAX_DEPTH)\n",
    "    read_delayed = delayed_func(lambda log_dir: dd.read_parquet(f\"{log_dir}/*.parquet\", index=False), f\"read_raw_parquet_{filter_group_index}\", [log_dir])\n",
    "    delayed_list.append(read_delayed)\n",
    "    index_delayed = delayed_func(lambda filter_group_index, ddf: ddf.set_index([filter_group_index]), f\"index_delayed_{filter_group_index}\", [filter_group_index, read_delayed])\n",
    "    delayed_list.append(index_delayed)\n",
    "    persist_delayed = delayed_func(lambda ddf: ddf.persist(), f\"persist_{filter_group_index}\" , [index_delayed])\n",
    "    delayed_list.append(persist_delayed)\n",
    "    #transform_delayed = delayed_func(transform, f\"transform_{filter_group_index}\" , [persist_delayed])\n",
    "    #delayed_list.append(transform_delayed)\n",
    "    wait_persist_delayed = delayed_func(wait_delayed, f\"wait_delayed_{filter_group_index}\", [persist_delayed])\n",
    "    delayed_list.append(wait_persist_delayed)\n",
    "    partition_delayed = delayed_func(lambda ddf, ignore: ddf.repartition(partition_size=\"128MB\"), f\"partitioned_{filter_group_index}\", [persist_delayed, wait_persist_delayed])\n",
    "    delayed_list.append(partition_delayed)\n",
    "    save_parquet_delayed = delayed_func(lambda ddf, filter_group_index, ignore: dd.to_parquet(ddf, f\"{log_dir}/{PARTITION_FOLDER}/{filter_group_index}\"), f\"save_{filter_group_index}\", [partition_delayed, filter_group_index,wait_persist_delayed])\n",
    "    delayed_list.append(save_parquet_delayed)\n",
    "else:\n",
    "    read_delayed = delayed_func(lambda log_dir: dd.read_parquet(f\"{log_dir}/{PARTITION_FOLDER}/{filter_group_index}/*.parquet\", calculate_divisions=True, index=[filter_group_index]), f\"read_indexed_parquet_{filter_group_index}\", [log_dir])\n",
    "    delayed_list.append(read_delayed)\n",
    "    persist_delayed = delayed_func(lambda ddf: ddf.persist(), f\"persist_{filter_group_index}\" , [read_delayed])\n",
    "    delayed_list.append(persist_delayed)\n",
    "    wait_persist_delayed = delayed_func(wait_delayed, f\"wait_delayed_{filter_group_index}\", [persist_delayed])\n",
    "    delayed_list.append(wait_persist_delayed)\n",
    "    \n",
    "    pass\n",
    "    if not json_exists:\n",
    "        unique_filenames = delayed_func(lambda ddf, ignore: ddf.index.unique().compute(), f\"groupby_{filter_group_index}\" , [persist_delayed, wait_persist_delayed])\n",
    "        delayed_list.append(unique_filenames)\n",
    "        dir1_ids = delayed_func(cal_range_filenames, f\"dir1_ids_{filter_group_index}\", [unique_filenames])\n",
    "        delayed_list.append(dir1_ids)\n",
    "    else:\n",
    "        with open(f\"{log_dir}/file.json\") as file:\n",
    "            files = json.load(file)\n",
    "        \n",
    "#         print(len(files))\n",
    "# #         dir1_mask =  (2**32 -1) << (64 - 32)\n",
    "# #         dir2_mask =  (2**16 -1) << (64 - 16)\n",
    "# #         dir3_mask =  (2**8 -1) << (64 - 8)\n",
    "# #         dir1_ids = set()\n",
    "# #         dir2_ids = set()\n",
    "# #         dir3_ids = set()\n",
    "# #         for file in files:\n",
    "# #             dir1_ids.add(file & dir1_mask)\n",
    "# #             dir2_ids.add(file & dir2_mask)\n",
    "# #             dir3_ids.add(file & dir3_mask)\n",
    "#         num_files_per_partition = 2**12\n",
    "#         file_range = range(0, len(files),num_files_per_partition)\n",
    "#         partition_index= 0\n",
    "#         num_partitions = len(file_range)\n",
    "#         partitions_delayed = [0]*num_partitions\n",
    "#         NUM_THREADS = 128\n",
    "#         def compute_metrics_list(index, file_id_list, ddf, filter_group_index):\n",
    "#             target_ddf = ddf.loc[file_id_list].reset_index()\n",
    "#             #target_ddf = delayed_func(splice_ddf, f\"splice_ddf{delimiter}{file_id}\", [persist_delayed, file_id])\n",
    "#             return delayed_func(filter, f\"filter{delimiter}{index}\", [target_ddf, filter_group_index])\n",
    "        \n",
    "# #         executor = concurrent.futures.ThreadPoolExecutor(NUM_THREADS)\n",
    "# #         index_futures = {executor.submit(group_filter_map, persist_delayed, files[index:index+num_files_per_partition-1], filter_group_index): i for i,index in enumerate(file_range)}\n",
    "# #         print(f\"waiting\")\n",
    "        \n",
    "# #         for future in concurrent.futures.as_completed(index_futures):\n",
    "# #             index = index_futures[future]\n",
    "# #             print(f\"partition {partition_index} of {num_partitions} done          \", end = '\\r')\n",
    "# #             partitions_delayed[index] = dask.delayed(dummy)( future.result(),dask_key_name=f\"partition_{partition_index}_{filter_group_index}\")\n",
    "# #             partition_index = partition_index + 1\n",
    "# #         print(f\"done\")\n",
    "# #         delayed_list.extend(partitions_delayed)\n",
    "        \n",
    "#         #executor = concurrent.futures.ThreadPoolExecutor(NUM_THREADS)\n",
    "#         print(\"Sumitting Futures\")\n",
    "#         index_futures = [client.submit(compute_metrics_list, i, files[index:index+num_files_per_partition-1], persist_delayed, filter_group_index) for i,index in enumerate(file_range)]\n",
    "# #         for future in as_completed(index_futures):\n",
    "# #             index = index_futures[future]\n",
    "# #             partitions_delayed[index] = dask.delayed(dummy)(future.result(),dask_key_name=f\"partition_{partition_index}_{filter_group_index}\")\n",
    "# #             partition_index = partition_index + 1\n",
    "# #             print(f\"Finished partition {partition_index} of {len(file_range)}           \", end='\\r')\n",
    "# #         delayed_list.extend(partitions_delayed)\n",
    "#         # for index in file_range:\n",
    "#         #    file_id_list = files[index:index+num_files_per_partition-1]\n",
    "#         #    tasks = []\n",
    "#         #    for file_id in file_id_list:\n",
    "#         #        target_ddf = delayed_func(splice_ddf, f\"splice_ddf{delimiter}{file_id}\", [persist_delayed, file_id])\n",
    "#         #        tasks.append(delayed_func(filter, f\"filter{delimiter}{file_id}\", [target_ddf, filter_group_index]))\n",
    "#         #    delayed_list.append(dask.delayed(dummy)(tasks,dask_key_name=f\"partition_{partition_index}_{filter_group_index}\"))\n",
    "#         #    partition_index = partition_index + 1\n",
    "#         #    print(f\"Finished partition {partition_index} of {len(file_range)}           \", end='\\r')\n",
    "            \n",
    "            \n",
    "#         print(\"Create sequence\") # took 10 mins\n",
    "#         file_sequence = [[persist_delayed, file_id] for file_id in files]\n",
    "#         num_files_per_partition = 2**12\n",
    "#         paritions = math.ceil(len(files)/num_files_per_partition)\n",
    "#         import dask.bag as db\n",
    "#         print(f\"Create bag {num_files_per_partition} {paritions}\")\n",
    "#         file_bag = db.from_sequence(file_sequence, partition_size=1).map(get_file_metrics)\n",
    "#         print(\"Done bag\")\n",
    "        #dir1_sequence = [(ddf, dir1_id) for dir1_id in dir1_ids]\n",
    "        #delayed_list.extend(file_bag.to_delayed())\n",
    "        \n",
    "        #         import dask.bag as db\n",
    "        #         interval = 2**10\n",
    "        #         b = db.from_sequence(range(0, len(files[:interval]), interval))\n",
    "        #         b = b.map(lambda x: compute_metrics(files[x, x + interval - 1], persist_delayed, filter_group_index, wait_persist_delayed))\n",
    "        #         delayed_list.extend(b)\n",
    "        #compute_metrics_delayed = compute_metrics(files,persist_delayed, filter_group_index, wait_persist_delayed)\n",
    "        #print(len(compute_metrics_delayed))\n",
    "        #iter_com = list(enumerate(compute_metrics_delayed))\n",
    "        #iter_com.reverse()\n",
    "        #for index, compute_metric_item in iter_com:\n",
    "        #    delayed_list.append(dask.delayed(dummy)(compute_metric_item,dask_key_name=f\"depth_{index}_{filter_group_index}\"))\n",
    "        \n",
    "        #delayed_list.append(compute_metrics_delayed)\n",
    "#compute_metrics_delayed.reverse()\n",
    "#for index, compute_metric_item in enumerate(compute_metrics_delayed):\n",
    "#    delayed_list.append(dask.delayed(dummy)(compute_metric_item,dask_key_name=f\"depth_{len(compute_metrics_delayed) - index - 1}_{filter_group_index}\"))\n",
    "print(len(delayed_list))\n",
    "#compute_metrics_delayed = delayed_func(compute_metrics, \"compute_metrics\", [wait_persist_delayed, filter_group_index, compute_min_max_delayed])\n",
    "total = dask.delayed(delayed_list)(dask_key_name=f\"{filter_group_index}_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7edbb1d9-ee3c-4468-9374-c7070601c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0 of 64\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.9/site-packages/distributed/worker.py:2938: UserWarning: Large object of size 3.17 MiB detected in task graph: \n",
      "  ('persist_file_id', 0, [-9223371986035283781, -922 ... layed_file_id')\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 38s, sys: 4.11 s, total: 5min 42s\n",
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_files_per_partition = math.ceil(len(files)*1.0/total_workers)\n",
    "file_range = range(0, len(files),num_files_per_partition)\n",
    "def cal_metrics_file(ddf, index, file_lists, log_dir, ignore):\n",
    "    filename = f\"{log_dir}/metrics/file_id/{index}.parquet\"\n",
    "    splice_ddf = ddf.loc[file_lists]\n",
    "    splice_ddf.reset_index()\n",
    "    target_ddf = splice_ddf.compute()\n",
    "    aggregate = target_ddf.groupby(['index','io_cat']).agg({'duration':sum, \n",
    "                                                              'size':sum, \n",
    "                                                              'bandwidth':sum, \n",
    "                                                              'index':'count', \n",
    "                                                              'proc_id':[min,max], \n",
    "                                                              'filename':min})\n",
    "    aggregate.reset_index(inplace=True)\n",
    "    aggregate.columns  = ['_'.join(col) for col in aggregate.columns.values]\n",
    "    aggregate.to_parquet(filename)\n",
    "    return filename\n",
    "futures = []\n",
    "#\n",
    "for index, file_index in enumerate(file_range):\n",
    "    print(f\"processing {index} of {len(file_range)}\", end='\\r')\n",
    "    seleceted_files = files[file_index:file_index+num_files_per_partition]\n",
    "    #seleceted_files_future = client.scatter(seleceted_files) doesnt help as list of futures is larger as list of file ids :D\n",
    "    #print(len(seleceted_files), index)\n",
    "    cal_metrics= dask.delayed(cal_metrics_file)(persist_delayed, index, seleceted_files,log_dir, wait_persist_delayed, dask_key_name=f\"cal_metrics_{index}_{filter_group_index}\")\n",
    "    futures.append(client.compute(cal_metrics, sync=False))\n",
    "    #cal_metrics.append(delayed_func(cal_metrics,  , []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e6913b-176f-4ad9-8d5a-d2edaf6c25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#futures = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886df918-0535-4234-93d5-f97f8b71ac79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: finished, type: str, key: cal_metrics_0_file_id>,\n",
       " <Future: finished, type: str, key: cal_metrics_1_file_id>,\n",
       " <Future: finished, type: str, key: cal_metrics_2_file_id>,\n",
       " <Future: finished, type: str, key: cal_metrics_3_file_id>,\n",
       " <Future: finished, type: str, key: cal_metrics_4_file_id>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futures[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66339fb6-0e25-4789-93ed-095ba9cc4803",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 63 of 64 in 0.00028105179468790696\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.9/site-packages/distributed/client.py:4968\u001b[0m, in \u001b[0;36mas_completed.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[1;32m   4967\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_condition:\n\u001b[0;32m-> 4968\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthread_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_and_raise()\n",
      "File \u001b[0;32m/usr/WS2/iopp/software/spack/opt/spack/linux-rhel7-power9le/gcc-8.3.1/python-3.9.6-rd2wz4o2deeo6ddb2thcwgniciy5chre/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "from dask.distributed import as_completed\n",
    "start_time = time.time()\n",
    "metrics = []\n",
    "for future in as_completed(futures):\n",
    "    end_time = time.time() - start_time\n",
    "    #filename = future.result()\n",
    "    metrics.append(future)\n",
    "    print(f\"Completed {len(metrics)} of {len(futures)} in {end_time/60}\", end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2054968-9617-457f-80b0-2e371feaf7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2caafd5-d72c-4aac-b182-db1f04241e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/p/gpfs1/iopp/recorder_app_logs/genome_pegasus/nodes-32/_parquet/metrics/file_id/63.parquet'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91330b-7ef5-4a54-8414-906f84943c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in futures:\n",
    "    if future.status in \"error\":\n",
    "        print(f\"printing exception {future.key}\")\n",
    "        print(future.exception())\n",
    "        future.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f483d-5452-4b11-8820-88ec304268db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Progress bar on a single-machine scheduler\n",
    "from dask.distributed import performance_report\n",
    "from dask.distributed import as_completed\n",
    "futures = client.compute(delayed_list, sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865cac9-2684-4a6d-a314-08aab889b88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import pandas as pd\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# def chunk(s):\n",
    "#     '''\n",
    "#     The function applied to the\n",
    "#     individual partition (map)\n",
    "#     '''    \n",
    "#     return s.apply(lambda x: list(set(x)))\n",
    "\n",
    "\n",
    "# def agg(s):\n",
    "#     '''\n",
    "#     The function whic will aggrgate \n",
    "#     the result from all the partitions(reduce)\n",
    "#     '''\n",
    "#     s = s._selected_obj    \n",
    "#     return s.groupby(level=list(range(s.index.nlevels))).sum()\n",
    "\n",
    "\n",
    "# def finalize(s):\n",
    "#     '''\n",
    "#     The optional functional that will be \n",
    "#     applied to the result of the agg_tu functions\n",
    "#     '''\n",
    "#     return s.apply(lambda x: len(set(x)))\n",
    "\n",
    "\n",
    "# tunique = dd.Aggregation('tunique', chunk, agg,finalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b411a7-e90b-4fa0-b888-086cff3e03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7e933-db63-4aa2-beef-b6c13c498bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_delayed = delayed_func(lambda ddf, ignore: ddf.reset_index().groupby(['file_id','io_cat']).agg({'duration':sum, \n",
    "                                                                       'size':sum, \n",
    "                                                                       'bandwidth':sum, \n",
    "                                                                       'index':'count', \n",
    "                                                                       'proc_id':[min,max], \n",
    "                                                                       'filename':min}), f\"calculate_metrics_{filter_group_index}\", [persist_delayed, wait_persist_delayed])\n",
    "delayed_list.append(file_delayed)\n",
    "\n",
    "save_metrics_delayed = delayed_func(lambda ddf, filter_group_index, ignore: dd.to_parquet(ddf.from_delayed(), f\"{log_dir}/metric/{filter_group_index}\"), f\"save_metric_{filter_group_index}\", [file_delayed, filter_group_index,wait_persist_delayed])\n",
    "delayed_list.append(save_metrics_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a429790-71d9-49f6-a07e-1fda2fd29877",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36688b46-8648-4824-8edb-ade75db87293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46830998-9909-4824-a495-01e601e5899d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e251be-1730-45fa-b2d3-4c97b62812e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ecab3-44c3-466e-ae50-74cdbab3cf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics = val.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde5b66-079a-469c-b458-eeaa4904f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e17f9-6f55-4eea-892b-cc29df3a1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"genome_metrics_all.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f01b9-0fb1-4ebc-8ac3-ae66a5b001c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "val = wait(index_futures, return_when=\"ALL_COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e0667-8041-48e5-bf48-274e9385f514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for future in val.done:\n",
    "    if future.status in [\"error\"]:\n",
    "        print(f\"printing exception {future.key}\")\n",
    "        print(future.exception())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6d37d-a68c-4d19-96cc-c4dd5d4a88c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#delayed_part_futures = [client.compute(future.result(), sync=False) for future in val.done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ed766-852c-426b-8447-f4bc1c872086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_partition_future(future):\n",
    "    import dask\n",
    "    return dask.compute(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b70f77-d111-4480-a471-1d9ad8ab8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = [client.submit(submit_partition_future, future) for future in val.done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e24b49-d434-4494-a7fc-4b41052aa625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import as_completed\n",
    "import time\n",
    "partition_index = 0\n",
    "metrics = [0]*total\n",
    "start_time = time.time()\n",
    "for f in as_completed(fs):\n",
    "    metrics[partition_index] = f.result()\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"finished {partition_index} of {total} {end_time/60} \", end='\\r')\n",
    "    partition_index = partition_index + 1\n",
    "    with open(\"genome_metrics_all.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd1c4a-9a1e-4e5b-a852-ff5897166449",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab27d3c-f947-4f96-b30b-1e4d9b75f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1789f09-c2c4-411f-8040-b7574b513826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "total = len(val.done)\n",
    "metrics = [0]*total\n",
    "start_time = time.time()\n",
    "future_list =list(val.done)\n",
    "from dask.distributed import as_completed\n",
    "parition_count = 1\n",
    "parition_range = range(partition_index, total, parition_count)\n",
    "for future_index in parition_range:\n",
    "    my_futures = future_list[future_index:future_index + parition_count]\n",
    "    fs = [client.submit(submit_partition_future, future) for future in my_futures]\n",
    "    for f in as_completed(fs):\n",
    "        end_time = time.time() - start_time\n",
    "        #f = client.submit(submit_partition_future, future)\n",
    "        metrics[partition_index] = f.result()\n",
    "        print(f\"finished {partition_index} of {total} {end_time/60} {len(metrics[partition_index])}\", end='\\r')\n",
    "        partition_index = partition_index + 1\n",
    "        with open(\"genome_metrics_all.json\", \"w\") as f:\n",
    "            json.dump(metrics, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b38d88-ebdd-44e2-a527-b90ecec4d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632b747-c469-414d-a46c-cb1f53b5de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in future_list:\n",
    "    future.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efd9c4-a1ed-45a3-98fe-249611e13a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"genome_metrics_all.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c404aa-8cde-4396-bdb3-b7f42b4b19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from dask.distributed import as_completed\n",
    "start_time = time.time()\n",
    "finished = 0\n",
    "total = len(delayed_part_futures)\n",
    "for future in as_completed(delayed_part_futures):\n",
    "    finished = finished + 1\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"finished {finished} of {total} {end_time/60}\", end='\\r')\n",
    "print(\"\\ndone\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340d836-823d-4a11-a5d7-1452e296179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#delayed_part_futures = await delayed_part_futures_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c099a-4432-4bcd-87f7-d42c39ac6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delayed_part_futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2342a-1011-49a2-ad41-3782017aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_partition_delayed  = delayed_part_futures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c62aee-19f0-4dce-8f60-a81853e5befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first_partition_delayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b8df9-7f52-46a2-ba0d-564d5079ec96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(delayed_part_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f91ccb-fbf4-4642-bdbe-2f0df8734a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_futures = client.compute(delayed_part_futures, sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d95a5-c3ff-4193-a680-a269f23d7cb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(partition_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe5ff7-2c6b-4350-b5cc-c1cd88468bc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partition_futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c3531-0544-47e9-b59e-f1a150f5b721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from dask.distributed import as_completed\n",
    "start_time = time.time()\n",
    "finished = 0\n",
    "total = len(partition_futures)\n",
    "for future in as_completed(partition_futures):\n",
    "    finished = finished + 1\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"finished {finished} of {total} {end_time/60}\", end='\\r')\n",
    "print(\"\\ndone\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae13c80-b29e-4bf1-9907-d62f72057d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for future in futures:\n",
    "    if future.status in [\"finished\"]:\n",
    "        metrics[future.key.split(\"-\")[1]] = future.result()\n",
    "#file_bag.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e7eac9-04e8-4902-85fc-ed579541a19d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2ffb2-7f12-4d80-a9d1-497a8ccaf8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"genome_metrics_4096.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f786d4-940a-41bc-9c0f-31359674b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_metrics_delayed = file_bag.to_delayed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227c3a2-9840-41f8-b799-6a4a5f70d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(delayed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cefc2-be99-43d3-9525-ccdca846d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_1024 = file_bag.take(2048, compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0a7fc-94e2-49b6-936c-5f3b6c7d1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.compute(file_bag, sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c37328-a407-4ca8-ae38-b2f0e648c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = bag_1024.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d60cd-d6f8-4a5a-a4c6-a555d064efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a47205-6a60-4b9c-b5e3-e0cff29a383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156daa8d-a431-4b6e-b753-ce0dc4a0087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = draw_graph(delayed_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7882abb-0851-4406-98e1-0e61dcda8de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362fa888-31b5-4176-90a2-f258f69c6b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = delayed_list[:(3+1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e9d55-1032-4ff0-b8c5-452ff49607f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Progress bar on a single-machine scheduler\n",
    "from dask.distributed import performance_report\n",
    "from dask.distributed import as_completed\n",
    "futures = client.compute(selected, sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd51323-a2c3-4d2a-ac2b-8dc4797303a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb3a62f-07e6-4718-963f-97695a6b66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1180e2-d714-49ba-a2c9-c655b8835158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "from dask.distributed import as_completed\n",
    "start_time = time.time()\n",
    "for future in as_completed(futures):\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"{future.key} {future.status} {end_time/60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ab6c6-af2d-4d72-a33b-05ed897fe459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for future in futures:\n",
    "    if future.status in [\"error\"]:\n",
    "        print(f\"printing exception {future.key}\")\n",
    "        print(future.exception())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac9d58-9ee5-4507-be54-f2a689ef6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in futures:\n",
    "    print(f\"{future.key} {future.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b58c99f-5800-482b-8e50-ea3702f2f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in futures:\n",
    "    if future.status not in [\"finished\"]:\n",
    "        print(f\"canceling {future.key}\")\n",
    "        future.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad61d10-da4c-472c-b4f1-b0a053761ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dirs = futures[4].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718cd3f-17fc-48d9-bb8c-4b1ce8844380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423338c-9935-4df0-a5cd-dcea473a62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics[0][44][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3862e-0860-4ea1-aabf-38ba8cfa9449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for i in futures[4:]:\n",
    "    metrics.append(i.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119923d2-a474-4992-a6f4-45144b806243",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb85d8-9d5b-46ea-8fc1-28dd7e2e2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b26d9-065e-41d6-9ff7-c2f4b9e17f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7c0e5c-04c7-4aa2-a4c8-c9a51b62f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"genome_metrics_full.json\", \"w\") as f:\n",
    "    json.dump(metrics[0], f, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e8f7e-a19c-499e-8c7b-aff62fe69589",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Progress bar on a single-machine scheduler\n",
    "from dask.diagnostics import ProgressBar\n",
    "futures_2 = client.compute(delayed_list[5:], sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec7fd0-6fe8-4abb-a671-00a715316ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf07812-961e-4f82-9a46-6e023141d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures_2[-1:][0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0f330-3349-42e2-b07d-add0e0efad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in futures:\n",
    "    if \"calculate\" in future.key:\n",
    "        print(f\"canceling {future.key}\")\n",
    "        future.cancel()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ad846f-0fc3-48d3-9f59-a3dcea830d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in futures:\n",
    "    future.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956b25d-e447-4cf5-96fc-cfc53079bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for future in futures:\n",
    "    if future.status  in [\"error\"]:\n",
    "        print(f\"canceling {future.key}\")\n",
    "        future.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41a04f-5739-4d9d-9e0d-cc99826ecec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if futures[4].done():\n",
    "    metrics = futures[4].result()\n",
    "else:\n",
    "    print(\"future is pending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a291c6-ffdd-4134-b474-366b8c4a215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053b7cf-4ffd-4a1a-a567-62e5b281c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "PARTITION_FOLDER = \"partitioned_hari\"\n",
    "def indexed_ddf(log_dir: str, filter_group_index: str):\n",
    "    ddf = dd.read_parquet(f\"{log_dir}/*.parquet\", index=False)\n",
    "    #ddf[filter_group_index] = ddf[filter_group_index].astype(np.int64)\n",
    "    ddf = ddf.set_index([filter_group_index])\n",
    "    ddf = ddf.persist()\n",
    "    #result = wait(ddf)\n",
    "    return ddf\n",
    "\n",
    "def compute_metrics(ddf, filter_group_index: str, interval):\n",
    "    min_val, max_val = interval\n",
    "    \n",
    "\n",
    "    depth = 10\n",
    "    next_tasks = 2 ** depth\n",
    "    interval = math.ceil((max_val - min_val) * 1.0 / next_tasks)\n",
    "    iterations = list(range(0, depth + 1))\n",
    "    iterations.reverse()\n",
    "    all_tasks = [0] * (depth + 1)\n",
    "    time_range = range(min_val, max_val, interval)\n",
    "    for i in iterations:\n",
    "        tasks = []\n",
    "        if i == depth:\n",
    "            for start in time_range:\n",
    "                stop = start + interval - 1\n",
    "                target_ddf = ddf.loc[start:stop]\n",
    "                tasks.append(dask.delayed(filter)(target_ddf, filter_group_index))\n",
    "        else:\n",
    "            next_tasks = len(all_tasks[i + 1])\n",
    "            if next_tasks % 2 == 1:\n",
    "                next_tasks = next_tasks - 1\n",
    "            for t in range(0, next_tasks, 2):\n",
    "                tasks.append(dask.delayed(merge)(all_tasks[i + 1][t], all_tasks[i + 1][t + 1]))\n",
    "            next_tasks = len(all_tasks[i + 1])\n",
    "            if next_tasks % 2 == 1:\n",
    "                tasks.append(all_tasks[i + 1][next_tasks - 1])\n",
    "            # TODO why are we calling len on everything?\n",
    "            for t, next_tasks in enumerate(all_tasks[i + 1]):\n",
    "                all_tasks[i + 1][t] = dask.delayed(cal_len)(next_tasks)\n",
    "        all_tasks[i] = tasks\n",
    "    for t, next_tasks in enumerate(all_tasks[0]):\n",
    "        all_tasks[0][t] = dask.delayed(cal_len)(next_tasks)\n",
    "    metrics = dask.compute(all_tasks)    \n",
    "    #result = wait(metrics_futures)\n",
    "    #metrics = client.gather(metrics_futures)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d003126-e7b2-4f00-9e91-c2c785e81735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_index = 'tmid'\n",
    "# futures = []\n",
    "# args = [log_dir, json_index, filter_group_index]\n",
    "# interval_ft = vn.clients[filter_group_index].submit(compute_min_max, *args, key=f'{filter_group_index}_compute_min_max')\n",
    "# futures.append(interval_ft)\n",
    "# args = [log_dir, filter_group_index]\n",
    "# indexed_ft = vn.clients[filter_group_index].submit(indexed_ddf, *args, key=f'{filter_group_index}_indexed_ddf')\n",
    "# futures.append(indexed_ft)\n",
    "# args = [indexed_ft, filter_group_index, interval_ft]\n",
    "# metrics_ft = vn.clients[filter_group_index].submit(compute_metrics, *args, key=f'{filter_group_index}_compute_metrics')\n",
    "# futures.append(metrics_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4287737e-0c1a-4a36-b300-9c8102338faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import as_completed\n",
    "# from tqdm.auto import tqdm\n",
    "# metrics_map = {}\n",
    "# for future in tqdm(as_completed(futures)):\n",
    "#     if \"compute_metrics\" in future.key:\n",
    "#         metrics_map[future.key] = future.result(timeout=(2*60*60))\n",
    "#     print(f'{future.key} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67575d-295b-4c1c-b1b9-4adfd94f0edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580447c9-0593-4bed-94e0-68fd4b779c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_keys = dict(tmid='tmid', proc_id='proc_id', file_id='file_id')\n",
    "futures = []\n",
    "for filter_group_index in vn.filter_group_indices:\n",
    "    json_index = json_keys[filter_group_index]\n",
    "    args = [log_dir, json_index, filter_group_index]\n",
    "    interval_ft = vn.clients[filter_group_index].submit(compute_min_max, *args, key=f'{filter_group_index}_compute_min_max')\n",
    "    futures.append(interval_ft)\n",
    "    args = [log_dir, filter_group_index]\n",
    "    indexed_ft = vn.clients[filter_group_index].submit(indexed_ddf, *args, key=f'{filter_group_index}_indexed_ddf')\n",
    "    futures.append(indexed_ft)\n",
    "    args = [indexed_ft, filter_group_index, interval_ft]\n",
    "    metrics_ft = vn.clients[filter_group_index].submit(compute_metrics, *args, key=f'{filter_group_index}_compute_metrics')\n",
    "    futures.append(metrics_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9ecbc-4a95-430d-96aa-7482cd428cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from dask.distributed import as_completed\n",
    "from tqdm.auto import tqdm\n",
    "metrics_map = {}\n",
    "for future in tqdm(as_completed(futures)):\n",
    "    if \"compute_metrics\" in future.key:\n",
    "        metrics_map[future.key] = future.result(timeout=(2*60*60))\n",
    "    print(f'{future.key} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5d0c7-694d-4d72-a8a9-c32482186efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_val, max_val = metrics_map[\"tmid_compute_min_max\"]\n",
    "# depth = 10\n",
    "# next_tasks = 2 ** depth\n",
    "# interval = math.ceil((max_val - min_val) * 1.0 / next_tasks)\n",
    "# time_range = range(min_val, max_val, interval)\n",
    "# for start in time_range:\n",
    "#     print(start, start + interval - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e891a2-f7ab-4463-8a08-641cd5d3cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_result(x, y):\n",
    "    return {\n",
    "        'read': {\n",
    "            'uniq_ranks': x['read']['uniq_ranks'] + y['read']['uniq_ranks'],\n",
    "            'agg_dur': x['read']['agg_dur'] + y['read']['agg_dur'],\n",
    "            'total_io_size': x['read']['total_io_size'] + y['read']['total_io_size'],\n",
    "            'uniq_filenames': x['read']['uniq_filenames'] + y['read']['uniq_filenames'],\n",
    "            'bw_sum': x['read']['bw_sum'] + y['read']['bw_sum'],\n",
    "            'ops': x['read']['ops'] + y['read']['ops'],\n",
    "        },\n",
    "        'write': {\n",
    "            'uniq_ranks': x['write']['uniq_ranks'] + y['write']['uniq_ranks'],\n",
    "            'agg_dur': x['write']['agg_dur'] + y['write']['agg_dur'],\n",
    "            'total_io_size': x['write']['total_io_size'] + y['write']['total_io_size'],\n",
    "            'uniq_filenames': x['write']['uniq_filenames'] + y['write']['uniq_filenames'],\n",
    "            'bw_sum': x['write']['bw_sum'] + y['write']['bw_sum'],\n",
    "            'ops': x['write']['ops'] + y['write']['ops'],\n",
    "        },\n",
    "        'metadata': {\n",
    "            'uniq_ranks': x['metadata']['uniq_ranks'] + y['metadata']['uniq_ranks'],\n",
    "            'agg_dur': x['metadata']['agg_dur'] + y['metadata']['agg_dur'],\n",
    "            'uniq_filenames': x['metadata']['uniq_filenames'] + y['metadata']['uniq_filenames'],\n",
    "            'ops': x['metadata']['ops'] + y['metadata']['ops'],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459637a-4a6b-4564-81fd-7c38569d5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['tmid_compute_metrics'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157a600-18a1-4247-9d4c-3bd2f1d6cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_result(metrics_map['proc_id_compute_metrics'][0][0][0],metrics_map['proc_id_compute_metrics'][0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad400d-8cdd-4dd3-a8ef-12175353311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['file_id_compute_metrics'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9e5a6-6ee9-4211-9898-f854f9d352b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa6121-a22e-4aed-b3aa-c7c1a781f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['file_id_indexed_ddf'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487d8b5-bb34-44b0-8a1c-6b5ac7f00921",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['tmid_compute_metrics'][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696688ed-27f3-4afd-a41b-ae7d6e013734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_map['proc_id_compute_metrics'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b0de2-9580-4d4b-8ba8-7b273e877ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['file_id_compute_metrics'][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ccfd6-e953-4b2e-9f80-8c0ddc928f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "local_client = Client(LocalCluster(n_workers=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80679d0b-c47b-4bcc-b0c4-a94e4f0d4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449aaae-13fb-4815-afdf-ea2bbfc6d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/p/gpfs1/iopp/recorder_app_logs/genome_pegasus/nodes-32/_parquet\"\n",
    "ddf = dd.read_parquet(f\"{log_dir}/*.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad1157-b3c5-4ef6-b43c-0695c34671ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dd.compute(ddf['tmid'].min(), ddf['tmid'].max(), ddf['file_id'].min(), ddf['file_id'].max(), ddf['proc_id'].min(), ddf['proc_id'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b67fc7-4055-4f00-bb34-396a5ff6cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c919e1-9a92-4ecb-af93-a2885fbddc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val, max_val = -9223371986035283781, 9223369538921024184\n",
    "\n",
    "next_tasks = 2 ** 10\n",
    "interval = math.ceil((max_val - min_val) * 1.0 / next_tasks)\n",
    "time_range = range(min_val, max_val, interval)\n",
    "print(time_range)\n",
    "for start in time_range:\n",
    "    print(start, start + interval -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3173a-d505-4624-931c-0a654afe9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efadee-a7e7-4fda-acb9-0bef0ae16138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digio",
   "language": "python",
   "name": "digio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
