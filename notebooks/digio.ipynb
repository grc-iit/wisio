{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bc6ef2-c509-4ef8-baec-c35bf6b61288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e619c48-bf08-42ce-92ff-e41e69fa713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vani.analyzer import Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb8ece5-4b75-4cb8-819b-32527d8bc590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at: tcp://192.168.66.200:43033\n",
      "distributed.scheduler - INFO -   dashboard at:       192.168.66.200:3446\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:40219'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:43275'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:33887'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:34147'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:37685'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:39451'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:36449'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:36691'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:43817'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:43543'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:45825'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:36359'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:33403'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:39753'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:40183'\n",
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.66.200:44189'\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-w4o52tyv', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-t3jfybw8', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-mw549yx4', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-dyw40tth', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-q5urrdx9', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-tjd86b92', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-wkbuczbe', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-d5umuumc', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-uzt2qeei', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-7pyhmazp', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-rh0198nb', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-060h0tpn', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-52q6rxxj', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-0koc6_o0', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-wikzkj7e', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/var/tmp/dask/local/dask-worker-space/worker-jqc25zef', purging\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:35549\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:35549\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:33425\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-_msuxi1s\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:45415\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:45415\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:34191\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-a3yp5_37\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:41313\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:41313\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:36375\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-dnghywsy\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:40897\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:40897\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:41527\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-bxxusldz\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:39269\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:39269\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:42291\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-ny7fh7h8\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:37843\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:37843\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:37115\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-4gbv71cc\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:41529\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:41529\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:38157\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-ms12_iy9\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:41317\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:41317\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:40319\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-a0f2rahv\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:37051\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:37051\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:34891\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-14sbe60o\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:37083\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:37083\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:32859\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-ezfhzyyj\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:40741\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:40741\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:35277\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-xojjk79s\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:34261\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:34261\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:33217\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-bpy_y3b_\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:35637\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:35637\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:33959\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-hsjbabhp\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:41723\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:41723\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:39151\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-5hprm7rc\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:42161\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:42161\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:42791\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-x72f9kv5\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.66.200:40753\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.66.200:40753\n",
      "distributed.worker - INFO -          dashboard at:       192.168.66.200:40893\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          8\n",
      "distributed.worker - INFO -       Local Directory: /var/tmp/dask/local/dask-worker-space/worker-u9srvasc\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:35549', name: 8, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:35549\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:45415', name: 4, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:45415\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:41529', name: 14, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:41529\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:40897', name: 10, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:40897\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:41313', name: 6, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:41313\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:37843', name: 13, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:37843\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:39269', name: 15, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:39269\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:37051', name: 9, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:37051\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:34261', name: 7, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:34261\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:37083', name: 2, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:37083\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:41723', name: 0, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:41723\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:41317', name: 12, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:41317\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:40741', name: 3, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:40741\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:40753', name: 1, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:40753\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:35637', name: 11, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:35637\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.66.200:42161', name: 5, status: undefined, memory: 0, processing: 0>\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.66.200:42161\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.66.200:43033\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard link: http://192.168.66.200:3446/status\n",
      "Dashboard link: http://192.168.66.200:3447/status\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#BSUB -J dask-worker\n",
      "#BSUB -G asccasc\n",
      "#BSUB -q pdebug\n",
      "#BSUB -W 120\n",
      "\n",
      "/usr/workspace/iopp/projects/digio-venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.66.200:38957 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name dummy-name --nanny --death-timeout 7200 --local-directory /var/tmp/dask/tmid\n",
      "\n",
      "Dashboard link: http://192.168.66.200:3448/status\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#BSUB -J dask-worker\n",
      "#BSUB -G asccasc\n",
      "#BSUB -q pdebug\n",
      "#BSUB -W 120\n",
      "\n",
      "/usr/workspace/iopp/projects/digio-venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.66.200:40695 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name dummy-name --nanny --death-timeout 7200 --local-directory /var/tmp/dask/proc_id\n",
      "\n",
      "Dashboard link: http://192.168.66.200:3449/status\n",
      "#!/usr/bin/env bash\n",
      "\n",
      "#BSUB -J dask-worker\n",
      "#BSUB -G asccasc\n",
      "#BSUB -q pdebug\n",
      "#BSUB -W 120\n",
      "\n",
      "/usr/workspace/iopp/projects/digio-venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.66.200:34425 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name dummy-name --nanny --death-timeout 7200 --local-directory /var/tmp/dask/file_id\n",
      "\n",
      "<Client: 'tcp://192.168.66.200:43033' processes=16 threads=128>\n",
      "<Client: 'tcp://192.168.66.200:38957' processes=0 threads=0, memory=0 B>\n",
      "<Client: 'tcp://192.168.66.200:40695' processes=0 threads=0, memory=0 B>\n",
      "<Client: 'tcp://192.168.66.200:34425' processes=0 threads=0, memory=0 B>\n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzer\n",
    "cluster_settings = dict(\n",
    "    dashboard_port=3446,\n",
    "    local_directory=\"/var/tmp/dask\",\n",
    "    log_file=\"digio.worker.log\"\n",
    ")\n",
    "vn = Analyzer(debug=True, cluster_settings=cluster_settings)\n",
    "\n",
    "# Analysis configuration\n",
    "log_dir = \"/p/gpfs1/iopp/recorder_app_logs/genome_pegasus/nodes-32/_parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6569cc59-2172-4770-865f-c5ac740b81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis = vn.analyze_parquet_logs(log_dir, depth=10, persist_stats=True, stats_file_prefix=\"cm1_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7541e2c4-f702-46bc-87a3-fc73fed7f372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#files = vn.ensure_logs_partitioned(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a044bd-a320-4b82-8909-c74cd70de7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_until_workers_alive(filter_group_index: str, n_workers: int = None):\n",
    "    # Get current number of workers\n",
    "    current_n_workers = len(vn.clients[filter_group_index].scheduler_info()['workers'])\n",
    "    expected_n_workers = int(vn.n_workers_per_node if n_workers is None else n_workers)\n",
    "    # Wait until enough number of workers alive\n",
    "    while vn.clients[filter_group_index].status == 'running' and current_n_workers < expected_n_workers:\n",
    "        # Log status\n",
    "        vn.logger.debug(\n",
    "            format_log(filter_group_index, f\"{current_n_workers}/{expected_n_workers} workers running\"))\n",
    "        # Ensure loop\n",
    "        vn.ensure_asyncio_loop()\n",
    "        # Try correcting state\n",
    "        # noinspection PyProtectedMember\n",
    "        vn.clusters[filter_group_index]._correct_state()\n",
    "        # Sleep a little\n",
    "        sleep(1)\n",
    "        # Get current number of workers\n",
    "        current_n_workers = len(vn.clients[filter_group_index].scheduler_info()['workers'])\n",
    "    # Print result\n",
    "    vn.logger.debug(format_log(filter_group_index, \"All workers alive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21796ca6-ccb6-471c-8410-a1caef730151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import functools\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import psutil\n",
    "import socket\n",
    "from anytree import PostOrderIter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dask.distributed import Client, LocalCluster, get_task_stream, progress, wait\n",
    "from distributed.diagnostics.plugin import SchedulerPlugin\n",
    "from dask_jobqueue import LSFCluster\n",
    "from time import perf_counter, sleep\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Union\n",
    "def compute_min_max(log_dir: str, json_key: str, filter_group_index: str):\n",
    "    with open(f\"{log_dir}/global.json\") as file:\n",
    "        global_metrics = json.load(file)\n",
    "        min_val, max_val = global_metrics[json_key][0], global_metrics[json_key][1]\n",
    "        return min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada62f1c-470e-4452-96cf-396945e92c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vani.utils.logger import ElapsedTimeLogger, create_logger, format_log\n",
    "filter_group_index='tmid'\n",
    "wait_until_workers_alive(filter_group_index, vn.n_workers_per_node / 2)\n",
    "#wait_workers = vn.clients[filter_group_index].submit()\n",
    "#wait_workers.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee5b93-a7bd-4413-9d21-749e668703e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4053b7cf-4ffd-4a1a-a567-62e5b281c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "PARTITION_FOLDER = \"partitioned_hari\"\n",
    "def indexed_ddf(log_dir: str, filter_group_index: str):\n",
    "    ddf = dd.read_parquet(f\"{log_dir}/*.parquet\", index=False)\n",
    "    #ddf[filter_group_index] = ddf[filter_group_index].astype(np.int64)\n",
    "    ddf = ddf.set_index([filter_group_index])\n",
    "    ddf = ddf.persist()\n",
    "    #result = wait(ddf)\n",
    "    return ddf\n",
    "def merge(x, y):\n",
    "    return {\n",
    "        'read': {\n",
    "            'uniq_ranks': np.union1d(x['read']['uniq_ranks'], y['read']['uniq_ranks']),\n",
    "            'agg_dur': x['read']['agg_dur'] + y['read']['agg_dur'],\n",
    "            'total_io_size': x['read']['total_io_size'] + y['read']['total_io_size'],\n",
    "            'uniq_filenames': np.union1d(x['read']['uniq_filenames'], y['read']['uniq_filenames']),\n",
    "            'bw_sum': x['read']['bw_sum'] + y['read']['bw_sum'],\n",
    "            'ops': x['read']['ops'] + y['read']['ops'],\n",
    "        },\n",
    "        'write': {\n",
    "            'uniq_ranks': np.union1d(x['write']['uniq_ranks'], y['write']['uniq_ranks']),\n",
    "            'agg_dur': x['write']['agg_dur'] + y['write']['agg_dur'],\n",
    "            'total_io_size': x['write']['total_io_size'] + y['write']['total_io_size'],\n",
    "            'uniq_filenames': np.union1d(x['write']['uniq_filenames'], y['write']['uniq_filenames']),\n",
    "            'bw_sum': x['write']['bw_sum'] + y['write']['bw_sum'],\n",
    "            'ops': x['write']['ops'] + y['write']['ops'],\n",
    "        },\n",
    "        'metadata': {\n",
    "            'uniq_ranks': np.union1d(x['metadata']['uniq_ranks'], y['metadata']['uniq_ranks']),\n",
    "            'agg_dur': x['metadata']['agg_dur'] + y['metadata']['agg_dur'],\n",
    "            'uniq_filenames': np.union1d(x['metadata']['uniq_filenames'], y['metadata']['uniq_filenames']),\n",
    "            'ops': x['metadata']['ops'] + y['metadata']['ops'],\n",
    "        }\n",
    "    }\n",
    "def compute_metrics(ddf, filter_group_index: str, interval):\n",
    "    min_val, max_val = interval\n",
    "    def filter(target_ddf, filter_group_index: str):\n",
    "        # Select dataframes\n",
    "        read_ddf = target_ddf[(target_ddf['io_cat'] == 1)]\n",
    "        write_ddf = target_ddf[(target_ddf['io_cat'] == 2)]\n",
    "        metadata_ddf = target_ddf[(target_ddf['io_cat'] == 3)]\n",
    "        # TODO: on same compute run for all ddf.\n",
    "        # Create tasks\n",
    "        read_tasks = [\n",
    "            read_ddf.index.unique() if filter_group_index == 'proc_id' else read_ddf['proc_id'].unique(),\n",
    "            read_ddf['duration'].sum(),\n",
    "            read_ddf['size'].sum(),\n",
    "            read_ddf.index.unique() if filter_group_index == 'file_id' else read_ddf['file_id'].unique(),\n",
    "            read_ddf['bandwidth'].sum(),\n",
    "            read_ddf['index'].count(),\n",
    "        ]\n",
    "        write_tasks = [\n",
    "            write_ddf.index.unique() if filter_group_index == 'proc_id' else write_ddf['proc_id'].unique(),\n",
    "            write_ddf['duration'].sum(),\n",
    "            write_ddf['size'].sum(),\n",
    "            write_ddf.index.unique() if filter_group_index == 'file_id' else write_ddf['file_id'].unique(),\n",
    "            write_ddf['bandwidth'].sum(),\n",
    "            write_ddf['index'].count(),\n",
    "        ]\n",
    "        metadata_tasks = [\n",
    "            metadata_ddf.index.unique() if filter_group_index == 'proc_id' else metadata_ddf['proc_id'].unique(),\n",
    "            metadata_ddf['duration'].sum(),\n",
    "            metadata_ddf.index.unique() if filter_group_index == 'file_id' else metadata_ddf['file_id'].unique(),\n",
    "            metadata_ddf['index'].count(),\n",
    "        ]\n",
    "        filter_tasks = []\n",
    "        filter_tasks.extend(read_tasks)\n",
    "        filter_tasks.extend(write_tasks)\n",
    "        filter_tasks.extend(metadata_tasks)\n",
    "        # Compute all\n",
    "        filter_results = dask.compute(*filter_tasks)\n",
    "        # Clear dataframes\n",
    "        del read_ddf\n",
    "        del write_ddf\n",
    "        del metadata_ddf\n",
    "        del target_ddf\n",
    "        # Arrange results\n",
    "        read_start, read_end = 0, len(read_tasks)\n",
    "        write_start, write_end = len(read_tasks), len(read_tasks) + len(write_tasks)\n",
    "        metadata_start, metadata_end = len(read_tasks) + len(write_tasks), 0\n",
    "        filter_result = {\n",
    "            'read': {\n",
    "                'uniq_ranks': filter_results[:read_end][0],\n",
    "                'agg_dur': filter_results[:read_end][1],\n",
    "                'total_io_size': filter_results[:read_end][2],\n",
    "                'uniq_filenames': filter_results[:read_end][3],\n",
    "                'bw_sum': filter_results[:read_end][4],\n",
    "                'ops': filter_results[:read_end][5],\n",
    "            },\n",
    "            'write': {\n",
    "                'uniq_ranks': filter_results[write_start:write_end][0],\n",
    "                'agg_dur': filter_results[write_start:write_end][1],\n",
    "                'total_io_size': filter_results[write_start:write_end][2],\n",
    "                'uniq_filenames': filter_results[write_start:write_end][3],\n",
    "                'bw_sum': filter_results[write_start:write_end][4],\n",
    "                'ops': filter_results[write_start:write_end][5],\n",
    "            },\n",
    "            'metadata': {\n",
    "                'uniq_ranks': filter_results[metadata_start:][0],\n",
    "                'agg_dur': filter_results[metadata_start:][1],\n",
    "                'uniq_filenames': filter_results[metadata_start:][2],\n",
    "                'ops': filter_results[metadata_start:][3],\n",
    "            }\n",
    "        }\n",
    "        # Return results\n",
    "        return filter_result\n",
    "\n",
    "    \n",
    "    def get_empty():\n",
    "        return {\n",
    "            'read': {\n",
    "                'uniq_ranks': [],\n",
    "                'agg_dur': 0.0,\n",
    "                'total_io_size': 0,\n",
    "                'uniq_filenames': [],\n",
    "                'bw_sum': 0.0,\n",
    "                'ops': 0,\n",
    "            },\n",
    "            'write': {\n",
    "                'uniq_ranks': [],\n",
    "                'agg_dur': 0.0,\n",
    "                'total_io_size': 0,\n",
    "                'uniq_filenames': [],\n",
    "                'bw_sum': 0.0,\n",
    "                'ops': 0,\n",
    "            },\n",
    "            'metadata': {\n",
    "                'uniq_ranks': [],\n",
    "                'agg_dur': 0.0,\n",
    "                'uniq_filenames': [],\n",
    "                'ops': 0,\n",
    "            }\n",
    "        }\n",
    "    def cal_len(x):\n",
    "        return {\n",
    "            'read': {\n",
    "                'uniq_ranks': len(x['read']['uniq_ranks']),\n",
    "                'agg_dur': x['read']['agg_dur'],\n",
    "                'total_io_size': x['read']['total_io_size'],\n",
    "                'uniq_filenames': len(x['read']['uniq_filenames']),\n",
    "                'bw_sum': x['read']['bw_sum'],\n",
    "                'ops': x['read']['ops'],\n",
    "            },\n",
    "            'write': {\n",
    "                'uniq_ranks': len(x['write']['uniq_ranks']),\n",
    "                'agg_dur': x['write']['agg_dur'],\n",
    "                'total_io_size': x['write']['total_io_size'],\n",
    "                'uniq_filenames': len(x['write']['uniq_filenames']),\n",
    "                'bw_sum': x['write']['bw_sum'],\n",
    "                'ops': x['write']['ops'],\n",
    "            },\n",
    "            'metadata': {\n",
    "                'uniq_ranks': len(x['metadata']['uniq_ranks']),\n",
    "                'agg_dur': x['metadata']['agg_dur'],\n",
    "                'uniq_filenames': len(x['metadata']['uniq_filenames']),\n",
    "                'ops': x['metadata']['ops']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    depth = 10\n",
    "    next_tasks = 2 ** depth\n",
    "    interval = math.ceil((max_val - min_val) * 1.0 / next_tasks)\n",
    "    iterations = list(range(0, depth + 1))\n",
    "    iterations.reverse()\n",
    "    all_tasks = [0] * (depth + 1)\n",
    "    time_range = range(min_val, max_val, interval)\n",
    "    for i in iterations:\n",
    "        tasks = []\n",
    "        if i == depth:\n",
    "            for start in time_range:\n",
    "                stop = start + interval - 1\n",
    "                target_ddf = ddf.loc[start:stop]\n",
    "                tasks.append(dask.delayed(filter)(target_ddf, filter_group_index))\n",
    "        else:\n",
    "            next_tasks = len(all_tasks[i + 1])\n",
    "            if next_tasks % 2 == 1:\n",
    "                next_tasks = next_tasks - 1\n",
    "            for t in range(0, next_tasks, 2):\n",
    "                tasks.append(dask.delayed(merge)(all_tasks[i + 1][t], all_tasks[i + 1][t + 1]))\n",
    "            next_tasks = len(all_tasks[i + 1])\n",
    "            if next_tasks % 2 == 1:\n",
    "                tasks.append(all_tasks[i + 1][next_tasks - 1])\n",
    "            # TODO why are we calling len on everything?\n",
    "            for t, next_tasks in enumerate(all_tasks[i + 1]):\n",
    "                all_tasks[i + 1][t] = dask.delayed(cal_len)(next_tasks)\n",
    "        all_tasks[i] = tasks\n",
    "    for t, next_tasks in enumerate(all_tasks[0]):\n",
    "        all_tasks[0][t] = dask.delayed(cal_len)(next_tasks)\n",
    "    metrics = dask.compute(all_tasks)    \n",
    "    #result = wait(metrics_futures)\n",
    "    #metrics = client.gather(metrics_futures)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d003126-e7b2-4f00-9e91-c2c785e81735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_index = 'tmid'\n",
    "# futures = []\n",
    "# args = [log_dir, json_index, filter_group_index]\n",
    "# interval_ft = vn.clients[filter_group_index].submit(compute_min_max, *args, key=f'{filter_group_index}_compute_min_max')\n",
    "# futures.append(interval_ft)\n",
    "# args = [log_dir, filter_group_index]\n",
    "# indexed_ft = vn.clients[filter_group_index].submit(indexed_ddf, *args, key=f'{filter_group_index}_indexed_ddf')\n",
    "# futures.append(indexed_ft)\n",
    "# args = [indexed_ft, filter_group_index, interval_ft]\n",
    "# metrics_ft = vn.clients[filter_group_index].submit(compute_metrics, *args, key=f'{filter_group_index}_compute_metrics')\n",
    "# futures.append(metrics_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4287737e-0c1a-4a36-b300-9c8102338faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import as_completed\n",
    "# from tqdm.auto import tqdm\n",
    "# metrics_map = {}\n",
    "# for future in tqdm(as_completed(futures)):\n",
    "#     if \"compute_metrics\" in future.key:\n",
    "#         metrics_map[future.key] = future.result(timeout=(2*60*60))\n",
    "#     print(f'{future.key} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef67575d-295b-4c1c-b1b9-4adfd94f0edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# metrics_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "580447c9-0593-4bed-94e0-68fd4b779c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_keys = dict(tmid='tmid', proc_id='proc_id', file_id='file_id')\n",
    "futures = []\n",
    "for filter_group_index in vn.filter_group_indices:\n",
    "    json_index = json_keys[filter_group_index]\n",
    "    args = [log_dir, json_index, filter_group_index]\n",
    "    interval_ft = vn.clients[filter_group_index].submit(compute_min_max, *args, key=f'{filter_group_index}_compute_min_max')\n",
    "    futures.append(interval_ft)\n",
    "    args = [log_dir, filter_group_index]\n",
    "    indexed_ft = vn.clients[filter_group_index].submit(indexed_ddf, *args, key=f'{filter_group_index}_indexed_ddf')\n",
    "    futures.append(indexed_ft)\n",
    "    args = [indexed_ft, filter_group_index, interval_ft]\n",
    "    metrics_ft = vn.clients[filter_group_index].submit(compute_metrics, *args, key=f'{filter_group_index}_compute_metrics')\n",
    "    futures.append(metrics_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2af9ecbc-4a95-430d-96aa-7482cd428cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089b0dbf14464f2b8b8337704fd0685d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmid_compute_min_max completed\n",
      "proc_id_compute_min_max completed\n",
      "file_id_compute_min_max completed\n",
      "tmid_indexed_ddf completed\n",
      "proc_id_indexed_ddf completed\n",
      "file_id_indexed_ddf completed\n",
      "proc_id_compute_metrics completed\n",
      "tmid_compute_metrics completed\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "('finalize-1664667b-d4fc-486f-b8f9-a28a9b43b045', <WorkerState 'tcp://192.168.64.30:44433', name: LSFCluster-0-10, status: closed, memory: 0, processing: 1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cancelled\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_62883/3990766379.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_tasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mall_tasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;31m#result = wait(metrics_futures)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m#metrics = client.gather(metrics_futures)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m()\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2992\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2994\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2995\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2150\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m             )\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             return sync(\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             )\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/workspace/iopp/projects/digio-venv-lassen/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2007\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKilledWorker\u001b[0m: ('finalize-1664667b-d4fc-486f-b8f9-a28a9b43b045', <WorkerState 'tcp://192.168.64.30:44433', name: LSFCluster-0-10, status: closed, memory: 0, processing: 1>)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from dask.distributed import as_completed\n",
    "from tqdm.auto import tqdm\n",
    "metrics_map = {}\n",
    "for future in tqdm(as_completed(futures)):\n",
    "    if \"compute_metrics\" in future.key:\n",
    "        metrics_map[future.key] = future.result(timeout=(2*60*60))\n",
    "    print(f'{future.key} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17f5d0c7-694d-4d72-a8a9-c32482186efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_val, max_val = metrics_map[\"tmid_compute_min_max\"]\n",
    "# depth = 10\n",
    "# next_tasks = 2 ** depth\n",
    "# interval = math.ceil((max_val - min_val) * 1.0 / next_tasks)\n",
    "# time_range = range(min_val, max_val, interval)\n",
    "# for start in time_range:\n",
    "#     print(start, start + interval - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2e891a2-f7ab-4463-8a08-641cd5d3cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_result(x, y):\n",
    "    return {\n",
    "        'read': {\n",
    "            'uniq_ranks': x['read']['uniq_ranks'] + y['read']['uniq_ranks'],\n",
    "            'agg_dur': x['read']['agg_dur'] + y['read']['agg_dur'],\n",
    "            'total_io_size': x['read']['total_io_size'] + y['read']['total_io_size'],\n",
    "            'uniq_filenames': x['read']['uniq_filenames'] + y['read']['uniq_filenames'],\n",
    "            'bw_sum': x['read']['bw_sum'] + y['read']['bw_sum'],\n",
    "            'ops': x['read']['ops'] + y['read']['ops'],\n",
    "        },\n",
    "        'write': {\n",
    "            'uniq_ranks': x['write']['uniq_ranks'] + y['write']['uniq_ranks'],\n",
    "            'agg_dur': x['write']['agg_dur'] + y['write']['agg_dur'],\n",
    "            'total_io_size': x['write']['total_io_size'] + y['write']['total_io_size'],\n",
    "            'uniq_filenames': x['write']['uniq_filenames'] + y['write']['uniq_filenames'],\n",
    "            'bw_sum': x['write']['bw_sum'] + y['write']['bw_sum'],\n",
    "            'ops': x['write']['ops'] + y['write']['ops'],\n",
    "        },\n",
    "        'metadata': {\n",
    "            'uniq_ranks': x['metadata']['uniq_ranks'] + y['metadata']['uniq_ranks'],\n",
    "            'agg_dur': x['metadata']['agg_dur'] + y['metadata']['agg_dur'],\n",
    "            'uniq_filenames': x['metadata']['uniq_filenames'] + y['metadata']['uniq_filenames'],\n",
    "            'ops': x['metadata']['ops'] + y['metadata']['ops'],\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3459637a-4a6b-4564-81fd-7c38569d5640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'read': {'uniq_ranks': 1764,\n",
       "   'agg_dur': 49288.523,\n",
       "   'total_io_size': 78001370679164,\n",
       "   'uniq_filenames': 10549344,\n",
       "   'bw_sum': 6142663000000.0,\n",
       "   'ops': 589699346},\n",
       "  'write': {'uniq_ranks': 1764,\n",
       "   'agg_dur': 3487.186,\n",
       "   'total_io_size': 112322741322,\n",
       "   'uniq_filenames': 10445412,\n",
       "   'bw_sum': 565536830.0,\n",
       "   'ops': 10730937},\n",
       "  'metadata': {'uniq_ranks': 3990,\n",
       "   'agg_dur': 80646.44,\n",
       "   'uniq_filenames': 21260259,\n",
       "   'ops': 114795803}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_map['tmid_compute_metrics'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9157a600-18a1-4247-9d4c-3bd2f1d6cddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'read': {'uniq_ranks': 1764,\n",
       "  'agg_dur': 49288.535,\n",
       "  'total_io_size': 78001370679164,\n",
       "  'uniq_filenames': 10549348,\n",
       "  'bw_sum': 6142664000000.0,\n",
       "  'ops': 589699346},\n",
       " 'write': {'uniq_ranks': 1764,\n",
       "  'agg_dur': 3487.186,\n",
       "  'total_io_size': 112322741322,\n",
       "  'uniq_filenames': 10445412,\n",
       "  'bw_sum': 565536830.0,\n",
       "  'ops': 10730937},\n",
       " 'metadata': {'uniq_ranks': 3990,\n",
       "  'agg_dur': 80646.44,\n",
       "  'uniq_filenames': 21260266,\n",
       "  'ops': 114795803}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_result(metrics_map['proc_id_compute_metrics'][0][0][0],metrics_map['proc_id_compute_metrics'][0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6ad400d-8cdd-4dd3-a8ef-12175353311f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'file_id_compute_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_62883/2173063591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_id_compute_metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'file_id_compute_metrics'"
     ]
    }
   ],
   "source": [
    "metrics_map['file_id_compute_metrics'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9e5a6-6ee9-4211-9898-f854f9d352b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa6121-a22e-4aed-b3aa-c7c1a781f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['file_id_indexed_ddf'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487d8b5-bb34-44b0-8a1c-6b5ac7f00921",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['tmid_compute_metrics'][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696688ed-27f3-4afd-a41b-ae7d6e013734",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_map['proc_id_compute_metrics'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b0de2-9580-4d4b-8ba8-7b273e877ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map['file_id_compute_metrics'][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ccfd6-e953-4b2e-9f80-8c0ddc928f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "local_client = Client(LocalCluster(n_workers=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80679d0b-c47b-4bcc-b0c4-a94e4f0d4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449aaae-13fb-4815-afdf-ea2bbfc6d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"/p/gpfs1/iopp/recorder_app_logs/genome_pegasus/nodes-32/_parquet\"\n",
    "ddf = dd.read_parquet(f\"{log_dir}/*.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad1157-b3c5-4ef6-b43c-0695c34671ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dd.compute(ddf['tmid'].min(), ddf['tmid'].max(), ddf['file_id'].min(), ddf['file_id'].max(), ddf['proc_id'].min(), ddf['proc_id'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b67fc7-4055-4f00-bb34-396a5ff6cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c919e1-9a92-4ecb-af93-a2885fbddc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val, max_val = -9223371986035283781, 9223369538921024184\n",
    "\n",
    "next_tasks = 2 ** 10\n",
    "interval = math.ceil((max_val - min_val) * 1.0 / next_tasks)\n",
    "time_range = range(min_val, max_val, interval)\n",
    "print(time_range)\n",
    "for start in time_range:\n",
    "    print(start, start + interval -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b3173a-d505-4624-931c-0a654afe9039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digio",
   "language": "python",
   "name": "digio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
