{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 319 Âµs (started: 2024-04-15 15:04:30 -05:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing a1_serial_pipeline.py\n",
      "time: 5.67 ms (started: 2024-04-15 15:04:36 -05:00)\n"
     ]
    }
   ],
   "source": [
    "%%writefile a1_serial_pipeline.py\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "XFER_SIZE_BINS = [\n",
    "    -np.inf,\n",
    "    4 * 1024.0,\n",
    "    # 16 * 1024.0,\n",
    "    64 * 1024.0,\n",
    "    # 256 * 1024.0,\n",
    "    1 * 1024.0 * 1024.0,\n",
    "    # 4 * 1024.0 * 1024.0,\n",
    "    16 * 1024.0 * 1024.0,\n",
    "    # 64 * 1024.0 * 1024.0,\n",
    "    np.inf\n",
    "]\n",
    "\n",
    "\n",
    "def calc_job_time(ddf):\n",
    "    return ddf['tend'].max() - ddf['tstart'].min()\n",
    "\n",
    "\n",
    "def calc_read_size(ddf):\n",
    "    return ddf[ddf['io_cat'] == 1]['size'].sum()\n",
    "\n",
    "\n",
    "def calc_write_size(ddf):\n",
    "    return ddf[ddf['io_cat'] == 2]['size'].sum()\n",
    "\n",
    "\n",
    "def calc_num_files(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2, 3]))]['file_name'].nunique()\n",
    "\n",
    "\n",
    "def calc_num_procs(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2, 3]))]['proc_name'].nunique()\n",
    "\n",
    "\n",
    "def calc_fpp(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2, 3]))].groupby(['file_name'])['proc_name'].nunique().to_frame().query('proc_name == 1')\n",
    "\n",
    "\n",
    "def calc_acc_pat(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))]['acc_pat'].value_counts()\n",
    "\n",
    "\n",
    "def calc_size(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))]['size'].sum()\n",
    "\n",
    "\n",
    "def calc_ops_dist(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2, 3]))]['io_cat'].value_counts()\n",
    "\n",
    "\n",
    "def calc_xfer_dist(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))]['size'].map_partitions(pd.cut, XFER_SIZE_BINS).value_counts()\n",
    "\n",
    "\n",
    "def calc_agg_bw(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))]['size'].sum() / ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))]['duration'].sum()\n",
    "\n",
    "\n",
    "def char_summary_tasks(ddf):\n",
    "    return [\n",
    "        calc_job_time(ddf),\n",
    "        calc_read_size(ddf),\n",
    "        calc_write_size(ddf),\n",
    "        calc_num_files(ddf),\n",
    "        calc_num_procs(ddf),\n",
    "        calc_fpp(ddf),\n",
    "        calc_acc_pat(ddf),\n",
    "        calc_size(ddf),\n",
    "        calc_ops_dist(ddf),\n",
    "        calc_xfer_dist(ddf),\n",
    "        calc_agg_bw(ddf),\n",
    "    ]\n",
    "\n",
    "\n",
    "def cm1_issue1_file_size_per_rank(ddf):\n",
    "    ddf0 = ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))] \\\n",
    "        .groupby(['file_name', 'io_cat']) \\\n",
    "        .agg({'size': ['mean', sum], 'rank': [min, max, 'count']})\n",
    "\n",
    "    ddf0.columns = ['_'.join(tup).rstrip('_') for tup in ddf0.columns.values]\n",
    "\n",
    "    ddf0 = ddf0.assign(rank_rank=lambda x: x['rank_min'].astype(str) + '-' + x['rank_max'].astype(str)) \\\n",
    "        .reset_index() \\\n",
    "        .groupby(['rank_rank', 'io_cat']) \\\n",
    "        .agg({'size_mean': 'mean', 'size_sum': sum})\n",
    "\n",
    "    ddf0['size_mean'] = ddf0['size_mean'] / 1024 ** 2\n",
    "    ddf0['size_sum'] = ddf0['size_sum'] / 1024 ** 3\n",
    "\n",
    "    return ddf0\n",
    "\n",
    "\n",
    "def cm1_issue3_rank_0_write_low_bw(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'] == 2)] \\\n",
    "        .groupby(['rank']) \\\n",
    "        .agg({'size': sum, 'duration': sum}) \\\n",
    "        .assign(bw=lambda x: x['size'] / x['duration'] / 1024 ** 3)\n",
    "\n",
    "\n",
    "def hacc_issue1_open_close(ddf):\n",
    "    return ddf[ddf['func_id'].str.contains('open|close')].groupby(['file_name', 'func_id'])['index'].count()\n",
    "\n",
    "\n",
    "def montagep_issue1_io_size_per_app(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))].groupby(['app', 'io_cat']).agg({'size': sum})\n",
    "\n",
    "\n",
    "def montagep_issue2_io_size_per_app_per_time(ddf):\n",
    "    def assign_time_bin(df):\n",
    "        df['time_bin'] = np.digitize(df['tmid'], bins=np.arange(434) * 1e7)\n",
    "        return df\n",
    "\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))] \\\n",
    "        .map_partitions(assign_time_bin) \\\n",
    "        .groupby(['app', 'time_bin']) \\\n",
    "        .agg({'size': sum}) \\\n",
    "        .sort_values('size', ascending=False)\n",
    "\n",
    "\n",
    "def generic_issue_bw_by_rank(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))] \\\n",
    "        .groupby(['rank']) \\\n",
    "        .agg({'size': sum, 'duration': sum}) \\\n",
    "        .assign(bw=lambda x: x['size'] / x['duration'] / 1024 ** 3)\n",
    "\n",
    "\n",
    "def generic_issue_low_bw(ddf):\n",
    "\n",
    "    def assign_size_bin(df):\n",
    "        df['size_bin'] = pd.cut(df['size'], XFER_SIZE_BINS)\n",
    "        return df\n",
    "\n",
    "    ddf0 = ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2]))]\n",
    "\n",
    "    return ddf0 \\\n",
    "        .map_partitions(assign_size_bin) \\\n",
    "        .groupby(['size_bin', 'io_cat']) \\\n",
    "        .agg({'index': 'count', 'size': sum, 'duration': sum}) \\\n",
    "        .assign(bw=lambda x: x['size'] / x['duration'] / 1024 ** 3) \\\n",
    "        .dropna()\n",
    "\n",
    "\n",
    "def generic_issue_metadata_access_per(ddf):\n",
    "    return ddf[(ddf['cat'] == 0) & (ddf['io_cat'].isin([1, 2, 3]))] \\\n",
    "        .groupby(['proc_name', 'io_cat']) \\\n",
    "        .sum() \\\n",
    "        .reset_index() \\\n",
    "        .groupby('io_cat')['duration'] \\\n",
    "        .max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to a1_serial_pipeline.py\n",
      "time: 5.13 ms (started: 2024-04-15 15:04:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a a1_serial_pipeline.py\n",
    "\n",
    "import dask\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import LSFCluster\n",
    "\n",
    "n_workers = 8\n",
    "n_threads_per_worker = 16\n",
    "\n",
    "cluster = LSFCluster(\n",
    "    cores=n_workers * n_threads_per_worker,\n",
    "    # death_timeout=self.config.death_timeout,\n",
    "    job_directives_skip=['-n', '-R', '-M', '-P', '-W 00:30'],\n",
    "    job_extra_directives=['-nnodes 1', '-G asccasc', '-q pdebug', '-W 120'],\n",
    "    # local_directory=self.config.local_dir,\n",
    "    memory=f\"1600GB\",\n",
    "    processes=n_workers,\n",
    "    scheduler_options=dict(\n",
    "        # dashboard_address=dashboard_address,\n",
    "        # host=self.config.host,\n",
    "    ),\n",
    "    use_stdin=True,\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "cluster.scale(n_workers)\n",
    "\n",
    "def _wait_until_workers_alive(client, sleep_seconds=2):\n",
    "    current_n_workers = len(client.scheduler_info()['workers'])\n",
    "    while client.status == 'running' and current_n_workers < n_workers:\n",
    "        current_n_workers = len(client.scheduler_info()['workers'])\n",
    "        print(f\"Waiting for workers ({current_n_workers}/{n_workers})\")\n",
    "        # Try to force cluster to boot workers\n",
    "        cluster._correct_state()\n",
    "        # Wait\n",
    "        time.sleep(sleep_seconds)\n",
    "    print('All workers alive')\n",
    "\n",
    "print('client dashboard', client.dashboard_link)\n",
    "\n",
    "_wait_until_workers_alive(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to a1_serial_pipeline.py\n",
      "time: 6.91 ms (started: 2024-04-15 15:05:40 -05:00)\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a a1_serial_pipeline.py\n",
    "\n",
    "app_traces = {\n",
    "    'hacc': '/usr/workspace/iopp/wisio_logs/recorder_hacc_32_0/_parquet',\n",
    "    'lbann_jag': '/usr/workspace/iopp/wisio_logs/recorder_lbann_jag_32/_parquet',\n",
    "    'cm1': '/p/gpfs1/iopp/wisio_logs/recorder_cm1_32_4/_parquet',\n",
    "    'montagep': '/usr/workspace/iopp/wisio_logs/recorder_montage_pegasus_32/_parquet',\n",
    "    'lbann_cosmoflow': '/usr/workspace/iopp/wisio_logs/recorder_lbann_cosmoflow_32/_parquet',\n",
    "}\n",
    "\n",
    "for app, trace in app_traces.items():\n",
    "\n",
    "    ddf = dd.read_parquet(trace)\n",
    "\n",
    "    char_tasks = char_summary_tasks(ddf)\n",
    "    char_t0 = time.perf_counter()\n",
    "    for i, t in enumerate(char_tasks):\n",
    "        t0 = time.perf_counter()\n",
    "        r, = dask.compute(t)\n",
    "        print(f\"{app} char {i + 1}/{len(char_tasks)} completed {time.perf_counter() - t0}\")\n",
    "    char_elapsed = time.perf_counter() - char_t0\n",
    "\n",
    "    app_tasks = []\n",
    "    if app == 'cm1':\n",
    "        app_tasks.extend([\n",
    "            generic_issue_low_bw(ddf),\n",
    "            generic_issue_metadata_access_per(ddf),\n",
    "            cm1_issue1_file_size_per_rank(ddf),\n",
    "            cm1_issue3_rank_0_write_low_bw(ddf),\n",
    "        ])\n",
    "    elif app == 'hacc':\n",
    "        app_tasks.extend([\n",
    "            generic_issue_bw_by_rank(ddf),\n",
    "            generic_issue_low_bw(ddf),\n",
    "            generic_issue_metadata_access_per(ddf),\n",
    "            hacc_issue1_open_close(ddf),\n",
    "        ])\n",
    "    elif app == 'montagep':\n",
    "        app_tasks.extend([\n",
    "            generic_issue_low_bw(ddf),\n",
    "            generic_issue_metadata_access_per(ddf),\n",
    "            montagep_issue1_io_size_per_app(ddf),\n",
    "            montagep_issue2_io_size_per_app_per_time(ddf),\n",
    "        ])\n",
    "    else:\n",
    "        app_tasks.extend([\n",
    "            generic_issue_bw_by_rank(ddf),\n",
    "            generic_issue_low_bw(ddf),\n",
    "            generic_issue_metadata_access_per(ddf),\n",
    "        ])\n",
    "\n",
    "    app_t0 = time.perf_counter()\n",
    "    for i, t in enumerate(app_tasks):\n",
    "        t0 = time.perf_counter()\n",
    "        r, = dask.compute(t)\n",
    "        print(f\"{app} issue {i + 1}/{len(app_tasks)} completed {time.perf_counter() - t0}\")\n",
    "    app_elapsed = time.perf_counter() - app_t0\n",
    "\n",
    "    print(f\"{app} total {char_elapsed + app_elapsed}\")\n",
    "\n",
    "    client.restart()\n",
    "\n",
    "    _wait_until_workers_alive(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
