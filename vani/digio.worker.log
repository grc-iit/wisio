/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35955'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37011'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:40139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:43163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:45215'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44215'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:34353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:36089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:35313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:34343'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44757'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45141'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:41591'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:41039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:40529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:43599'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:43091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:33825'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:46555'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:32965'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42915'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37879'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:35069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45989'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:33091'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:34431'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:40799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:42839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36223'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:35455'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:38821'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:33499'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:46175'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:33125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37395'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45519'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41815'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:38465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45575'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36367'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41635'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33027'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36019'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34585'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33447'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:37229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38357'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42715'
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-s4e_qh4r', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6u23fanl', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7ipg2vg7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-po5w28xk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oy9x8ajj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-iijjiye4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7fwjnkh2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4yi08f9w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-y0e4t366', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bx78utmc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ju22k5ze', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bbqelbue', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-h6kdrsz2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-131v0gg4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6bat35s4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7of70q29', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bu9cg_g1', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-e8rdeuzz', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xjt1h7oh', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lx5nlyah', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-4wt_vwq8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6ur0o0v3', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-89ii7r12', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bn_zb0h5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hq6t4tx6', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-gsbraks9', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-iugg13yj', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-r7j3k64o', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_6rlvghf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-jfkj5c3u', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-3cjvzags', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-99_zz6wj', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-vmosvf7b', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-3uwbv31b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6qo2205k', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qh63wxnw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bm5t_fqd', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i0o3e6mn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-q2l2m0s4', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lvl2o243', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a6wew9oo', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a6wew9oo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-liuvx1b4', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a6wew9oo' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a6wew9oo'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-om6pxocq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ausaxtik', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mab11n5o', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bcy6azsz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0bkd9oi_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-o9u5ekpo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-w7gpgr6n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-c6viznzo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-buriyc2a', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-uor6jt_w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-h2rilquq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-34h42a95', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5c9_8x2k', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_2oo_ssr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7yjlgh87', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ke_aeeag', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ao23jdwh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-m13m75kp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mjz9lnzm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-3tqv1afj', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-4b7g4ui2', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-43k1bs5x', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hm7k8ekf', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-62but2g0', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-orftu8a6', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-vmkwb424', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-zjg3mqhi', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-xoetx4fe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-titjhw_t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6qt7kn1u', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-svvhmaj5', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-574aijzk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-xiutxxhg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2yv4dvx3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ojsf75r5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6f_7txil', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_mq315_n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_mq315_n', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_mq315_n' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_mq315_n'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hjlgbj9u', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hjlgbj9u', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hjlgbj9u/storage' (failed in <built-in function unlink>): [Errno 20] Not a directory: 'storage'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hjlgbj9u' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hjlgbj9u'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-62fcdsx1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rg2c5d6v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-c3st3tsb', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:43175
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40829
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40313
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40829
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40313
distributed.worker - INFO -          dashboard at:        192.168.64.32:45541
distributed.worker - INFO -          dashboard at:        192.168.64.31:46553
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:34513
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:43175
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:34513
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:39745
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:35767
distributed.worker - INFO -          dashboard at:        192.168.64.31:33979
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43197
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:36513
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43197
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.32:36579
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40713
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:35767
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:38015
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:36513
distributed.worker - INFO -          dashboard at:        192.168.64.33:34703
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:38015
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34679
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-mm9hkrwr
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          dashboard at:        192.168.64.33:39303
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40713
distributed.worker - INFO -          dashboard at:        192.168.64.30:45263
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-63bk73xz
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:43873
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:46173
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34679
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40853
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:43873
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2yq9fabn
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38003
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-l9hn9fdw
distributed.worker - INFO -          dashboard at:        192.168.64.31:40621
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34271
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34851
distributed.worker - INFO -          dashboard at:        192.168.64.29:35959
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37957
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40853
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:41001
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:43619
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37957
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34271
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5ydaqxi_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45899
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45899
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:36041
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:35987
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37797
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-g6czp_6i
distributed.worker - INFO -          dashboard at:        192.168.64.33:37381
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:41001
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:43619
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hi2djub0
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:32795
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44695
distributed.worker - INFO -          dashboard at:        192.168.64.33:41395
distributed.worker - INFO -          dashboard at:        192.168.64.33:38813
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-gf262wxi
distributed.worker - INFO -          dashboard at:        192.168.64.31:44291
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41557
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37245
distributed.worker - INFO -          dashboard at:        192.168.64.32:44575
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34851
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33595
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          dashboard at:        192.168.64.29:42863
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45131
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38457
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34877
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45131
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44695
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:37621
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:32795
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42159
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33595
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41557
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37245
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38075
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37817
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36981
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38457
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34877
distributed.worker - INFO -          dashboard at:        192.168.64.30:45465
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36981
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.31:33077
distributed.worker - INFO -          dashboard at:        192.168.64.31:35011
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44179
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:37621
distributed.worker - INFO -          dashboard at:        192.168.64.33:46409
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:38887
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:36833
distributed.worker - INFO -          dashboard at:        192.168.64.33:45255
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44179
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:36693
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33293
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-83tkp7rg
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          dashboard at:        192.168.64.32:37321
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38075
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37817
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42081
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-1dpeuvt8
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.32:40259
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -          dashboard at:        192.168.64.29:45833
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:39263
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -          dashboard at:        192.168.64.30:32809
distributed.worker - INFO -          dashboard at:        192.168.64.30:40131
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41843
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42159
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          dashboard at:        192.168.64.33:37813
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:36833
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:38887
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:34447
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.33:35133
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t0lmtjkv
distributed.worker - INFO -          dashboard at:        192.168.64.32:41351
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41843
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40527
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6hjpiof4
distributed.worker - INFO -          dashboard at:        192.168.64.29:37789
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:39551
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40527
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33667
distributed.worker - INFO -          dashboard at:        192.168.64.33:41331
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8v4ygc89
distributed.worker - INFO -          dashboard at:        192.168.64.33:41009
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:34447
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8hccs27q
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41327
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-vgcnnk0t
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41327
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37939
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6pz61b78
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42081
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-in6olhd_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44731
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33667
distributed.worker - INFO -          dashboard at:        192.168.64.30:40599
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-vg9htc_6
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8ev7sy0d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:43405
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:40703
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37939
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:32933
distributed.worker - INFO -          dashboard at:        192.168.64.31:43203
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:45335
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45539
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yq8goqse
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45539
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38243
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45819
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-g8zvh2q6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ka2ogj0l
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2gl95fwf
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-grdb2f7_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3u3hem9e
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44731
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:43247
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-v9t85_5g
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-82muxekw
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:43247
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33203
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:35505
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40575
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33203
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:35505
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u3semqnw
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yym0joyp
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41717
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34895
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38243
distributed.worker - INFO -          dashboard at:        192.168.64.29:43495
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34513
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45819
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-m99ru7ql
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-sueg4k2t
distributed.worker - INFO -          dashboard at:        192.168.64.32:35897
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:45109
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-wwdxu5_u
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e_xl9j45
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-tqaj58k5
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40575
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bwjn7mza
distributed.worker - INFO -          dashboard at:        192.168.64.33:44193
distributed.worker - INFO -          dashboard at:        192.168.64.33:32817
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:37801
distributed.worker - INFO -          dashboard at:        192.168.64.33:45211
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35753
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34895
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41717
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:33347
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34513
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35753
distributed.worker - INFO -          dashboard at:        192.168.64.29:38815
distributed.worker - INFO -          dashboard at:        192.168.64.29:34409
distributed.worker - INFO -          dashboard at:        192.168.64.29:40423
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:37801
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-krmn9g5l
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38549
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f42rfl9_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38549
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:42497
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:42619
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hjxta50l
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:34357
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-sp74in89
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42823
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-b45b37at
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mv60cneg
distributed.worker - INFO -          dashboard at:        192.168.64.31:34475
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ai6zkd8w
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hrtuu1sq
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yylgungn
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-25fjbg7g
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-24he31v_
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wqbivp_3
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-cc7ak0xd
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-zcr74gys
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42823
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-khbhrjwc
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:34895
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i7hszhlr
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mh7h_id3
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ocxvmhfa
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l68d150q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-h7df6nk3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hyac5e0m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0b_56vt4
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-zyaib6wr
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41033
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41033
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:34667
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-t8ilroek
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35903
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35903
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:45081
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-v3qep_12
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5v1dsjyx
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43927
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36663
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35627
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43749
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36663
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43927
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35627
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43749
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:33383
distributed.worker - INFO -          dashboard at:        192.168.64.32:42653
distributed.worker - INFO -          dashboard at:        192.168.64.30:38449
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -          dashboard at:        192.168.64.30:39773
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43469
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43469
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kx6zx95f
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lf4njbe9
distributed.worker - INFO -          dashboard at:        192.168.64.29:46753
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37649
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40999
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37649
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40999
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-j0z7kq8h
distributed.worker - INFO -          dashboard at:        192.168.64.31:33229
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41643
distributed.worker - INFO -          dashboard at:        192.168.64.31:44307
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aeqh0zen
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40559
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41643
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35109
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40559
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-znp5725h
distributed.worker - INFO -          dashboard at:        192.168.64.30:38095
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:43679
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41903
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35109
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37035
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ghs452fh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41903
distributed.worker - INFO -          dashboard at:        192.168.64.30:36711
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:46589
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hq99h597
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-asoqav3h
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44521
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37715
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44521
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37715
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:38701
distributed.worker - INFO -          dashboard at:        192.168.64.32:45583
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cbs678l_
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43087
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ngld8yr8
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43087
distributed.worker - INFO -          dashboard at:        192.168.64.32:41985
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-o84hatff
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oi5jq1_u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6v60rris
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9dztcn4a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33535
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33535
distributed.worker - INFO -          dashboard at:        192.168.64.32:33485
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35173
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35173
distributed.worker - INFO -          dashboard at:        192.168.64.32:37919
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37955
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gbf4nl0m
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37955
distributed.worker - INFO -          dashboard at:        192.168.64.30:40181
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yar48oby
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9n7uvzhg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42375
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42375
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36313
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36313
distributed.worker - INFO -          dashboard at:        192.168.64.30:45399
distributed.worker - INFO -          dashboard at:        192.168.64.30:41079
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-wrwqazkx
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2gpw4maa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:46825
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:46825
distributed.worker - INFO -          dashboard at:        192.168.64.28:42435
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43859
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43859
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:39999
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:33731
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:33731
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:40903
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gg99o4ej
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35447
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2d8e6mkn
distributed.worker - INFO -          dashboard at:        192.168.64.28:33685
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40345
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f32dn50r
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:42509
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40345
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:42509
distributed.worker - INFO -          dashboard at:        192.168.64.28:43409
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:42223
distributed.worker - INFO -          dashboard at:        192.168.64.28:42175
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:42223
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-g_fki0gy
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -          dashboard at:        192.168.64.28:35993
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:46305
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45903
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:46305
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45903
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:41699
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45839
distributed.worker - INFO -          dashboard at:        192.168.64.28:45705
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45839
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:37779
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mqvvanfp
distributed.worker - INFO -          dashboard at:        192.168.64.28:36357
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:46359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t7ut9w40
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:37779
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:34535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:46359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:43489
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:34535
distributed.worker - INFO -          dashboard at:        192.168.64.28:34161
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:42563
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mrfsk8v2
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44385
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44385
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-udhh80sh
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gt07gfyd
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:43595
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hcpvw6q2
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7jvv7ll9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ynajsikc
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7d26jtuo
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-msaa2z60
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:42109
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:42109
distributed.worker - INFO -          dashboard at:        192.168.64.28:37899
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43989
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43989
distributed.worker - INFO -          dashboard at:        192.168.64.28:39605
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:41537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-bebd6xv9
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-9jy_s7zd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:41537
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37035
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40165
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920196: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:40:42 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:40:45 2022
                            <40*lassen28>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:40:45 2022
Terminated at Sat Sep 17 18:43:12 2022
Results reported at Sat Sep 17 18:43:12 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:41537 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.68 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   147 sec.
    Turnaround time :                            150 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920195: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:40:42 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:40:45 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:40:45 2022
Terminated at Sat Sep 17 18:43:12 2022
Results reported at Sat Sep 17 18:43:12 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37035 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.81 sec.
    Max Memory :                                 58 MB
    Average Memory :                             58.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   722 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   147 sec.
    Turnaround time :                            150 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920194: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:40:42 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:40:45 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:40:45 2022
Terminated at Sat Sep 17 18:43:12 2022
Results reported at Sat Sep 17 18:43:12 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:40165 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.70 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   147 sec.
    Turnaround time :                            150 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920193: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:40:41 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:40:43 2022
                            <40*lassen31>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:40:43 2022
Terminated at Sat Sep 17 18:43:20 2022
Results reported at Sat Sep 17 18:43:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:41537 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.74 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   149 sec.
    Turnaround time :                            159 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920191: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:40:41 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:40:43 2022
                            <40*lassen33>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:40:43 2022
Terminated at Sat Sep 17 18:43:53 2022
Results reported at Sat Sep 17 18:43:53 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37035 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.72 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   190 sec.
    Turnaround time :                            192 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920192: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:40:41 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:40:43 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:40:43 2022
Terminated at Sat Sep 17 18:43:54 2022
Results reported at Sat Sep 17 18:43:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:40165 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.73 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   190 sec.
    Turnaround time :                            193 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:38017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:33017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:38267'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:36437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:37799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:45853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:46259'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:35367'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:42713'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:32839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33877'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39329'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:41401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:42167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33605'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33423'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38389'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33253'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:39997'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:42461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:46735'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:34241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:43581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:35033'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:33111'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:38185'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:37523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:37625'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:44319'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40025'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43439'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:35675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:37073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:36419'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41013'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37255'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41551'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41943'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35713'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36265'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41639'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38457'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42353'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:46111'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45373'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39627'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35689'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:38701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45045'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:32917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33947'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:46649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:46617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:40057'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:33735'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37381'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:40015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:32973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:43151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:40295'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:36449'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:43775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:36245'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39479'
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-zcr74gys', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-o84hatff', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-mm9hkrwr', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gbf4nl0m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-v3qep_12', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9n7uvzhg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2gl95fwf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-g8zvh2q6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3u3hem9e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-m99ru7ql', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lf4njbe9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hi2djub0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-sueg4k2t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-wwdxu5_u', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ka2ogj0l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ai6zkd8w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-j0z7kq8h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-in6olhd_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kx6zx95f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9dztcn4a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-znp5725h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5ydaqxi_', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oi5jq1_u', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ngld8yr8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cbs678l_', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2gpw4maa', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6v60rris', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-t8ilroek', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yar48oby', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-63bk73xz', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-b45b37at', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-grdb2f7_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hrtuu1sq', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-wrwqazkx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f32dn50r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-1dpeuvt8', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gt07gfyd', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-msaa2z60', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t0lmtjkv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ghs452fh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-82muxekw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2d8e6mkn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-asoqav3h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t7ut9w40', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hcpvw6q2', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-v9t85_5g', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-l9hn9fdw', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ynajsikc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6pz61b78', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7d26jtuo', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f42rfl9_', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-25fjbg7g', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-vgcnnk0t', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7jvv7ll9', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mrfsk8v2', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-g_fki0gy', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2yq9fabn', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-9jy_s7zd', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-udhh80sh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-h7df6nk3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mqvvanfp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mv60cneg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gg99o4ej', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-bebd6xv9', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34393
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34393
distributed.worker - INFO -          dashboard at:        192.168.64.29:34509
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39419
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46181
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39419
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46153
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36411
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46181
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41635
distributed.worker - INFO -          dashboard at:        192.168.64.29:38599
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-eh2kpp__
distributed.worker - INFO -          dashboard at:        192.168.64.29:36097
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46425
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36411
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41635
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40745
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46425
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36927
distributed.worker - INFO -          dashboard at:        192.168.64.29:44003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:44511
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38715
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33637
distributed.worker - INFO -          dashboard at:        192.168.64.29:45333
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40745
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36927
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46153
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38715
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33637
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:45131
distributed.worker - INFO -          dashboard at:        192.168.64.29:43615
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:39301
distributed.worker - INFO -          dashboard at:        192.168.64.29:43949
distributed.worker - INFO -          dashboard at:        192.168.64.29:46219
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35467
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44323
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-pwzi03k2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35467
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44323
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-mj3_gfjz
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:35117
distributed.worker - INFO -          dashboard at:        192.168.64.29:36201
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-o9b1obqx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5107katj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-l74wkszk
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9nw9zy0n
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dqbfqo5o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45511
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36033
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4b1thvp5
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45511
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36033
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:36045
distributed.worker - INFO -          dashboard at:        192.168.64.29:37375
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35061
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35061
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:37065
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rnlhn9bo
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-d5i_ruys
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6corchqz
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-z0uzexi5
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-h56lvu20
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-l_mn3ryo
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yvebxv4y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44931
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44931
distributed.worker - INFO -          dashboard at:        192.168.64.33:45669
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:32771
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:42329
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:32771
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:42717
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:42329
distributed.worker - INFO -          dashboard at:        192.168.64.33:41385
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-honzomen
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:42717
distributed.worker - INFO -          dashboard at:        192.168.64.33:36449
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -          dashboard at:        192.168.64.33:40139
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:39629
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:39629
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.33:43953
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40101
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:37225
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3rbpe_gw
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40101
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-unv2py5k
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-v83h16wy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:37225
distributed.worker - INFO -          dashboard at:        192.168.64.33:35947
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:34899
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:36417
distributed.worker - INFO -          dashboard at:        192.168.64.33:40187
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:34899
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:36417
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.33:33235
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.33:40667
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:39419
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hle8jru9
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:39419
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:34623
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-zqksk7_n
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ursrhgd4
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_ilq7m89
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:39533
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:39533
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.33:35045
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xemt7o4u
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:45761
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:45761
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.33:45729
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-u527vxnn
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-87f32p4d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kl07ioce
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:37481
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:43183
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:37481
distributed.worker - INFO -          dashboard at:        192.168.64.33:44171
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:43183
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO -          dashboard at:        192.168.64.33:35011
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-r3ly8ttr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:46797
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:46797
distributed.worker - INFO -          dashboard at:        192.168.64.33:43627
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-m04uaab4
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oluq1mke
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40361
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40361
distributed.worker - INFO -          dashboard at:        192.168.64.33:45057
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-do28wta5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34463
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38203
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38203
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41641
distributed.worker - INFO -          dashboard at:        192.168.64.30:41845
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44121
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41641
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44121
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:38693
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -          dashboard at:        192.168.64.30:43679
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-b37y0qtk
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-1y1d_m09
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-4_yqu1mj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35511
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35511
distributed.worker - INFO -          dashboard at:        192.168.64.30:39555
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43903
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43903
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:38691
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36635
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36635
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-z7wtept9
distributed.worker - INFO -          dashboard at:        192.168.64.30:39747
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-goo0ek0w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nrxa0ba_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44049
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44049
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39417
distributed.worker - INFO -          dashboard at:        192.168.64.30:41063
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41907
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39417
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41907
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:39961
distributed.worker - INFO -          dashboard at:        192.168.64.30:38845
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38915
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tj790ud_
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2l_0tz3h
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34733
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38915
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34733
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:37315
distributed.worker - INFO -          dashboard at:        192.168.64.30:42565
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-euxzk44u
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-27v_b4bl
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-y6k4wtxw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:38281
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:38281
distributed.worker - INFO -          dashboard at:        192.168.64.27:45753
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:35825
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-v7rrszd4
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:35825
distributed.worker - INFO -          dashboard at:        192.168.64.27:35713
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:39519
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-9ugl_7so
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:39519
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:35527
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:36847
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:43159
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.27:34999
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:35527
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:36847
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:43159
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -          dashboard at:        192.168.64.27:38157
distributed.worker - INFO -          dashboard at:        192.168.64.27:38481
distributed.worker - INFO -          dashboard at:        192.168.64.27:39005
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:42169
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:42169
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:33359
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.27:36567
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:33359
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-maioczl2
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.27:37615
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_hodiz2k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-o1eakjga
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ewxomjfs
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:45709
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:43567
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:45709
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:34675
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:36047
distributed.worker - INFO -          dashboard at:        192.168.64.27:45067
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:34675
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.27:38737
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:36047
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ainu5qvv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:44781
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.27:40229
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:43567
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:44781
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mkkobs4m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.27:44333
distributed.worker - INFO -          dashboard at:        192.168.64.27:33241
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-15h7v71c
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t7ft9t5j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kc83j3qo
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-weljjdl2
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8ljqotc7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:44993
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:44993
distributed.worker - INFO -          dashboard at:        192.168.64.27:43779
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:45849
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:45849
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-qexueewi
distributed.worker - INFO -          dashboard at:        192.168.64.27:45475
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sygq8p19
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45015
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45015
distributed.worker - INFO -          dashboard at:        192.168.64.30:39939
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-a4vgu0ix
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40527
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40527
distributed.worker - INFO -          dashboard at:        192.168.64.30:35117
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0xgf1wy4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44147
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44147
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:43769
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.core - INFO - Starting established connection
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-xzmt8po8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39383
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39383
distributed.worker - INFO -          dashboard at:        192.168.64.30:43811
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-azmzu_gz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:33303
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:33303
distributed.worker - INFO -          dashboard at:        192.168.64.27:42937
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-1qf2x47m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38593
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38593
distributed.worker - INFO -          dashboard at:        192.168.64.30:36397
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-3e98u6bu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42291
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-83tkp7rg', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i7hszhlr', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-khbhrjwc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l68d150q', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5v1dsjyx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hyac5e0m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hjxta50l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0b_56vt4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8v4ygc89', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-g6czp_6i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bwjn7mza', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wqbivp_3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-gf262wxi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-vg9htc_6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hq99h597', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yq8goqse', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8ev7sy0d', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8hccs27q', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-tqaj58k5', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e_xl9j45', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yym0joyp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-24he31v_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-cc7ak0xd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-sp74in89', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-sp74in89', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-sp74in89' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-sp74in89'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mh7h_id3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mh7h_id3', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mh7h_id3' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mh7h_id3'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ocxvmhfa', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6hjpiof4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6hjpiof4', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6hjpiof4' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6hjpiof4'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aeqh0zen', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aeqh0zen', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aeqh0zen' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aeqh0zen'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yylgungn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yylgungn', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yylgungn' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yylgungn'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-krmn9g5l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-krmn9g5l', purging
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36027
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:33993
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44491
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:33993
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40107
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44491
distributed.worker - INFO -          dashboard at:        192.168.64.28:36889
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40107
distributed.worker - INFO -          dashboard at:        192.168.64.28:35811
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36027
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -          dashboard at:        192.168.64.28:44345
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -          dashboard at:        192.168.64.28:41401
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:46339
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:46339
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36571
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:38449
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.28:45249
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36571
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aj87uao9
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-prakc83j
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wg_vluz7
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:38449
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:38069
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:41509
distributed.worker - INFO -          dashboard at:        192.168.64.28:43651
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:42707
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e5xx8kts
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40077
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:43905
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:42707
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:38069
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40077
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:41027
distributed.worker - INFO -          dashboard at:        192.168.64.28:46319
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:41509
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35167
distributed.worker - INFO -          dashboard at:        192.168.64.28:45277
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35167
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:33523
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -          dashboard at:        192.168.64.28:40035
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yem317a_
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6ne1aqsx
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-rqtmtwn0
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40317
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45371
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40317
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45371
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:36577
distributed.worker - INFO -          dashboard at:        192.168.64.28:39931
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8tq4vxg_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ya986ue7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_7lu7vrx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-n5of2bsr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44843
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bp2esf6b
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nyomikcc
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qt05xuud
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44843
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:41589
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-afdcuk86
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-krmn9g5l' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-krmn9g5l'
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u3semqnw', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u3semqnw', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u3semqnw/storage' (failed in <built-in function unlink>): [Errno 2] No such file or directory: 'storage'
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u3semqnw' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u3semqnw'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-zyaib6wr', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:39121
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:39121
distributed.worker - INFO -          dashboard at:        192.168.64.28:46039
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5rb5x7ov
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34011
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:45533
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41143
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43609
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41925
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35427
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34011
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41143
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43609
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35427
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41925
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:45533
distributed.worker - INFO -          dashboard at:        192.168.64.32:39651
distributed.worker - INFO -          dashboard at:        192.168.64.32:46179
distributed.worker - INFO -          dashboard at:        192.168.64.32:42829
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40971
distributed.worker - INFO -          dashboard at:        192.168.64.32:40471
distributed.worker - INFO -          dashboard at:        192.168.64.32:41613
distributed.worker - INFO -          dashboard at:        192.168.64.32:38319
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40971
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43679
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37129
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43679
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37129
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37353
distributed.worker - INFO -          dashboard at:        192.168.64.32:45231
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37353
distributed.worker - INFO -          dashboard at:        192.168.64.32:33063
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33417
distributed.worker - INFO -          dashboard at:        192.168.64.32:44557
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33417
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-dmsv4b02
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nnw4s2h9
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-d9x6pecx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:46501
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yec6_1gg
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8hsofwfk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-w55mra2d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39483
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39483
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:33073
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0mg3dbec
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-t07gjpnq
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-d0f_sk5z
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-kgbmcprh
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:40159
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-chzcdhh4
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-jq1sld8v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36173
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36309
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36173
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36309
distributed.worker - INFO -          dashboard at:        192.168.64.32:35067
distributed.worker - INFO -          dashboard at:        192.168.64.32:33087
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-j4noonaj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-jk1ezomh
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42197
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42197
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38645
distributed.worker - INFO -          dashboard at:        192.168.64.32:33977
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38645
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:39555
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34873
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_79jc649
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nuhs25zw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34873
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 53.06 MiB from 521 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 53.12 MiB from 700 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144225141344, 144535971390)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235920004914, 236230834960)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d3c8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:56480 remote=tcp://192.168.64.232:42291>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d208>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:56486 remote=tcp://192.168.64.232:42291>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d278>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:56476 remote=tcp://192.168.64.232:42291>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207945300774, 208256130820)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d0f0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:56482 remote=tcp://192.168.64.232:42291>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b710f0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:56478 remote=tcp://192.168.64.232:42291>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:42291 after 30 s
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235298344822, 235609174868)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208256130820, 208566960866)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206391150544, 206701980590)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236230834960, 236541665006)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173132335622, 173443165668)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b72048>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47516 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d160>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47518 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d2e8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47520 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d278>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47542 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d0f0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47514 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d128>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47524 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d128>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47526 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b710f0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47522 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001f9f9e8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47530 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d2b0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47536 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d1d0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47538 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001cf8160>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47544 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d278>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47540 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200000b4fb70>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47534 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6c2b0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47528 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6c4a8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:47532 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6afd0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58044 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d208>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58054 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001f99e48>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58036 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d0f0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58052 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d160>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58066 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d240>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58040 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d128>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58072 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58056 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b71128>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58060 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d080>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58038 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6bf60>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58062 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d278>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58068 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d128>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58064 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d160>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58070 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58042 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207012810636, 207323640682)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145157631482, 145468461528)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6d160>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:58058 remote=tcp://192.168.64.232:34873>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34873 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b57748>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.33:49228 remote=tcp://192.168.64.232:34463>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6c2e8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.33:49226 remote=tcp://192.168.64.232:34463>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b71128>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.33:49230 remote=tcp://192.168.64.232:34463>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172821505576, 173132335622)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172199845484, 172510675530)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144846801436, 145157631482)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234987514776, 235298344822)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172510675530, 172821505576)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236852495052, 237163325098)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6c208>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:55394 remote=tcp://192.168.64.232:34463>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:34463 after 30 s
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208877790912, 209188620958)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236541665006, 236852495052)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:56150 remote=tcp://192.168.64.232:42291>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207634470728, 207945300774)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144535971390, 144846801436)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171889015438, 172199845484)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208566960866, 208877790912)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206701980590, 207012810636)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235609174868, 235920004914)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:56128 remote=tcp://192.168.64.232:42291>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207323640682, 207634470728)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55254 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55252 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55256 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031caae8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55258 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55260 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55262 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55264 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55266 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c4598>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55270 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000032182f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55268 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c46a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55272 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55274 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55276 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55278 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55280 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:55282 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49084 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c4378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49086 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c46a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49088 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49090 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3ae8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49092 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c56a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49094 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49096 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49098 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49100 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49102 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3ae8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49104 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49106 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba598>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49108 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49110 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49112 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.33:49114 remote=tcp://192.168.64.232:34463> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003217158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56088 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c4378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56092 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56090 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000036dd158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56094 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c4378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56096 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003217158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56098 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56100 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56102 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56104 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56106 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000032182f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56108 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55234 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bc8c8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55236 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55240 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba950>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55238 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba950>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55242 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55244 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb950>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55246 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba950>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55252 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55248 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba8c8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55250 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55254 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55256 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c4378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55258 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55260 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55262 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56110 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218d08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56112 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56114 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ca378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55264 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56116 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:56118 remote=tcp://192.168.64.232:42291> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bc378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47376 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47378 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba950>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47380 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb840>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47382 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47384 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218d08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47386 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218d08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47388 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c4510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47390 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47392 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47394 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47396 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47398 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47400 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c38c8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47402 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba950>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47404 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c8ae8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:47436 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000036dd158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57900 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57898 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57902 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57904 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57906 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57908 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57910 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57912 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57914 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c4378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57916 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c38c8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57918 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ba378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57920 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb400>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57922 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57924 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c3840>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57926 remote=tcp://192.168.64.232:34873> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ca378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:57928 remote=tcp://192.168.64.232:34873> already closed.
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:33637
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:39419
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:42717
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:45761
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:46797
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:36417
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:39419
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:40101
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:42329
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:43183
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:35467
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36033
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:38715
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:34393
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:46153
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:40745
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:37225
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:35527
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:45511
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:37481
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:35061
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36411
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:41635
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:34899
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:36847
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:34675
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:40527
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44323
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:46425
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:39629
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:32771
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:46181
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:34733
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:39519
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:44121
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:43567
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:35511
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:44049
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:44993
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:33359
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:36635
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:41641
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:39417
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:38915
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:42169
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:45709
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:44147
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:38203
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:38281
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:45015
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:39383
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:41907
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:45849
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:44931
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:39533
distributed.worker - INFO - Stopping worker at tcp://192.168.64.33:40361
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:36027
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:43609
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:40107
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:41143
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:41925
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:34011
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:33993
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:45533
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:36571
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35427
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:46339
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:37353
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:37129
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:38449
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:40077
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:44491
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:43679
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33417
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:41509
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:39483
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:35167
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:38069
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:45371
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:40971
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:40317
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:42707
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:44843
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:36173
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:36309
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:42197
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:38645
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:38593
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:39121
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:35825
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:43159
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:36047
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:44781
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36927
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:33303
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:43151'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:38267'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:38185'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:46735'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:42461'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:42453'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:43903
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:38241'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:45085'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:41013'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:41157'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43581'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:46751'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:42167'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46111'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:39133'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:44659'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:33423'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:39265'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:42617'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:42713'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:38457'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:36437'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:32839'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:33877'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:33605'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:41401'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:38389'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:46259'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:36419'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:37799'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:39329'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:44533'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:37073'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:39467'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45045'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:40053'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:32917'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:43103'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45373'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42853'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:39627'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:35367'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45617'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:38701'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:40009'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:33947'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:35689'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:37523'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:40017'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.33:33253'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:45029'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:44319'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:39997'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:36693'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:35033'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:37625'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41639'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35713'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35871'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:37255'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:45853'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:43233'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:42353'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:39829'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41943'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:36449'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36265'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:44535'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:36245'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:40015'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:37381'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:42695'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:46617'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:40295'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39479'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:33735'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:43775'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:37715'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:46649'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:32973'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:38017'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:46817'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41551'
distributed.dask_worker - INFO - End worker
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40149'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:33111'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:33017'
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:43439'
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40769'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:40057'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:38211'
distributed.dask_worker - INFO - End worker
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:34241'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40025'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:36493'
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:34031'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:43667'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:35675'
distributed.dask_worker - INFO - End worker
logout

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920206: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:44:18 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:44:21 2022
                            <40*lassen27>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:44:21 2022
Terminated at Sat Sep 17 18:59:34 2022
Results reported at Sat Sep 17 18:59:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:42291 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.39 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.58 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   914 sec.
    Turnaround time :                            916 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920205: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:44:18 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:44:21 2022
                            <40*lassen28>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:44:21 2022
Terminated at Sat Sep 17 18:59:34 2022
Results reported at Sat Sep 17 18:59:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:34873 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.44 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.58 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1361 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   914 sec.
    Turnaround time :                            916 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920204: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:44:18 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:44:21 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:44:21 2022
Terminated at Sat Sep 17 18:59:34 2022
Results reported at Sat Sep 17 18:59:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:34463 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.38 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.58 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   914 sec.
    Turnaround time :                            916 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920203: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:44:17 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:44:21 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:44:21 2022
Terminated at Sat Sep 17 18:59:34 2022
Results reported at Sat Sep 17 18:59:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:42291 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.48 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.58 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   914 sec.
    Turnaround time :                            917 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920202: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:44:17 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:44:21 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:44:21 2022
Terminated at Sat Sep 17 18:59:34 2022
Results reported at Sat Sep 17 18:59:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:34873 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.46 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.58 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   914 sec.
    Turnaround time :                            917 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920201: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 18:44:17 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 18:44:21 2022
                            <40*lassen33>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 18:44:21 2022
Terminated at Sat Sep 17 18:59:34 2022
Results reported at Sat Sep 17 18:59:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:34463 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.37 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.58 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   914 sec.
    Turnaround time :                            917 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:39287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:45489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:42949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:45867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:46275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:45103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:33869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:41787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:43871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:43619'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:34391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:32769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:45879'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:33119'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:33053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.27:37927'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:32885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41555'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36309'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36193'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42231'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38255'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:34617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36927'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43647'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:38509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:41157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:36123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:36327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43283'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:34131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:41129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:33617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:46139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:42443'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35545'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:33933'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:34919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42589'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39343'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:38895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42661'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43703'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36173'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37057'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44815'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33993'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:36163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:41863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45515'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:46817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:34525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44213'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:35997'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:35607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45699'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:37027'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:37241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42723'
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-zdgraxw9', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-buqkk37r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nrvj1xot', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-399m9bf_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-56cb7ien', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-4p9kui64', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvwr9ueo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wi35hizl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-347_qh1i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sr66piow', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvtzmdv1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-c119cwyo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-02iicrl4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0tvrxi67', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a59w83lo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uq87krhl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1kfhnywo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-o6gjuz4w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9pxxoj0_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7zosrbl4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nh8m1o2m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-psamthfq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wpbzffs3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u_s5d0t1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_cmzijhe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-gj64j68t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-4cz79xxq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xuxo9zvi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ad4k869p', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2680g_m4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9f4l9w5f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7ghch1kx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngxm47ru', purging
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40015
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:33809
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40015
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:37193
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:33809
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:34857
distributed.worker - INFO -          dashboard at:        192.168.64.28:43575
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35531
distributed.worker - INFO -          dashboard at:        192.168.64.34:37761
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:37193
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:34857
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36723
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35531
distributed.worker - INFO -          dashboard at:        192.168.64.32:38067
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43295
distributed.worker - INFO -          dashboard at:        192.168.64.28:42915
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36723
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:39125
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37729
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33483
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43295
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -          dashboard at:        192.168.64.28:34037
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:42783
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:43251
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:43251
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33483
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37729
distributed.worker - INFO -          dashboard at:        192.168.64.34:43845
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7iq55kiz
distributed.worker - INFO -          dashboard at:        192.168.64.32:34795
distributed.worker - INFO -          dashboard at:        192.168.64.32:40213
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-h9r5rky0
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-347_qh1i
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-zdgraxw9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:40025
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:35267
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-790j4157
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44411
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lwuuwrf_
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44411
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0b32ihej
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:35267
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:36813
distributed.worker - INFO -          dashboard at:        192.168.64.27:46237
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:36813
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:40025
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:37427
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39987
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvtzmdv1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uq87krhl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:37427
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39987
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:42885
distributed.worker - INFO -          dashboard at:        192.168.64.28:35829
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:33643
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          dashboard at:        192.168.64.27:42635
distributed.worker - INFO -          dashboard at:        192.168.64.27:44903
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40887
distributed.worker - INFO -          dashboard at:        192.168.64.34:36599
distributed.worker - INFO -          dashboard at:        192.168.64.34:38581
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:42885
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44785
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:45235
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43203
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:33643
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:41127
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:41127
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40887
distributed.worker - INFO -          dashboard at:        192.168.64.28:37361
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40903
distributed.worker - INFO -          dashboard at:        192.168.64.28:40431
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44785
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:45235
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43203
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39759
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33981
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:41779
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40903
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:33169
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:43969
distributed.worker - INFO -          dashboard at:        192.168.64.34:39173
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -          dashboard at:        192.168.64.28:44991
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.27:42325
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:39537
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:45939
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:39141
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.34:33775
distributed.worker - INFO -          dashboard at:        192.168.64.32:36537
distributed.worker - INFO -          dashboard at:        192.168.64.32:38791
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33981
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43643
distributed.worker - INFO -          dashboard at:        192.168.64.32:34613
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34257
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35379
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44593
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:43969
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:42929
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:41779
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:33169
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:39537
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:45939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:36707
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nh8m1o2m
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.27:43119
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:40747
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:36707
distributed.worker - INFO -          dashboard at:        192.168.64.27:42609
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1kfhnywo
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:35081
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43643
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35379
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34257
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42157
distributed.worker - INFO -          dashboard at:        192.168.64.29:33471
distributed.worker - INFO -          dashboard at:        192.168.64.29:39043
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44593
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-mf0rruwe
distributed.worker - INFO -          dashboard at:        192.168.64.34:46419
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xq6gic7o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0zbuq7_6
distributed.worker - INFO -          dashboard at:        192.168.64.28:33537
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:41689
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:40747
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wpbzffs3
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:39141
distributed.worker - INFO -          dashboard at:        192.168.64.27:44597
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.27:46571
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39425
distributed.worker - INFO -          dashboard at:        192.168.64.34:40035
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42157
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:38757
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:37231
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39425
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mfxq0eb0
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          dashboard at:        192.168.64.27:38211
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9f4l9w5f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:42249
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:37231
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -          dashboard at:        192.168.64.34:37405
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7zosrbl4
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sr66piow
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_4r8ahrm
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gdbdclqw
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:39789
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-gj64j68t
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:39195
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40617
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33701
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41629
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:39789
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-buqkk37r
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40617
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33701
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-e4c47034
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2i_30mbn
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41629
distributed.worker - INFO -          dashboard at:        192.168.64.29:35269
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:36559
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xuxo9zvi
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.27:46491
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-nqr6u6qc
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ad4k869p
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-02iicrl4
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:39195
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:36123
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-sbe7mcz8
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-11e07ub0
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-41y35l9c
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-383v992j
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-drfwvg_8
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39133
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngxm47ru
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34465
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-psamthfq
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-026v3l60
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39133
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.27:34179
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:39953
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34465
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41275
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33897
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44025
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.34:34565
distributed.worker - INFO -          dashboard at:        192.168.64.29:39011
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41275
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33897
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44025
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:39953
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.27:34439
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-wpi3bqbp
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -          dashboard at:        192.168.64.29:33445
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34241
distributed.worker - INFO -          dashboard at:        192.168.64.29:43941
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-admug7bk
distributed.worker - INFO -          dashboard at:        192.168.64.29:36397
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2680g_m4
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41091
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dj0me5bl
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34241
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36291
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5diaeqly
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41091
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:36621
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:46523
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -          dashboard at:        192.168.64.29:37963
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36291
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-us6x052p
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7ghch1kx
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:46523
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36895
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ceqc_2z7
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:37217
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37093
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36895
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36007
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.27:33557
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a59w83lo
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-24zu78n8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wendt0wr
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36007
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gq1mf20a
distributed.worker - INFO -          dashboard at:        192.168.64.29:37141
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2dvinslr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:36195
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-06xkfy_v
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0q5hunkj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37093
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:40305
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5ek42an8
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:42659
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lgis9ea3
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u_s5d0t1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-k9dyq95_
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:40305
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.27:38633
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-luov9ijw
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dswpktqt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-56cb7ien
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_cmzijhe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:46097
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39277
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:46097
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44935
distributed.worker - INFO -          dashboard at:        192.168.64.34:41335
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44935
distributed.worker - INFO -          dashboard at:        192.168.64.32:46717
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:33341
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:33341
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.27:46731
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-c119cwyo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-x0ecvqgj
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33859
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33761
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35777
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44327
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33859
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33761
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35777
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39277
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44327
distributed.worker - INFO -          dashboard at:        192.168.64.32:39281
distributed.worker - INFO -          dashboard at:        192.168.64.32:43993
distributed.worker - INFO -          dashboard at:        192.168.64.32:34159
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.34:33375
distributed.worker - INFO -          dashboard at:        192.168.64.32:45283
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-4cz79xxq
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36627
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36627
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45521
distributed.worker - INFO -          dashboard at:        192.168.64.32:46023
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46457
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44641
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46457
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45521
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44641
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.27:40903
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:32841
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45253
distributed.worker - INFO -          dashboard at:        192.168.64.32:36475
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-4p9kui64
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nrvj1xot
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-399m9bf_
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36701
distributed.worker - INFO -          dashboard at:        192.168.64.28:35045
distributed.worker - INFO -          Listening to:  tcp://192.168.64.27:40903
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0tvrxi67
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45253
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:36757
distributed.worker - INFO -          dashboard at:        192.168.64.27:33099
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36701
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:35561
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:45193
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-uq9vv9za
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvwr9ueo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-o6gjuz4w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:36757
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39997
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.34:45361
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39997
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44873
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -          dashboard at:        192.168.64.32:34401
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9pxxoj0_
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44873
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:33303
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-jel481lk
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rjr715b1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wi35hizl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2wekwqt1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:36667
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5zcoq8r4
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4asjc7b1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:36667
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:41775
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-snmedhm5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:41789
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:41789
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7f3t8glq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:39585
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-lwi9b6mw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39265
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39265
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33353
distributed.worker - INFO -          dashboard at:        192.168.64.30:34993
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41507
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33353
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37719
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41507
distributed.worker - INFO -          dashboard at:        192.168.64.30:45945
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37719
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41985
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33875
distributed.worker - INFO -          dashboard at:        192.168.64.30:36057
distributed.worker - INFO -          dashboard at:        192.168.64.30:37663
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41985
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33875
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:38285
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:46745
distributed.worker - INFO -          dashboard at:        192.168.64.30:35103
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45733
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:46745
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-iv2w0tmo
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45733
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42257
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:44621
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43079
distributed.worker - INFO -          dashboard at:        192.168.64.30:45345
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42257
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43079
distributed.worker - INFO -          dashboard at:        192.168.64.30:33647
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -          dashboard at:        192.168.64.30:35681
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-c4zipdsx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38481
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ljoko0vt
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6cavhdjf
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38481
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9elitl2s
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1_unaess
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:37167
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-3m6p4ql2
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-faqq903s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-oi952j3z
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bpt5e9y7
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42999
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42999
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:46631
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bexpzet_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngtod_ve
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36597
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36597
distributed.worker - INFO -          dashboard at:        192.168.64.30:33433
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41889
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39611
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39611
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41889
distributed.worker - INFO -          dashboard at:        192.168.64.30:34839
distributed.worker - INFO -          dashboard at:        192.168.64.30:41573
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42503
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42503
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:39009
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1dq_55ms
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46881
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5ff5so2x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5acw1ho1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ibp0j_8n
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40247
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46881
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37003
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:37452 remote=tcp://192.168.64.232:40247>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:37448 remote=tcp://192.168.64.232:40247>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46254 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46256 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46234 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46258 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46230 remote=tcp://192.168.64.232:37003>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:37218 remote=tcp://192.168.64.232:40247>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:59834 remote=tcp://192.168.64.232:40247>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46238 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46252 remote=tcp://192.168.64.232:37003>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46236 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46228 remote=tcp://192.168.64.232:37003>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.34:46232 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:59636 remote=tcp://192.168.64.232:40247>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:37216 remote=tcp://192.168.64.232:40247>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:55152 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:55148 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:55144 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:55154 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:55150 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:55146 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:43552 remote=tcp://192.168.64.232:37003>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:43556 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:43554 remote=tcp://192.168.64.232:37003>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:43558 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:43530 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:43528 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55130 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55136 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55132 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55126 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55134 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55128 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55138 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55140 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:37234 remote=tcp://192.168.64.232:40247>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x20000241d4a8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:37003 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:43582 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:37003 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:37003 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:43586 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:37003 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70208>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:37003 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:43584 remote=tcp://192.168.64.232:37003>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:37003 after 30 s
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55142 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.27:55144 remote=tcp://192.168.64.232:46881>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55184 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55178 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55180 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x2000028f3f28>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55176 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f470>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55174 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55190 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55194 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f438>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55188 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b703c8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55186 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4e0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:55192 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f198>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:55176 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:55178 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:55180 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f438>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:55184 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f358>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.27:55182 remote=tcp://192.168.64.232:46881>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:46881 after 30 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (265759689330, 266070519376)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294045223516, 294356053562)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262651388870, 262962218916)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (44137866532, 44448696578)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158523323460, 158834153506)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290936923056, 291247753102)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260786408594, 261097238640)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (246488226478, 246799056524)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39164585796, 39475415842)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247109886570, 247420716616)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (77085851408, 77396681454)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (258921428318, 259232258364)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27974704140, 28285534186)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20825613082, 21136443128)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (219135182430, 219446012476)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59679368832, 59990198878)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73355890856, 73666720902)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242758265926, 243069095972)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283787831998, 284098662044)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267003009514, 267313839560)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (21136443128, 21447273174)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291558583148, 291869413194)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78950831684, 79261661730)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285341982228, 285652812274)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283166171906, 283477001952)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (224108463166, 224419293212)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282855341860, 283166171906)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (44759526624, 45070356670)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282233681768, 282544511814)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280368701492, 280679531538)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278503721216, 278814551262)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316735816874, 317046646920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161942453966, 162253284012)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (2486640368, 2797470414)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62476839246, 62787669292)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242447435880, 242758265926)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (265448859284, 265759689330)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154482532862, 154793362908)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (91384033524, 91694863570)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184943877370, 185254707416)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (37921265612, 38232095658)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18649802760, 18960632806)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198620399394, 198931229440)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33258814922, 33569644968)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (186187197554, 186498027600)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (81437472052, 81748302098)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87654072972, 87964903018)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (79883321822, 80194151868)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (88275733064, 88586563110)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (167226564748, 167537394794)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278814551262, 279125381308)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (269489649882, 269800479928)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279747041400, 280057871446)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (273841270526, 274152100572)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7459921104, 7770751150)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62166009200, 62476839246)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (91073203478, 91384033524)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179970596634, 180281426680)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46624506900, 46935336946)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (218824352384, 219135182430)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73045060810, 73355890856)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26731383956, 27042214002)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294666883608, 294977713654)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (263583879008, 263894709054)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244623246202, 244934076248)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313627516414, 313938346460)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281922851722, 282233681768)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58436048648, 58746878694)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289382772826, 289693602872)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (98222294536, 98533124582)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (318289967104, 318600797150)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157280003276, 157590833322)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6527430966, 6838261012)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (263894709054, 264205539100)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253637317536, 253948147582)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (243690756064, 244001586110)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191471308336, 191782138382)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (83302452328, 83613282374)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249907356984, 250218187030)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191160478290, 191471308336)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24866403680, 25177233726)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26109723864, 26420553910)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (91694863570, 92005693616)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78329171592, 78640001638)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31704664692, 32015494738)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (192403798474, 192714628520)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (104128065410, 104438895456)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280990361584, 281301191630)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (108790516100, 109101346146)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248353206754, 248664036800)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (273219610434, 273530440480)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36677945428, 36988775474)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (72112570672, 72423400718)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136454390194, 136765220240)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152306722540, 152617552586)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (164429094334, 164739924380)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (54084428004, 54395258050)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261408068686, 261718898732)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221621822798, 221932652844)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (220067672568, 220378502614)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191782138382, 192092968428)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46002846808, 46313676854)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (45692016762, 46002846808)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87964903018, 88275733064)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1554150230, 1864980276)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15541502300, 15852332346)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61855179154, 62166009200)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (37610435566, 37921265612)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (103506405318, 103817235364)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317979137058, 318289967104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280679531538, 280990361584)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228770913856, 229081743902)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (180903086772, 181213916818)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (19893122944, 20203952990)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241204115696, 241514945742)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96357314260, 96668144306)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158212493414, 158523323460)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210431941142, 210742771188)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (84545772512, 84856602558)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (220378502614, 220689332660)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166915734702, 167226564748)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (107858025962, 108168856008)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (0, 310830046)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23933913542, 24244743588)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75531701178, 75842531224)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12744031886, 13054861932)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295599373746, 295910203792)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (200174549624, 200485379670)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308343405632, 308654235678)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204215340222, 204526170268)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225973443442, 226284273488)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (200485379670, 200796209716)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (93870673892, 94181503938)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57192728464, 57503558510)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (38853755750, 39164585796)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (178105616358, 178416446404)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (66828459890, 67139289936)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (264827199192, 265138029238)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136765220240, 137076050286)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82059132144, 82369962190)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193025458566, 193336288612)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194268778750, 194579608796)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (45381186716, 45692016762)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (25488063772, 25798893818)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (71180080534, 71490910580)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239960795512, 240271625558)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272287120296, 272597950342)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152617552586, 152928382632)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (60922689016, 61233519062)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283477001952, 283787831998)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (185254707416, 185565537462)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101330594996, 101641425042)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287206962504, 287517792550)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41029566072, 41340396118)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (180592256726, 180903086772)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (186498027600, 186808857646)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (89208223202, 89519053248)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64030989476, 64341819522)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5284110782, 5594940828)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (226905933580, 227216763626)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157590833322, 157901663368)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (19582292898, 19893122944)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (89829883294, 90140713340)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312695026276, 313005856322)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (162564114058, 162874944104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175929806036, 176240636082)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31083004600, 31393834646)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (212296921418, 212607751464)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301505144620, 301815974666)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174375655806, 174686485852)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171889015438, 172199845484)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27042214002, 27353044048)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (243379926018, 243690756064)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (932490138, 1243320184)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296531863884, 296842693930)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56881898418, 57192728464)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (150441742264, 150752572310)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172821505576, 173132335622)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174064825760, 174375655806)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280057871446, 280368701492)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270732970066, 271043800112)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (182457237002, 182768067048)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285031152182, 285341982228)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57503558510, 57814388556)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289071942780, 289382772826)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (293112733378, 293423563424)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156347513138, 156658343184)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292180243240, 292491073286)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (10257391518, 10568221564)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204526170268, 204837000314)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (293423563424, 293734393470)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48800317222, 49111147268)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203282850084, 203593680130)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (159144983552, 159455813598)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62787669292, 63098499338)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (98533124582, 98843954628)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (72734230764, 73045060810)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (259543088410, 259853918456)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28596364232, 28907194278)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204837000314, 205147830360)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202661189992, 202972020038)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (34812965152, 35123795198)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24555573634, 24866403680)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99465614720, 99776444766)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (177794786312, 178105616358)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (266692179468, 267003009514)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240893285650, 241204115696)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138630200516, 138941030562)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290626093010, 290936923056)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (266070519376, 266381349422)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313005856322, 313316686368)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6216600920, 6527430966)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57814388556, 58125218602)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (84234942466, 84545772512)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (222243482890, 222554312936)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (88897393156, 89208223202)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68693440166, 69004270212)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51908617682, 52219447728)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221932652844, 222243482890)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262340558824, 262651388870)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169402375070, 169713205116)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96046484214, 96357314260)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24244743588, 24555573634)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73977550948, 74288380994)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (16784822484, 17095652530)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27353044048, 27663874094)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170334865208, 170645695254)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172199845484, 172510675530)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (177483956266, 177794786312)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175308145944, 175618975990)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174686485852, 174997315898)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196133759026, 196444589072)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101019764950, 101330594996)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160699133782, 161009963828)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235298344822, 235609174868)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199552889532, 199863719578)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261097238640, 261408068686)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36988775474, 37299605520)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292491073286, 292801903332)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6838261012, 7149091058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261718898732, 262029728778)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239339135420, 239649965466)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56571068372, 56881898418)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82680792236, 82991622282)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188052177830, 188363007876)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133035259688, 133346089734)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313938346460, 314249176506)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (223486803074, 223797633120)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30461344508, 30772174554)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13676522024, 13987352070)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160388303736, 160699133782)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170024035162, 170334865208)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234365854684, 234676684730)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31393834646, 31704664692)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4040790598, 4351620644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (65274309660, 65585139706)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20203952990, 20514783036)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239028305374, 239339135420)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75842531224, 76153361270)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206391150544, 206701980590)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155104192954, 155415023000)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (150130912218, 150441742264)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187119687692, 187430517738)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (85478262650, 85789092696)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171578185392, 171889015438)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (95735654168, 96046484214)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316424986828, 316735816874)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173753995714, 174064825760)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248974866846, 249285696892)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (164739924380, 165050754426)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (102884745226, 103195575272)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131791939504, 132102769550)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (176240636082, 176551466128)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55016918142, 55327748188)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17095652530, 17406482576)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (70247590396, 70558420442)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (63409329384, 63720159430)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (3108300460, 3419130506)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (85167432604, 85478262650)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189295498014, 189606328060)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206701980590, 207012810636)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244934076248, 245244906294)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308654235678, 308965065724)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (224730123258, 225040953304)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197377079210, 197687909256)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268246329698, 268557159744)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245244906294, 245555736340)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296842693930, 297153523976)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151995892494, 152306722540)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169713205116, 170024035162)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187741347784, 188052177830)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (159455813598, 159766643644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297153523976, 297464354022)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18338972714, 18649802760)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (63098499338, 63409329384)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35434625244, 35745455290)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (93249013800, 93559843846)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (60301028924, 60611858970)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43516206440, 43827036486)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202972020038, 203282850084)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (213851071648, 214161901694)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43205376394, 43516206440)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312384196230, 312695026276)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41340396118, 41651226164)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22379763312, 22690593358)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262962218916, 263273048962)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244312416156, 244623246202)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49421977314, 49732807360)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (219756842522, 220067672568)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105993045686, 106303875732)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132413599596, 132724429642)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52841107820, 53151937866)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69936760350, 70247590396)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39475415842, 39786245888)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193957948704, 194268778750)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (168469884932, 168780714978)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (195512098934, 195822928980)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (93559843846, 93870673892)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (150752572310, 151063402356)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (67450119982, 67760950028)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13987352070, 14298182116)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155415023000, 155725853046)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154793362908, 155104192954)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (86721582834, 87032412880)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (183700557186, 184011387232)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (85789092696, 86099922742)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (183078897094, 183389727140)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247420716616, 247731546662)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (8392411242, 8703241288)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (97911464490, 98222294536)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (25798893818, 26109723864)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9324901380, 9635731426)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29528854370, 29839684416)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (71490910580, 71801740626)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48489487176, 48800317222)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (226595103534, 226905933580)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172510675530, 172821505576)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314870836598, 315181666644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106303875732, 106614705778)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260164748502, 260475578548)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69004270212, 69315100258)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290315262964, 290626093010)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29839684416, 30150514462)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267935499652, 268246329698)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30772174554, 31083004600)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138941030562, 139251860608)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (190849648244, 191160478290)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245555736340, 245866566386)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199242059486, 199552889532)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228149253764, 228460083810)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (86099922742, 86410752788)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (265138029238, 265448859284)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59368538786, 59679368832)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52530277774, 52841107820)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310208385908, 310519215954)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292801903332, 293112733378)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69315100258, 69625930304)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156658343184, 156969173230)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247731546662, 248042376708)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13365691978, 13676522024)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248042376708, 248353206754)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170956525300, 171267355346)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309586725816, 309897555862)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (192714628520, 193025458566)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (107547195916, 107858025962)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (167537394794, 167848224840)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291869413194, 292180243240)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (182768067048, 183078897094)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238717475328, 239028305374)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173132335622, 173443165668)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (277260401032, 277571231078)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225040953304, 225351783350)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199863719578, 200174549624)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310830046000, 311140876046)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (79572491776, 79883321822)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92316523662, 92627353708)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11189881656, 11500711702)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (71801740626, 72112570672)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (21758103220, 22068933266)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5905770874, 6216600920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229703403994, 230014234040)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (181524746864, 181835576910)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238406645282, 238717475328)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109101346146, 109412176192)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (224419293212, 224730123258)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242136605834, 242447435880)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (65895969752, 66206799798)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5594940828, 5905770874)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (34502135106, 34812965152)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92938183754, 93249013800)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50976127544, 51286957590)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174997315898, 175308145944)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (162253284012, 162564114058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (81126642006, 81437472052)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311451706092, 311762536138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47556997038, 47867827084)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35123795198, 35434625244)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (134589409918, 134900239964)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158834153506, 159144983552)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23312253450, 23623083496)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (293734393470, 294045223516)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11811541748, 12122371794)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240582455604, 240893285650)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64652649568, 64963479614)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7149091058, 7459921104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112520476652, 112831306698)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (72423400718, 72734230764)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59057708740, 59368538786)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42894546348, 43205376394)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (180281426680, 180592256726)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (186808857646, 187119687692)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267624669606, 267935499652)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (222865142982, 223175973028)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (190538818198, 190849648244)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22690593358, 23001423404)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (90140713340, 90451543386)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36367115382, 36677945428)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151063402356, 151374232402)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (79261661730, 79572491776)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250529017076, 250839847122)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29218024324, 29528854370)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152928382632, 153239212678)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (246799056524, 247109886570)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (14919842208, 15230672254)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315492496690, 315803326736)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82991622282, 83302452328)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160077473690, 160388303736)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (178727276450, 179038106496)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197687909256, 197998739302)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64963479614, 65274309660)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (178416446404, 178727276450)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184633047324, 184943877370)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284409492090, 284720322136)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (190227988152, 190538818198)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193336288612, 193647118658)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316114156782, 316424986828)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189917158106, 190227988152)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99154784674, 99465614720)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23001423404, 23312253450)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (37299605520, 37610435566)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (19271462852, 19582292898)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133656919780, 133967749826)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288450282688, 288761112734)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (159766643644, 160077473690)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (195201268888, 195512098934)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (83924112420, 84234942466)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (181835576910, 182146406956)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101641425042, 101952255088)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188363007876, 188673837922)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244001586110, 244312416156)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74599211040, 74910041086)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294977713654, 295288543700)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (63720159430, 64030989476)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155725853046, 156036683092)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179038106496, 179348936542)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175618975990, 175929806036)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260475578548, 260786408594)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169091545024, 169402375070)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (104438895456, 104749725502)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (263273048962, 263583879008)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (84856602558, 85167432604)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (134278579872, 134589409918)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32326324784, 32637154830)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272597950342, 272908780388)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136143560148, 136454390194)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28285534186, 28596364232)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241514945742, 241825775788)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55638578234, 55949408280)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211053601234, 211364431280)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (94492333984, 94803164030)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53151937866, 53462767912)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112209646606, 112520476652)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297775184068, 298086014114)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (34191305060, 34502135106)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (8081581196, 8392411242)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (223175973028, 223486803074)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (185876367508, 186187197554)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308965065724, 309275895770)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227527593672, 227838423718)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165672414518, 165983244564)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56260238326, 56571068372)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22068933266, 22379763312)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156036683092, 156347513138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115317947066, 115628777112)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51286957590, 51597787636)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170645695254, 170956525300)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41651226164, 41962056210)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (80194151868, 80504981914)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (67139289936, 67450119982)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111277156468, 111587986514)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (243069095972, 243379926018)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109723006238, 110033836284)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (70869250488, 71180080534)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289693602872, 290004432918)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33569644968, 33880475014)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (177173126220, 177483956266)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82369962190, 82680792236)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (185565537462, 185876367508)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (104749725502, 105060555548)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26420553910, 26731383956)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9014071334, 9324901380)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298396844160, 298707674206)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (65585139706, 65895969752)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163185774150, 163496604196)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312073366184, 312384196230)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201728699854, 202039529900)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92005693616, 92316523662)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73666720902, 73977550948)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298086014114, 298396844160)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297464354022, 297775184068)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12122371794, 12433201840)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (16473992438, 16784822484)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116561267250, 116872097296)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (226284273488, 226595103534)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105682215640, 105993045686)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221310992752, 221621822798)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (83613282374, 83924112420)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116872097296, 117182927342)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27663874094, 27974704140)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (76775021362, 77085851408)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (67760950028, 68071780074)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32015494738, 32326324784)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (8703241288, 9014071334)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4351620644, 4662450690)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202350359946, 202661189992)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173443165668, 173753995714)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (176862296174, 177173126220)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163807434242, 164118264288)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (97600634444, 97911464490)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149509252126, 149820082172)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151685062448, 151995892494)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20514783036, 20825613082)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184322217278, 184633047324)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309275895770, 309586725816)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198931229440, 199242059486)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291247753102, 291558583148)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18960632806, 19271462852)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (66517629844, 66828459890)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (114385456928, 114696286974)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9635731426, 9946561472)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36056285336, 36367115382)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (70558420442, 70869250488)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131481109458, 131791939504)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (81748302098, 82059132144)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268867989790, 269178819836)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (259232258364, 259543088410)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106925535824, 107236365870)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (252704827398, 253015657444)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305545935218, 305856765264)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (232500874408, 232811704454)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299018504252, 299329334298)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307100085448, 307410915494)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50665297498, 50976127544)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23623083496, 23933913542)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (97289804398, 97600634444)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (10879051610, 11189881656)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55949408280, 56260238326)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68071780074, 68382610120)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307410915494, 307721745540)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165983244564, 166294074610)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (223797633120, 224108463166)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313316686368, 313627516414)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207945300774, 208256130820)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (129616129182, 129926959228)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (153860872770, 154171702816)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (153550042724, 153860872770)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (38232095658, 38542925704)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1243320184, 1554150230)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305235105172, 305545935218)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (88586563110, 88897393156)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215405221878, 215716051924)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231568384270, 231879214316)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302748464804, 303059294850)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231257554224, 231568384270)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229081743902, 229392573948)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230014234040, 230325064086)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230325064086, 230635894132)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (95424824122, 95735654168)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53773597958, 54084428004)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17717312622, 18028142668)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130237789274, 130548619320)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227216763626, 227527593672)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208256130820, 208566960866)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317668307012, 317979137058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101952255088, 102263085134)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (25177233726, 25488063772)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (153239212678, 153550042724)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50043637406, 50354467452)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32637154830, 32947984876)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115007117020, 115317947066)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64341819522, 64652649568)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (621660092, 932490138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69625930304, 69936760350)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (168159054886, 168469884932)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214783561786, 215094391832)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207634470728, 207945300774)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (14298182116, 14609012162)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92627353708, 92938183754)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (54706088096, 55016918142)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149198422080, 149509252126)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151374232402, 151685062448)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47246166992, 47556997038)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (103195575272, 103506405318)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310519215954, 310830046000)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305856765264, 306167595310)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230635894132, 230946724178)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303370124896, 303680954942)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303680954942, 303991784988)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133967749826, 134278579872)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115628777112, 115939607158)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59990198878, 60301028924)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310830046, 621660092)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (10568221564, 10879051610)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (89519053248, 89829883294)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (76464191316, 76775021362)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30150514462, 30461344508)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230946724178, 231257554224)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100398104858, 100708934904)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (181213916818, 181524746864)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (80504981914, 80815811960)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (220689332660, 221000162706)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (167848224840, 168159054886)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315181666644, 315492496690)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74910041086, 75220871132)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35745455290, 36056285336)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (66206799798, 66517629844)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133346089734, 133656919780)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (264205539100, 264516369146)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113142136744, 113452966790)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49732807360, 50043637406)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (98843954628, 99154784674)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (86410752788, 86721582834)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299329334298, 299640164344)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203593680130, 203904510176)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15230672254, 15541502300)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75220871132, 75531701178)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111587986514, 111898816560)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (3419130506, 3729960552)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61544349108, 61855179154)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201107039762, 201417869808)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179348936542, 179659766588)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315803326736, 316114156782)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227838423718, 228149253764)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237163325098, 237474155144)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188673837922, 188984667968)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (162874944104, 163185774150)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58125218602, 58436048648)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (94803164030, 95113994076)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (38542925704, 38853755750)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299640164344, 299950994390)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207323640682, 207634470728)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306789255402, 307100085448)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105060555548, 105371385594)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (16163162392, 16473992438)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4973280736, 5284110782)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (118426247526, 118737077572)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61233519062, 61544349108)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236852495052, 237163325098)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165050754426, 165361584472)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (182146406956, 182457237002)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74288380994, 74599211040)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156969173230, 157280003276)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157901663368, 158212493414)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241825775788, 242136605834)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207012810636, 207323640682)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138319370470, 138630200516)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (54395258050, 54706088096)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18028142668, 18338972714)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221000162706, 221310992752)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (108479686054, 108790516100)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99776444766, 100087274812)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236541665006, 236852495052)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317046646920, 317357476966)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (2175810322, 2486640368)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (76153361270, 76464191316)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96668144306, 96978974352)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110344666330, 110655496376)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187430517738, 187741347784)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228460083810, 228770913856)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206080320498, 206391150544)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302437634758, 302748464804)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50354467452, 50665297498)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (14609012162, 14919842208)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (137386880332, 137697710378)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78018341546, 78329171592)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100087274812, 100398104858)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302126804712, 302437634758)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225351783350, 225662613396)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236230834960, 236541665006)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135521900056, 135832730102)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4662450690, 4973280736)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55327748188, 55638578234)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9946561472, 10257391518)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253015657444, 253326487490)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163496604196, 163807434242)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (183389727140, 183700557186)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179659766588, 179970596634)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100708934904, 101019764950)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225662613396, 225973443442)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78640001638, 78950831684)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96978974352, 97289804398)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (90451543386, 90762373432)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237784985190, 238095815236)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (200796209716, 201107039762)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161320793874, 161631623920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119047907618, 119358737664)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13054861932, 13365691978)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279125381308, 279436211354)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53462767912, 53773597958)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301815974666, 302126804712)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306478425356, 306789255402)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (222554312936, 222865142982)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (164118264288, 164429094334)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (102263085134, 102573915180)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237474155144, 237784985190)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11500711702, 11811541748)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (129926959228, 130237789274)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (205769490452, 206080320498)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1864980276, 2175810322)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52219447728, 52530277774)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105371385594, 105682215640)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203904510176, 204215340222)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166604904656, 166915734702)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132102769550, 132413599596)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17406482576, 17717312622)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (114696286974, 115007117020)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238095815236, 238406645282)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308032575586, 308343405632)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (107236365870, 107547195916)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304924275126, 305235105172)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (3729960552, 4040790598)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58746878694, 59057708740)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234676684730, 234987514776)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113763796836, 114074626882)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109412176192, 109723006238)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110655496376, 110966326422)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161631623920, 161942453966)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106614705778, 106925535824)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138008540424, 138319370470)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (134900239964, 135211070010)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276017080848, 276327910894)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271665460204, 271976290250)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161009963828, 161320793874)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299950994390, 300261824436)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (117182927342, 117493757388)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303059294850, 303370124896)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (102573915180, 102884745226)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271976290250, 272287120296)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165361584472, 165672414518)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304613445080, 304924275126)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (219446012476, 219756842522)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (118737077572, 119047907618)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253948147582, 254258977628)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301194314574, 301505144620)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (114074626882, 114385456928)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231879214316, 232190044362)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (266381349422, 266692179468)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130548619320, 130859449366)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229392573948, 229703403994)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (137076050286, 137386880332)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245866566386, 246177396432)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113452966790, 113763796836)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (252393997352, 252704827398)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300261824436, 300572654482)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135211070010, 135521900056)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110966326422, 111277156468)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (233744194592, 234055024638)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234055024638, 234365854684)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286896132458, 287206962504)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135832730102, 136143560148)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (117493757388, 117804587434)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257056448042, 257367278088)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306167595310, 306478425356)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (269178819836, 269489649882)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234987514776, 235298344822)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300572654482, 300883484528)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116250437204, 116561267250)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235609174868, 235920004914)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130859449366, 131170279412)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307721745540, 308032575586)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257988938180, 258299768226)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12433201840, 12744031886)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275084590710, 275395420756)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131170279412, 131481109458)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235920004914, 236230834960)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304302615034, 304613445080)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (118115417480, 118426247526)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248664036800, 248974866846)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285652812274, 285963642320)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (258299768226, 258610598272)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110033836284, 110344666330)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (60611858970, 60922689016)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115939607158, 116250437204)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311762536138, 312073366184)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33880475014, 34191305060)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28907194278, 29218024324)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112831306698, 113142136744)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (129305299136, 129616129182)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275706250802, 276017080848)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (233122534500, 233433364546)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (233433364546, 233744194592)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255813127858, 256123957904)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132724429642, 133035259688)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (137697710378, 138008540424)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (232190044362, 232500874408)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (251772337260, 252083167306)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257367278088, 257678108134)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314249176506, 314560006552)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68382610120, 68693440166)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166294074610, 166604904656)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257678108134, 257988938180)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (77707511500, 78018341546)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255191467766, 255502297812)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250839847122, 251150677168)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15852332346, 16163162392)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:59828 remote=tcp://192.168.64.232:40247>: Stream is closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (232811704454, 233122534500)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281301191630, 281612021676)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (258610598272, 258921428318)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281612021676, 281922851722)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303991784988, 304302615034)
kwargs:    {}
Exception: 'CancelledError()'

distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59606 remote=tcp://192.168.64.232:40247>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 95, in _background_send
    payload, serializers=self.serializers, on_error="raise"
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59606 remote=tcp://192.168.64.232:40247>: Stream is closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47867827084, 48178657130)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (252083167306, 252393997352)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255502297812, 255813127858)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184011387232, 184322217278)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249285696892, 249596526938)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253326487490, 253637317536)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254569807674, 254880637720)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51597787636, 51908617682)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (251461507214, 251772337260)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (264516369146, 264827199192)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (251150677168, 251461507214)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286585302412, 286896132458)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171267355346, 171578185392)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42272886256, 42583716302)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211986091372, 212296921418)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216959372108, 217270202154)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197066249164, 197377079210)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240271625558, 240582455604)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (176551466128, 176862296174)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290004432918, 290315262964)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210121111096, 210431941142)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48178657130, 48489487176)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216026881970, 216337712016)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (212607751464, 212918581510)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215094391832, 215405221878)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111898816560, 112209646606)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (213540241602, 213851071648)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217891862246, 218202692292)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209810281050, 210121111096)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209499451004, 209810281050)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128061978952, 128372808998)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214161901694, 214472731740)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216337712016, 216648542062)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (213229411556, 213540241602)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215716051924, 216026881970)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295910203792, 296221033838)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256434787950, 256745617996)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (117804587434, 118115417480)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119358737664, 119669567710)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249596526938, 249907356984)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254258977628, 254569807674)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254880637720, 255191467766)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250218187030, 250529017076)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (205147830360, 205458660406)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256123957904, 256434787950)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (80815811960, 81126642006)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256745617996, 257056448042)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122156208078, 122467038124)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (246177396432, 246488226478)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314560006552, 314870836598)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279436211354, 279747041400)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188984667968, 189295498014)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202039529900, 202350359946)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (218202692292, 218513522338)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211364431280, 211675261326)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32947984876, 33258814922)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124021188354, 124332018400)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124332018400, 124642848446)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288761112734, 289071942780)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284098662044, 284409492090)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208877790912, 209188620958)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216648542062, 216959372108)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209188620958, 209499451004)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285963642320, 286274472366)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (212918581510, 213229411556)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211675261326, 211986091372)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (218513522338, 218824352384)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214472731740, 214783561786)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217581032200, 217891862246)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210742771188, 211053601234)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276638740940, 276949570986)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217270202154, 217581032200)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196755419118, 197066249164)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46313676854, 46624506900)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (40407905980, 40718736026)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (90762373432, 91073203478)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (120602057848, 120912887894)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125575338584, 125886168630)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124953678492, 125264508538)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128994469090, 129305299136)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267313839560, 267624669606)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287517792550, 287828622596)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (21447273174, 21758103220)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (108168856008, 108479686054)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145779291574, 146090121620)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122777868170, 123088698216)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121223717940, 121534547986)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125886168630, 126196998676)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124642848446, 124953678492)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262029728778, 262340558824)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311140876046, 311451706092)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284720322136, 285031152182)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (195822928980, 196133759026)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (44448696578, 44759526624)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (77396681454, 77707511500)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (103817235364, 104128065410)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145468461528, 145779291574)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123088698216, 123399528262)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119669567710, 119980397756)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126507828722, 126818658768)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126818658768, 127129488814)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288139452642, 288450282688)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189606328060, 189917158106)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126196998676, 126507828722)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208566960866, 208877790912)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (205458660406, 205769490452)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39786245888, 40097075934)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (94181503938, 94492333984)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121845378032, 122156208078)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121534547986, 121845378032)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (40097075934, 40407905980)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125264508538, 125575338584)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128683639044, 128994469090)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239649965466, 239960795512)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300883484528, 301194314574)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294356053562, 294666883608)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287828622596, 288139452642)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (192092968428, 192403798474)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194890438842, 195201268888)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201417869808, 201728699854)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (45070356670, 45381186716)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41962056210, 42272886256)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154171702816, 154482532862)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122467038124, 122777868170)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123399528262, 123710358308)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119980397756, 120291227802)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (168780714978, 169091545024)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196444589072, 196755419118)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194579608796, 194890438842)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42583716302, 42894546348)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (120291227802, 120602057848)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123710358308, 124021188354)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (127129488814, 127440318860)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (127440318860, 127751148906)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317357476966, 317668307012)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282544511814, 282855341860)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286274472366, 286585302412)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198309569348, 198620399394)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (95113994076, 95424824122)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149820082172, 150130912218)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (120912887894, 121223717940)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128372808998, 128683639044)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197998739302, 198309569348)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49111147268, 49421977314)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (40718736026, 41029566072)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (2797470414, 3108300460)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87032412880, 87343242926)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193647118658, 193957948704)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43827036486, 44137866532)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7770751150, 8081581196)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87343242926, 87654072972)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (127751148906, 128061978952)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (277882061124, 278192891170)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (273530440480, 273841270526)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270422140020, 270732970066)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271043800112, 271354630158)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278192891170, 278503721216)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309897555862, 310208385908)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276949570986, 277260401032)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274152100572, 274462930618)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272908780388, 273219610434)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271354630158, 271665460204)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274462930618, 274773760664)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274773760664, 275084590710)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (277571231078, 277882061124)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268557159744, 268867989790)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (269800479928, 270111309974)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276327910894, 276638740940)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275395420756, 275706250802)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142670991114, 142981821160)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (141427670930, 141738500976)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (141738500976, 142049331022)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142360161068, 142670991114)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (146400951666, 146711781712)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142981821160, 143292651206)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144846801436, 145157631482)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144535971390, 144846801436)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46935336946, 47246166992)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (140495180792, 140806010838)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (143292651206, 143603481252)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147333441804, 147644271850)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145157631482, 145468461528)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (146090121620, 146400951666)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (259853918456, 260164748502)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270111309974, 270422140020)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (140184350746, 140495180792)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (139251860608, 139562690654)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (140806010838, 141116840884)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147022611758, 147333441804)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (139562690654, 139873520700)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (141116840884, 141427670930)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (143914311298, 144225141344)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (143603481252, 143914311298)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147955101896, 148265931942)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (146711781712, 147022611758)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295288543700, 295599373746)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148887592034, 149198422080)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (139873520700, 140184350746)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147644271850, 147955101896)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296221033838, 296531863884)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142049331022, 142360161068)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148576761988, 148887592034)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148265931942, 148576761988)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298707674206, 299018504252)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144225141344, 144535971390)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55090 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c79d8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55092 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003621ea0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37176 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c79d8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55094 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37178 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55096 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37182 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000032162f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46196 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55100 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59602 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46198 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55098 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59604 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46200 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55102 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59606 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46202 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c79d8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55104 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361b2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37184 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000322d0d0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46204 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55106 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37186 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43496 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55108 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59608 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46206 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55110 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37188 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43500 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55112 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000321a488>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59610 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43498 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c8510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55114 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361b2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37190 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46208 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c86a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55116 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000369a1e0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59612 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46210 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55118 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37192 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000036af268>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46212 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c76a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.27:55120 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37194 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000322d0d0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43502 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59614 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43504 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37196 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361c2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55092 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c8510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46214 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37198 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361b158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55094 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43506 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361dea0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55096 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c86a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59616 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c8840>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46216 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37200 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361b2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55098 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c8510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43508 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000322d0d0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59618 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031be6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55100 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46218 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59620 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55102 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000369a1e0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43510 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000322d0d0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59624 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55104 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000369a1e0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43512 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59622 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361c2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55106 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000032172f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43514 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003219ea0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59626 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55108 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000322d0d0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43516 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003219ea0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59628 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55110 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43518 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000322d0d0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59630 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55112 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43520 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59632 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55114 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031ca510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43522 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37202 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55116 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218ea0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46220 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37204 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55120 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218ea0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46222 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37206 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55118 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7840>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43524 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb840>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:37208 remote=tcp://192.168.64.232:40247> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:55122 remote=tcp://192.168.64.232:46881> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7840>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46224 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031c7510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:43526 remote=tcp://192.168.64.232:37003> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003218488>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.34:46226 remote=tcp://192.168.64.232:37003> already closed.
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:39425
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44025
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:40617
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:35379
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:41275
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:43969
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:37231
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:39133
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:36813
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:45235
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:33897
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:33701
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:34241
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:40887
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wpbzffs3' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wpbzffs3'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:39277
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:36757
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:46097
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:36667
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:37427
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:39987
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:40015
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:37193
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:44593
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:43251
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:42503
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:39611
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:41889
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:43079
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:42999
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:36597
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:40903
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:36723
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:43295
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44873
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:42157
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:34257
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:35267
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:39789
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:39537
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36895
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:36707
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36701
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:41091
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:33809
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:42885
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:39759
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:40747
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:39141
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:45253
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:41127
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:45939
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33761
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:44935
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:44327
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:39953
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:44785
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35531
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33981
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:39195
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:41779
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:44411
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:37093
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:43203
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:45521
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:34857
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33859
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:36291
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2680g_m4' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2680g_m4'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:46457
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nh8m1o2m' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nh8m1o2m'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xuxo9zvi' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xuxo9zvi'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a59w83lo' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a59w83lo'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-gj64j68t' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-gj64j68t'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ad4k869p' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ad4k869p'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-02iicrl4' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-02iicrl4'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9f4l9w5f' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9f4l9w5f'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7ghch1kx' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7ghch1kx'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:44641
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngxm47ru' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngxm47ru'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:46523
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35777
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:40305
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:43643
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:36627
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7zosrbl4' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7zosrbl4'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-56cb7ien' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-56cb7ien'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-399m9bf_' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-399m9bf_'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-o6gjuz4w' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-o6gjuz4w'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:33341
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-zdgraxw9' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-zdgraxw9'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u_s5d0t1' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u_s5d0t1'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-4cz79xxq' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-4cz79xxq'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:37729
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_cmzijhe' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_cmzijhe'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:45699'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:39997
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.28:34857
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:50332 remote=tcp://192.168.64.28:34857>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70390>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59616 remote=tcp://192.168.64.232:40247> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59616 remote=tcp://192.168.64.232:40247> already closed.
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:33869'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-4p9kui64' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-4p9kui64'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:40903
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uq87krhl' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uq87krhl'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9pxxoj0_' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9pxxoj0_'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:45103'
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nrvj1xot' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nrvj1xot'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-c119cwyo' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-c119cwyo'
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0tvrxi67' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0tvrxi67'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvwr9ueo' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvwr9ueo'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:37927'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:41629
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-buqkk37r' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-buqkk37r'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:41787'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:42375'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:33541'
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.28:43295
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:54432 remote=tcp://192.168.64.28:43295>: Stream is closed
distributed.nanny - INFO - Worker closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4e0>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59610 remote=tcp://192.168.64.232:40247> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59610 remote=tcp://192.168.64.232:40247> already closed.
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:35607'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sr66piow' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sr66piow'
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-347_qh1i' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-347_qh1i'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:43643'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:37241'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33483
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42491'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43619'
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.28:42885
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:45866 remote=tcp://192.168.64.28:42885>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59602 remote=tcp://192.168.64.232:40247> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59602 remote=tcp://192.168.64.232:40247> already closed.
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43871'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:41789
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:45489'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:33119'
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.28:34857 -> tcp://192.168.64.32:37093
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:34857 remote=tcp://192.168.64.32:50332>: Stream is closed
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-psamthfq' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-psamthfq'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.28:43295 -> tcp://192.168.64.32:43203
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:43295 remote=tcp://192.168.64.32:54432>: Stream is closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.28:42885 -> tcp://192.168.64.32:37729
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:42885 remote=tcp://192.168.64.32:45866>: Stream is closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:39287'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:46275'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:45867'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42661'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:39265
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wi35hizl' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wi35hizl'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvtzmdv1' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rvtzmdv1'
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.28:34857
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:50192 remote=tcp://192.168.64.28:34857>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70240>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59606 remote=tcp://192.168.64.232:40247> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59606 remote=tcp://192.168.64.232:40247> already closed.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.28:36291
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:41192 remote=tcp://192.168.64.28:36291>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70240>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59630 remote=tcp://192.168.64.232:40247> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:59630 remote=tcp://192.168.64.232:40247> already closed.
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.28:34857 -> tcp://192.168.64.32:33483
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:34857 remote=tcp://192.168.64.32:50192>: Stream is closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:45879'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:32769'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:42949'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:34391'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.28:36291 -> tcp://192.168.64.32:39997
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.28:36291 remote=tcp://192.168.64.32:41192>: Stream is closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:46745
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:42257
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:36927'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:36069'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:36173'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:41985
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:35723'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42589'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:43283'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:42231'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:33353
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44009'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:34919'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:36327'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:42481'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:36123'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:34131'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:42443'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:39343'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:43647'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:43155'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:38695'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:33617'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:34617'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:38255'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:43437'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:37719
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:44815'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:33169
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:33933'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:34677'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:41157'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:45733
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:37057'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40831'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44961'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:38509'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:46207'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:41129'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42321'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:33875
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42123'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:38895'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36007
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:34467'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:39043'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.27:40025
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:38481
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:46139'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:35545'
distributed.dask_worker - INFO - End worker
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1kfhnywo' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1kfhnywo'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44937'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:41507
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:34465
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39211'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:39685'
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:34525'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39415'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:37027'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:33643
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35433'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:43703'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:43949'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36309'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:37053'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:33993'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:33053'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:36085'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:38089'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:42723'
distributed.dask_worker - INFO - End worker
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
logout
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:32885'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:35997'
logout
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:43973'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:45515'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36193'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36125'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:36163'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:45099'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39897'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:44475'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:44523'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:38207'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35543'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:41863'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:46817'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:38387'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:44213'
distributed.dask_worker - INFO - End worker
logout
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41555'
distributed.dask_worker - INFO - End worker

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920243: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:13:38 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:13:41 2022
                            <40*lassen27>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:13:41 2022
Terminated at Sat Sep 17 19:18:35 2022
Results reported at Sat Sep 17 19:18:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:46881 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.21 sec.
    Max Memory :                                 59 MB
    Average Memory :                             57.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   294 sec.
    Turnaround time :                            297 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920242: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:13:38 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:13:41 2022
                            <40*lassen28>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:13:41 2022
Terminated at Sat Sep 17 19:18:35 2022
Results reported at Sat Sep 17 19:18:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:40247 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.28 sec.
    Max Memory :                                 59 MB
    Average Memory :                             57.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   294 sec.
    Turnaround time :                            297 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920241: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:13:38 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:13:41 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:13:41 2022
Terminated at Sat Sep 17 19:18:35 2022
Results reported at Sat Sep 17 19:18:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37003 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.18 sec.
    Max Memory :                                 59 MB
    Average Memory :                             57.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   294 sec.
    Turnaround time :                            297 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920240: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:13:38 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:13:41 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:13:41 2022
Terminated at Sat Sep 17 19:18:35 2022
Results reported at Sat Sep 17 19:18:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:46881 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.30 sec.
    Max Memory :                                 59 MB
    Average Memory :                             57.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   294 sec.
    Turnaround time :                            297 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920239: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:13:38 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:13:41 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:13:41 2022
Terminated at Sat Sep 17 19:18:35 2022
Results reported at Sat Sep 17 19:18:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:40247 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.29 sec.
    Max Memory :                                 59 MB
    Average Memory :                             57.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   294 sec.
    Turnaround time :                            297 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920238: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:13:38 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:13:41 2022
                            <40*lassen34>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:13:41 2022
Terminated at Sat Sep 17 19:18:35 2022
Results reported at Sat Sep 17 19:18:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37003 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.17 sec.
    Max Memory :                                 59 MB
    Average Memory :                             57.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   294 sec.
    Turnaround time :                            297 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:37361'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:39505'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:33151'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:34335'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:39305'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:38613'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44803'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:33975'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:35427'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44173'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:41305'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:36611'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44747'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:34321'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:46707'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:39765'
distributed.dask_worker - INFO - Timed out starting worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:43265'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:33351'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46081'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45151'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45075'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:44367'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46525'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:34399'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46509'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:41119'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:40899'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45199'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:36337'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:33679'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:37395'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:35401'
distributed.dask_worker - INFO - Timed out starting worker
distributed.dask_worker - INFO - End worker
logout
logout
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:45429'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35089'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41871'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:43963'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:42233'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:32991'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36541'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:39007'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41737'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:39371'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41643'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:40707'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:43965'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36069'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:44937'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:44993'
distributed.dask_worker - INFO - Timed out starting worker
distributed.dask_worker - INFO - End worker

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920254: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:21:46 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:21:49 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:21:49 2022
Terminated at Sat Sep 17 19:23:01 2022
Results reported at Sat Sep 17 19:23:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44357 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.19 sec.
    Max Memory :                                 59 MB
    Average Memory :                             54.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   72 sec.
    Turnaround time :                            75 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920253: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:21:46 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:21:49 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:21:49 2022
Terminated at Sat Sep 17 19:23:01 2022
Results reported at Sat Sep 17 19:23:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:45701 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.16 sec.
    Max Memory :                                 59 MB
    Average Memory :                             54.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   72 sec.
    Turnaround time :                            75 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920252: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:21:46 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:21:49 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:21:49 2022
Terminated at Sat Sep 17 19:23:01 2022
Results reported at Sat Sep 17 19:23:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44157 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.20 sec.
    Max Memory :                                 59 MB
    Average Memory :                             54.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   72 sec.
    Turnaround time :                            75 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38551'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:35089'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39907'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45545'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38093'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46859'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42691'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33567'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:46481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44199'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:32935'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:46235'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:46303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35817'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33739'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37637'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33199'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37341'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45539'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45947'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46533'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46419
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46139
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46419
distributed.worker - INFO -          dashboard at:        192.168.64.32:45421
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46139
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:33103
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_se64f33
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33373
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41471
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42573
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-tn1kny8t
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33373
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41471
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34877
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43447
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42573
distributed.worker - INFO -          dashboard at:        192.168.64.32:36267
distributed.worker - INFO -          dashboard at:        192.168.64.32:42081
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34877
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36571
distributed.worker - INFO -          dashboard at:        192.168.64.29:33389
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45953
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36571
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45953
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:46007
distributed.worker - INFO -          dashboard at:        192.168.64.29:42087
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35081
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gexy82qy
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44985
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35081
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43447
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36881
distributed.worker - INFO -          dashboard at:        192.168.64.30:43231
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36881
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -          dashboard at:        192.168.64.30:32887
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40483
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-pttvujf3
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40483
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l0u059ei
distributed.worker - INFO -          dashboard at:        192.168.64.30:44677
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34095
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44981
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44985
distributed.worker - INFO -          dashboard at:        192.168.64.29:33703
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-bbmug4x7
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45151
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45793
distributed.worker - INFO -          dashboard at:        192.168.64.32:46327
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34095
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:37355
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36203
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44981
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wnnqnwq6
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -          dashboard at:        192.168.64.29:46065
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45151
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45793
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:35027
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44517
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36203
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37633
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:37415
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44517
distributed.worker - INFO -          dashboard at:        192.168.64.29:33547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39297
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:37023
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5wm3mxs3
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37633
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:33611
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39299
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39297
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-np86kfgv
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-11ala5qj
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:39071
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39299
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:44843
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7xj4d_le
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9yar8bsa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-9t8zu641
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:44983
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-zgxuc2kd
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m8srn3mw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rofwxc5x
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45079
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wghm66j_
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45079
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-si2rx1n2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42715
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:34281
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42715
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40433
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jodx37t1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40433
distributed.worker - INFO -          dashboard at:        192.168.64.32:43239
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-xencjsjp
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nv_fcqp6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:42161
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41581
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35691
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41581
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-sjpkie0j
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36541
distributed.worker - INFO -          dashboard at:        192.168.64.29:45167
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36541
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:45501
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -          dashboard at:        192.168.64.29:36941
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35691
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38621
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -          dashboard at:        192.168.64.30:39501
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4sw4vtbl
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:45501
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ke4u4w4l
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38621
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:38117
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:40891
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5sxmxr6p
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-bc91js4d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-na0llcff
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-oswk2o_f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-3_bl4h6z
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-pimgfg48
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36347
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40075
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35693
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40075
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35693
distributed.worker - INFO -          dashboard at:        192.168.64.32:39803
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36347
distributed.worker - INFO -          dashboard at:        192.168.64.32:42451
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42031
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40543
distributed.worker - INFO -          dashboard at:        192.168.64.32:35019
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42031
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -          dashboard at:        192.168.64.32:35131
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37585
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37585
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ox3okx5e
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.32:37509
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37317
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36805
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-f223lx4u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36805
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37317
distributed.worker - INFO -          dashboard at:        192.168.64.32:40567
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ov6715vh
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39497
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -          dashboard at:        192.168.64.30:37399
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39497
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:41545
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-l20kb0m3
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35149
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35149
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:43983
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kmogk6p8
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35107
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40543
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35107
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38485
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-zhe5dmii
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:37015
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38485
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:39969
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:45227
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33857
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33331
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:46879
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33857
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33331
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:46879
distributed.worker - INFO -          dashboard at:        192.168.64.32:42185
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45701
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-o6jktx3q
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:43657
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:45709
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45971
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qptdv3wt
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q7y0llq6
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45971
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gahu6rdd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:36373
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hck7exuh
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-phusa0zo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-909dq7zd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44357
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rus7257f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-jzyvx7rw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-95je30s1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41929
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41929
distributed.worker - INFO -          dashboard at:        192.168.64.30:38117
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43365
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43365
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:37441
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-w_426tj6
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-omlezin8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45701
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44357
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44157
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:35802 remote=tcp://192.168.64.232:44357>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:35810 remote=tcp://192.168.64.232:44357>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:38578 remote=tcp://192.168.64.232:44157>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:38582 remote=tcp://192.168.64.232:44157>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:38584 remote=tcp://192.168.64.232:44157>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:38580 remote=tcp://192.168.64.232:44157>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:38586 remote=tcp://192.168.64.232:44157>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:35794 remote=tcp://192.168.64.232:44357>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Client->Scheduler local=tcp://192.168.64.29:36042 remote=tcp://192.168.64.232:44357>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 95, in _background_send
    payload, serializers=self.serializers, on_error="raise"
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Client->Scheduler local=tcp://192.168.64.29:36048 remote=tcp://192.168.64.232:44357>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 95, in _background_send
    payload, serializers=self.serializers, on_error="raise"
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35776 remote=tcp://192.168.64.232:44357>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 95, in _background_send
    payload, serializers=self.serializers, on_error="raise"
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35774 remote=tcp://192.168.64.232:44357>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 95, in _background_send
    payload, serializers=self.serializers, on_error="raise"
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35752 remote=tcp://192.168.64.232:44357>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 95, in _background_send
    payload, serializers=self.serializers, on_error="raise"
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35752 remote=tcp://192.168.64.232:44357>: Stream is closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (98222294536, 98533124582)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (162564114058, 162874944104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4040790598, 4351620644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204837000314, 205147830360)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 77)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (40718736026, 41029566072)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216648542062, 216959372108)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61233519062, 61544349108)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290936923056, 291247753102)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26731383956, 27042214002)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26109723864, 26420553910)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204526170268, 204837000314)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296221033838, 296531863884)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275084590710, 275395420756)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22379763312, 22690593358)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284098662044, 284409492090)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (167537394794, 167848224840)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (219756842522, 220067672568)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30772174554, 31083004600)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256434787950, 256745617996)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (38853755750, 39164585796)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43205376394, 43516206440)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15541502300, 15852332346)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257056448042, 257367278088)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26420553910, 26731383956)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (192714628520, 193025458566)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210121111096, 210431941142)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32637154830, 32947984876)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208877790912, 209188620958)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (251461507214, 251772337260)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287206962504, 287517792550)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (21758103220, 22068933266)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23623083496, 23933913542)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (205147830360, 205458660406)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228149253764, 228460083810)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199863719578, 200174549624)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (232190044362, 232500874408)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215405221878, 215716051924)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (223175973028, 223486803074)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289693602872, 290004432918)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (66517629844, 66828459890)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47246166992, 47556997038)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12433201840, 12744031886)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261097238640, 261408068686)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274773760664, 275084590710)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296842693930, 297153523976)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276327910894, 276638740940)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (14298182116, 14609012162)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283787831998, 284098662044)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237784985190, 238095815236)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193647118658, 193957948704)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227527593672, 227838423718)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239649965466, 239960795512)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272597950342, 272908780388)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270732970066, 271043800112)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229081743902, 229392573948)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280368701492, 280679531538)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (264205539100, 264516369146)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241825775788, 242136605834)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234365854684, 234676684730)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262029728778, 262340558824)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29839684416, 30150514462)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (183389727140, 183700557186)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196755419118, 197066249164)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29528854370, 29839684416)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283166171906, 283477001952)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161320793874, 161631623920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239028305374, 239339135420)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174686485852, 174997315898)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254569807674, 254880637720)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281612021676, 281922851722)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294977713654, 295288543700)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27042214002, 27353044048)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24244743588, 24555573634)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33258814922, 33569644968)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191160478290, 191471308336)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (273841270526, 274152100572)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49732807360, 50043637406)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35123795198, 35434625244)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33569644968, 33880475014)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203282850084, 203593680130)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179038106496, 179348936542)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201728699854, 202039529900)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221932652844, 222243482890)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247109886570, 247420716616)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206391150544, 206701980590)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275395420756, 275706250802)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225662613396, 225973443442)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230014234040, 230325064086)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (263273048962, 263583879008)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268557159744, 268867989790)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (621660092, 932490138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (269800479928, 270111309974)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179348936542, 179659766588)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253637317536, 253948147582)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278814551262, 279125381308)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191471308336, 191782138382)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (213229411556, 213540241602)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11189881656, 11500711702)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23312253450, 23623083496)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270422140020, 270732970066)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188673837922, 188984667968)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6838261012, 7149091058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187741347784, 188052177830)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197066249164, 197377079210)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284720322136, 285031152182)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267003009514, 267313839560)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165361584472, 165672414518)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27663874094, 27974704140)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203904510176, 204215340222)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209188620958, 209499451004)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289382772826, 289693602872)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (25798893818, 26109723864)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (190538818198, 190849648244)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261718898732, 262029728778)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (34191305060, 34502135106)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199242059486, 199552889532)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244001586110, 244312416156)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13365691978, 13676522024)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (83924112420, 84234942466)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31704664692, 32015494738)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255191467766, 255502297812)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30150514462, 30461344508)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175929806036, 176240636082)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278192891170, 278503721216)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39786245888, 40097075934)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39475415842, 39786245888)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50665297498, 50976127544)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194268778750, 194579608796)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27974704140, 28285534186)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207012810636, 207323640682)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (246799056524, 247109886570)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294356053562, 294666883608)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216337712016, 216648542062)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276949570986, 277260401032)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189295498014, 189606328060)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27353044048, 27663874094)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260786408594, 261097238640)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (164739924380, 165050754426)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290315262964, 290626093010)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17406482576, 17717312622)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (182146406956, 182457237002)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287517792550, 287828622596)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268867989790, 269178819836)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198309569348, 198620399394)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297153523976, 297464354022)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257678108134, 257988938180)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242758265926, 243069095972)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47867827084, 48178657130)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272287120296, 272597950342)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62476839246, 62787669292)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (185254707416, 185565537462)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (243069095972, 243379926018)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193025458566, 193336288612)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56571068372, 56881898418)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75842531224, 76153361270)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261408068686, 261718898732)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279125381308, 279436211354)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92627353708, 92938183754)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207945300774, 208256130820)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 78)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361eea0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45984 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109723006238, 110033836284)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 5)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78640001638, 78950831684)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110966326422, 111277156468)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 6)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38524 remote=tcp://192.168.64.232:44157> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45986 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208256130820, 208566960866)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 78)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35752 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (117182927342, 117493757388)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 12)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53462767912, 53773597958)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38526 remote=tcp://192.168.64.232:44157> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45988 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131481109458, 131791939504)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 23)")'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.29:35812 remote=tcp://192.168.64.232:44357>: ConnectionResetError: [Errno 104] Connection reset by peer
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38528 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51597787636, 51908617682)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb400>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115007117020, 115317947066)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 10)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196444589072, 196755419118)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 73)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45990 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136454390194, 136765220240)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 28)")'

distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 95, in _background_send
    payload, serializers=self.serializers, on_error="raise"
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50354467452, 50665297498)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35756 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38530 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109101346146, 109412176192)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 5)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361a2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45992 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197998739302, 198309569348)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 73)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56881898418, 57192728464)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38532 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106925535824, 107236365870)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 5)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254258977628, 254569807674)
kwargs:    {}
Exception: 'Exception(\'Long error message\', \'Tried sending message after closing.  Status: closed\\nMessage: {\\\'op\\\': \\\'update-graph-hlg\\\', \\\'hlg\\\': {\\\'layers\\\': [{\\\'__module__\\\': \\\'dask.highlevelgraph\\\', \\\'__name__\\\': \\\'MaterializedLayer\\\', \\\'state\\\': {\\\'dsk\\\': {"(\\\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\\\', 95)": <Serialize: (\\\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\\\', 95)>}, \\\'dependencies\\\': {"(\\\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\\\', 95)": {"(\\\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\\\', 95)"}}}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.highlevelgraph\\\', \\\'__name__\\\': \\\'MaterializedLayer\\\', \\\'state\\\': {\\\'dsk\\\': {"(\\\'loc-5a5f243c8c1e3bb3f6a4a3e0d63e9612\\\', 0)": {\\\'function\\\': b\\\'\\\\x80\\\\x04\\\\x95"\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x8c\\\\x16dask.dataframe.methods\\\\x94\\\\x8c\\\\x03loc\\\\x94\\\\x93\\\\x94.\\\', \\\'args\\\': b"\\\\x80\\\\x04\\\\x95\\\\xcf\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x8c3(\\\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\\\', 95)\\\\x94\\\\x8c\\\\x08builtins\\\\x94\\\\x8c\\\\x05slice\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x15numpy.core.multiarray\\\\x94\\\\x8c\\\\x06scalar\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x05numpy\\\\x94\\\\x8c\\\\x05dtype\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x02i8\\\\x94\\\\x89\\\\x88\\\\x87\\\\x94R\\\\x94(K\\\\x03\\\\x8c\\\\x01<\\\\x94NNNJ\\\\xff\\\\xff\\\\xff\\\\xffJ\\\\xff\\\\xff\\\\xff\\\\xffK\\\\x00t\\\\x94bC\\\\x08\\\\\\\\\\\\x1b\\\\x043;\\\\x00\\\\x00\\\\x00\\\\x94\\\\x86\\\\x94R\\\\x94h\\\\x06h\\\\x0cC\\\\x08:\\\\xff\\\\x8aE;\\\\x00\\\\x00\\\\x00\\\\x94\\\\x86\\\\x94R\\\\x94N\\\\x87\\\\x94R\\\\x94N\\\\x87\\\\x94."}}, \\\'dependencies\\\': {"(\\\'loc-5a5f243c8c1e3bb3f6a4a3e0d63e9612\\\', 0)": {"(\\\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\\\', 95)"}}}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.blockwise\\\', \\\'__name__\\\': \\\'Blockwise\\\', \\\'state\\\': {\\\'output\\\': \\\'getitem-7553534b26fd9eda1293920ac5317960\\\', \\\'output_indices\\\': (\\\'.0\\\',), \\\'func\\\': b\\\'\\\\x80\\\\x04\\\\x95\\\\x85\\\\x01\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x8c\\\\x11dask.optimization\\\\x94\\\\x8c\\\\x10SubgraphCallable\\\\x94\\\\x93\\\\x94(}\\\\x94(\\\\x8c(getitem-7553534b26fd9eda1293920ac5317960\\\\x94\\\\x8c+eq-getitem-7553534b26fd9eda1293920ac5317960\\\\x94h\\\\x05\\\\x8c\\\\t_operator\\\\x94\\\\x8c\\\\x07getitem\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13__dask_blockwise__0\\\\x94h\\\\x06\\\\x8c\\\\x02eq\\\\x94\\\\x93\\\\x94h\\\\x08h\\\\t\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x87\\\\x94\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x87\\\\x94\\\\x87\\\\x94\\\\x8c\\\\x13__dask_blockwise__1\\\\x94K\\\\x01\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x8c\\\\x06io_cat\\\\x94uh\\\\x04\\\\x8c\\\\x13__dask_blockwise__0\\\\x94\\\\x85\\\\x94\\\\x8c6subgraph_callable-f7486813-754d-4f5e-8e11-af80e2518000\\\\x94t\\\\x94R\\\\x94.\\\', \\\'func_future_args\\\': (), \\\'global_dependencies\\\': set(), \\\'indices\\\': [\\\'loc-5a5f243c8c1e3bb3f6a4a3e0d63e9612\\\', (\\\'.0\\\',)], \\\'is_list\\\': [False, False], \\\'numblocks\\\': {\\\'loc-5a5f243c8c1e3bb3f6a4a3e0d63e9612\\\': (1,)}, \\\'concatenate\\\': None, \\\'new_axes\\\': {}, \\\'output_blocks\\\': None, \\\'dims\\\': {\\\'.0\\\': 1}, \\\'io_deps\\\': {}}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.blockwise\\\', \\\'__name__\\\': \\\'Blockwise\\\', \\\'state\\\': {\\\'output\\\': \\\'unique-chunk-6c74e8f10630c221b9e0a938f1b04bf0-853ec2282731466892755a262ed53d53\\\', \\\'output_indices\\\': (\\\'.0\\\',), \\\'func\\\': b\\\'\\\\x80\\\\x04\\\\x95I\\\\x02\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x8c\\\\x11dask.optimization\\\\x94\\\\x8c\\\\x10SubgraphCallable\\\\x94\\\\x93\\\\x94(}\\\\x94(\\\\x8cNunique-chunk-6c74e8f10630c221b9e0a938f1b04bf0-853ec2282731466892755a262ed53d53\\\\x94\\\\x8cVgetitem-unique-chunk-6c74e8f10630c221b9e0a938f1b04bf0-853ec2282731466892755a262ed53d53\\\\x94h\\\\x05(\\\\x8c\\\\ndask.utils\\\\x94\\\\x8c\\\\x05apply\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x16dask.dataframe.methods\\\\x94\\\\x8c\\\\x06unique\\\\x94\\\\x93\\\\x94]\\\\x94\\\\x8c\\\\t_operator\\\\x94\\\\x8c\\\\x07getitem\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x87\\\\x94a\\\\x8c\\\\x08builtins\\\\x94\\\\x8c\\\\x04dict\\\\x94\\\\x93\\\\x94]\\\\x94]\\\\x94(\\\\x8c\\\\x0bseries_name\\\\x94\\\\x8c\\\\x04rank\\\\x94ea\\\\x86\\\\x94t\\\\x94\\\\x8c\\\\x13__dask_blockwise__0\\\\x94\\\\x8c(getitem-3e0cdb8bf02b1a4959d1c7dc0b87f54d\\\\x94\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x8c\\\\x04rank\\\\x94uh\\\\x04\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x85\\\\x94\\\\x8c6subgraph_callable-50845210-1cb0-42e4-b64b-ffb1b1bd86a1\\\\x94t\\\\x94R\\\\x94.\\\', \\\'func_future_args\\\': (), \\\'global_dependencies\\\': set(), \\\'indices\\\': [\\\'getitem-7553534b26fd9eda1293920ac5317960\\\', (\\\'.0\\\',)], \\\'is_list\\\': [False, False], \\\'numblocks\\\': {\\\'getitem-7553534b26fd9eda1293920ac5317960\\\': (1,)}, \\\'concatenate\\\': None, \\\'new_axes\\\': {}, \\\'output_blocks\\\': None, \\\'dims\\\': {\\\'.0\\\': 1}, \\\'io_deps\\\': {}}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.layers\\\', \\\'__name__\\\': \\\'DataFrameTreeReduction\\\', \\\'state\\\': {\\\'name\\\': \\\'unique-agg-6c74e8f10630c221b9e0a938f1b04bf0\\\', \\\'name_input\\\': \\\'unique-chunk-6c74e8f10630c221b9e0a938f1b04bf0-853ec2282731466892755a262ed53d53\\\', \\\'npartitions_input\\\': 1, \\\'concat_func\\\': <Serialize: functools.partial(<function _concat at 0x2000175c7730>, ignore_index=False)>, \\\'tree_node_func\\\': <Serialize: functools.partial(<function unique at 0x2000175b7b70>, series_name=\\\'rank\\\')>, \\\'finalize_func\\\': <Serialize: functools.partial(<function unique at 0x2000175b7b70>, series_name=\\\'rank\\\')>, \\\'split_out\\\': None, \\\'output_partitions\\\': [0], \\\'tree_node_name\\\': \\\'unique-combine-6c74e8f10630c221b9e0a938f1b04bf0\\\'}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.blockwise\\\', \\\'__name__\\\': \\\'Blockwise\\\', \\\'state\\\': {\\\'output\\\': \\\'series-count-chunk-6cb98788211e168b7ea8663f515bd7c3-81e63b64907cd6d72dd99f31f771614b\\\', \\\'output_indices\\\': (\\\'.0\\\',), \\\'func\\\': b\\\'\\\\x80\\\\x04\\\\x95s\\\\x02\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x8c\\\\x11dask.optimization\\\\x94\\\\x8c\\\\x10SubgraphCallable\\\\x94\\\\x93\\\\x94(}\\\\x94(\\\\x8cTseries-count-chunk-6cb98788211e168b7ea8663f515bd7c3-81e63b64907cd6d72dd99f31f771614b\\\\x94\\\\x8c\\\\\\\\getitem-series-count-chunk-6cb98788211e168b7ea8663f515bd7c3-81e63b64907cd6d72dd99f31f771614b\\\\x94h\\\\x05(\\\\x8c\\\\ndask.utils\\\\x94\\\\x8c\\\\x05apply\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13dask.dataframe.core\\\\x94\\\\x8c\\\\x10_reduction_chunk\\\\x94\\\\x93\\\\x94]\\\\x94\\\\x8c\\\\t_operator\\\\x94\\\\x8c\\\\x07getitem\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x87\\\\x94a\\\\x8c\\\\x08builtins\\\\x94\\\\x8c\\\\x04dict\\\\x94\\\\x93\\\\x94]\\\\x94]\\\\x94(\\\\x8c\\\\taca_chunk\\\\x94h\\\\x06\\\\x8c\\\\x0cmethodcaller\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x05count\\\\x94\\\\x85\\\\x94R\\\\x94ea\\\\x86\\\\x94t\\\\x94\\\\x8c\\\\x13__dask_blockwise__0\\\\x94\\\\x8c(getitem-3de9a7225ccb1cfa9131f6ad56e30ef3\\\\x94\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x8c\\\\x05index\\\\x94uh\\\\x04\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x85\\\\x94\\\\x8c6subgraph_callable-1e41a676-1c30-404b-bdee-517809f713dc\\\\x94t\\\\x94R\\\\x94.\\\', \\\'func_future_args\\\': (), \\\'global_dependencies\\\': set(), \\\'indices\\\': [\\\'getitem-7553534b26fd9eda1293920ac5317960\\\', (\\\'.0\\\',)], \\\'is_list\\\': [False, False], \\\'numblocks\\\': {\\\'getitem-7553534b26fd9eda1293920ac5317960\\\': (1,)}, \\\'concatenate\\\': None, \\\'new_axes\\\': {}, \\\'output_blocks\\\': None, \\\'dims\\\': {\\\'.0\\\': 1}, \\\'io_deps\\\': {}}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.layers\\\', \\\'__name__\\\': \\\'DataFrameTreeReduction\\\', \\\'state\\\': {\\\'name\\\': \\\'series-count-agg-6cb98788211e168b7ea8663f515bd7c3\\\', \\\'name_input\\\': \\\'series-count-chunk-6cb98788211e168b7ea8663f515bd7c3-81e63b64907cd6d72dd99f31f771614b\\\', \\\'npartitions_input\\\': 1, \\\'concat_func\\\': <Serialize: functools.partial(<function _concat at 0x2000175c7730>, ignore_index=False)>, \\\'tree_node_func\\\': <Serialize: functools.partial(<function _reduction_combine at 0x2000176190d0>, aca_combine=<function _count_aggregate at 0x200017619488>)>, \\\'finalize_func\\\': <Serialize: functools.partial(<function _reduction_aggregate at 0x200017619158>, aca_aggregate=<function _count_aggregate at 0x200017619488>)>, \\\'split_out\\\': None, \\\'output_partitions\\\': [0], \\\'tree_node_name\\\': \\\'series-count-combine-6cb98788211e168b7ea8663f515bd7c3\\\'}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.blockwise\\\', \\\'__name__\\\': \\\'Blockwise\\\', \\\'state\\\': {\\\'output\\\': \\\'series-sum-chunk-250234613cc7565a42718506d5237dda-cfc0fa686518540833f06d899c6f21e2\\\', \\\'output_indices\\\': (\\\'.0\\\',), \\\'func\\\': b\\\'\\\\x80\\\\x04\\\\x95\\\\x8d\\\\x02\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x8c\\\\x11dask.optimization\\\\x94\\\\x8c\\\\x10SubgraphCallable\\\\x94\\\\x93\\\\x94(}\\\\x94(\\\\x8cRseries-sum-chunk-250234613cc7565a42718506d5237dda-cfc0fa686518540833f06d899c6f21e2\\\\x94\\\\x8cZgetitem-series-sum-chunk-250234613cc7565a42718506d5237dda-cfc0fa686518540833f06d899c6f21e2\\\\x94h\\\\x05(\\\\x8c\\\\ndask.utils\\\\x94\\\\x8c\\\\x05apply\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13dask.dataframe.core\\\\x94\\\\x8c\\\\x10_reduction_chunk\\\\x94\\\\x93\\\\x94]\\\\x94\\\\x8c\\\\t_operator\\\\x94\\\\x8c\\\\x07getitem\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x87\\\\x94a\\\\x8c\\\\x08builtins\\\\x94\\\\x8c\\\\x04dict\\\\x94\\\\x93\\\\x94]\\\\x94(]\\\\x94(\\\\x8c\\\\taca_chunk\\\\x94h\\\\x06\\\\x8c\\\\x0cmethodcaller\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x03sum\\\\x94\\\\x85\\\\x94R\\\\x94e]\\\\x94(\\\\x8c\\\\x06skipna\\\\x94\\\\x88e]\\\\x94(\\\\x8c\\\\x04axis\\\\x94K\\\\x00ee\\\\x86\\\\x94t\\\\x94\\\\x8c\\\\x13__dask_blockwise__0\\\\x94\\\\x8c(getitem-38dc9d5a5b8dd97e55854c510878dfe3\\\\x94\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x8c\\\\tbandwidth\\\\x94uh\\\\x04\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x85\\\\x94\\\\x8c6subgraph_callable-0553cf5e-7e59-4211-b812-75172d95dd6c\\\\x94t\\\\x94R\\\\x94.\\\', \\\'func_future_args\\\': (), \\\'global_dependencies\\\': set(), \\\'indices\\\': [\\\'getitem-7553534b26fd9eda1293920ac5317960\\\', (\\\'.0\\\',)], \\\'is_list\\\': [False, False], \\\'numblocks\\\': {\\\'getitem-7553534b26fd9eda1293920ac5317960\\\': (1,)}, \\\'concatenate\\\': None, \\\'new_axes\\\': {}, \\\'output_blocks\\\': None, \\\'dims\\\': {\\\'.0\\\': 1}, \\\'io_deps\\\': {}}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.layers\\\', \\\'__name__\\\': \\\'DataFrameTreeReduction\\\', \\\'state\\\': {\\\'name\\\': \\\'series-sum-agg-250234613cc7565a42718506d5237dda\\\', \\\'name_input\\\': \\\'series-sum-chunk-250234613cc7565a42718506d5237dda-cfc0fa686518540833f06d899c6f21e2\\\', \\\'npartitions_input\\\': 1, \\\'concat_func\\\': <Serialize: functools.partial(<function _concat at 0x2000175c7730>, ignore_index=False)>, \\\'tree_node_func\\\': <Serialize: functools.partial(<function _reduction_combine at 0x2000176190d0>, aca_combine=<methodcaller: sum>, skipna=True, axis=0)>, \\\'finalize_func\\\': <Serialize: functools.partial(<function _reduction_aggregate at 0x200017619158>, aca_aggregate=<methodcaller: sum>, skipna=True, axis=0)>, \\\'split_out\\\': None, \\\'output_partitions\\\': [0], \\\'tree_node_name\\\': \\\'series-sum-combine-250234613cc7565a42718506d5237dda\\\'}, \\\'annotations\\\': {}}, {\\\'__module__\\\': \\\'dask.blockwise\\\', \\\'__name__\\\': \\\'Blockwise\\\', \\\'state\\\': {\\\'output\\\': \\\'series-sum-chunk-f1ed8cbd0ca2f2561828926cb06bb43e-f35c60add0a94c5bad1d383b383ad409\\\', \\\'output_indices\\\': (\\\'.0\\\',), \\\'func\\\': b\\\'\\\\x80\\\\x04\\\\x95\\\\x88\\\\x02\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x8c\\\\x11dask.optimization\\\\x94\\\\x8c\\\\x10SubgraphCallable\\\\x94\\\\x93\\\\x94(}\\\\x94(\\\\x8cRseries-sum-chunk-f1ed8cbd0ca2f2561828926cb06bb43e-f35c60add0a94c5bad1d383b383ad409\\\\x94\\\\x8cZgetitem-series-sum-chunk-f1ed8cbd0ca2f2561828926cb06bb43e-f35c60add0a94c5bad1d383b383ad409\\\\x94h\\\\x05(\\\\x8c\\\\ndask.utils\\\\x94\\\\x8c\\\\x05apply\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13dask.dataframe.core\\\\x94\\\\x8c\\\\x10_reduction_chunk\\\\x94\\\\x93\\\\x94]\\\\x94\\\\x8c\\\\t_operator\\\\x94\\\\x8c\\\\x07getitem\\\\x94\\\\x93\\\\x94\\\\x8c\\\\x13__dask_blockwise__1\\\\x94\\\\x8c\\\\x13__dask_blockwise__2\\\\x94\\\\x87\\\\x94a\\\\x8c\\\\x08\')'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193957948704, 194268778750)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 70)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45994 remote=tcp://192.168.64.232:45701> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41029566072, 41340396118)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38534 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241204115696, 241514945742)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 91)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105060555548, 105371385594)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 4)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45996 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179970596634, 180281426680)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 63)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48800317222, 49111147268)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229703403994, 230014234040)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 87)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb620>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35764 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152306722540, 152617552586)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 40)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bbae8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361b2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38536 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (180281426680, 180592256726)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 63)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111898816560, 112209646606)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 6)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169402375070, 169713205116)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 54)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:45998 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152928382632, 153239212678)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 41)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35762 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112209646606, 112520476652)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 6)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187119687692, 187430517738)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 67)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38538 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165983244564, 166294074610)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 51)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361b2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46000 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113763796836, 114074626882)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 8)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154482532862, 154793362908)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 42)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184633047324, 184943877370)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 65)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36677945428, 36988775474)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38540 remote=tcp://192.168.64.232:44157> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46002 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113142136744, 113452966790)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 7)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157280003276, 157590833322)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 45)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50043637406, 50354467452)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173753995714, 174064825760)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 57)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38542 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111587986514, 111898816560)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 6)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361ad08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46004 remote=tcp://192.168.64.232:45701> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115628777112, 115939607158)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 11)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (143292651206, 143603481252)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 32)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160388303736, 160699133782)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 47)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361ad08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38544 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116872097296, 117182927342)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 12)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46006 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126818658768, 127129488814)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 20)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35766 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147333441804, 147644271850)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 36)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160077473690, 160388303736)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 47)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119047907618, 119358737664)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 13)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35772 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149509252126, 149820082172)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 38)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003691048>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38546 remote=tcp://192.168.64.232:44157> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361a2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46008 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115939607158, 116250437204)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 11)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163807434242, 164118264288)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 49)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151374232402, 151685062448)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 39)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361ad08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35774 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38548 remote=tcp://192.168.64.232:44157> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46010 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116561267250, 116872097296)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 12)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (153860872770, 154171702816)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 41)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31393834646, 31704664692)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35776 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155104192954, 155415023000)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 43)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (108168856008, 108479686054)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 5)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb598>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38550 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32326324784, 32637154830)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46012 remote=tcp://192.168.64.232:45701> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158212493414, 158523323460)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 46)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105371385594, 105682215640)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 4)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38552 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33880475014, 34191305060)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.32:46014 remote=tcp://192.168.64.232:45701> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35778 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101330594996, 101641425042)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157590833322, 157901663368)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 45)")'

tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:38554 remote=tcp://192.168.64.232:44157> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28285534186, 28596364232)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99776444766, 100087274812)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142049331022, 142360161068)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 31)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101952255088, 102263085134)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (141738500976, 142049331022)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 30)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23001423404, 23312253450)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142981821160, 143292651206)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 32)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (104128065410, 104438895456)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 2)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24555573634, 24866403680)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145468461528, 145779291574)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 35)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (134589409918, 134900239964)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 26)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22068933266, 22379763312)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125575338584, 125886168630)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 19)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (19893122944, 20203952990)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151995892494, 152306722540)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 40)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7149091058, 7459921104)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147644271850, 147955101896)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 37)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148887592034, 149198422080)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 37)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5594940828, 5905770874)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4662450690, 4973280736)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4351620644, 4662450690)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (16473992438, 16784822484)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18649802760, 18960632806)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17717312622, 18028142668)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64341819522, 64652649568)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74599211040, 74910041086)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78018341546, 78329171592)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74288380994, 74599211040)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51286957590, 51597787636)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58436048648, 58746878694)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57503558510, 57814388556)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48178657130, 48489487176)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48489487176, 48800317222)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82369962190, 82680792236)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112831306698, 113142136744)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 6)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106614705778, 106925535824)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 5)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100708934904, 101019764950)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (54395258050, 54706088096)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100087274812, 100398104858)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (103195575272, 103506405318)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132102769550, 132413599596)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 23)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136143560148, 136454390194)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 28)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136765220240, 137076050286)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 28)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138008540424, 138319370470)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 29)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126196998676, 126507828722)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 20)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122777868170, 123088698216)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 16)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154171702816, 154482532862)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 42)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155725853046, 156036683092)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 43)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156969173230, 157280003276)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 45)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142360161068, 142670991114)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 31)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (146711781712, 147022611758)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 36)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286585302412, 286896132458)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286896132458, 287206962504)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285963642320, 286274472366)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285031152182, 285341982228)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285341982228, 285652812274)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285652812274, 285963642320)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284409492090, 284720322136)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283477001952, 283787831998)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282544511814, 282855341860)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282233681768, 282544511814)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282855341860, 283166171906)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281301191630, 281612021676)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280990361584, 281301191630)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281922851722, 282233681768)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279747041400, 280057871446)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290626093010, 290936923056)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290004432918, 290315262964)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288450282688, 288761112734)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292801903332, 293112733378)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291558583148, 291869413194)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295599373746, 295910203792)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295288543700, 295599373746)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308654235678, 308965065724)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300261824436, 300572654482)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307100085448, 307410915494)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309275895770, 309586725816)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303370124896, 303680954942)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (293734393470, 294045223516)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313627516414, 313938346460)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301194314574, 301505144620)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304924275126, 305235105172)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312073366184, 312384196230)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310830046000, 311140876046)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295910203792, 296221033838)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299950994390, 300261824436)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306167595310, 306478425356)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314249176506, 314560006552)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317357476966, 317668307012)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313005856322, 313316686368)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299329334298, 299640164344)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297775184068, 298086014114)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306478425356, 306789255402)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301505144620, 301815974666)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301815974666, 302126804712)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298396844160, 298707674206)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304302615034, 304613445080)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298086014114, 298396844160)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303680954942, 303991784988)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316424986828, 316735816874)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317046646920, 317357476966)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317979137058, 318289967104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308343405632, 308654235678)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313938346460, 314249176506)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310208385908, 310519215954)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297464354022, 297775184068)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302126804712, 302437634758)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302437634758, 302748464804)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299018504252, 299329334298)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306789255402, 307100085448)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231879214316, 232190044362)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 88)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316114156782, 316424986828)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316735816874, 317046646920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314870836598, 315181666644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315492496690, 315803326736)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313316686368, 313627516414)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303059294850, 303370124896)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300572654482, 300883484528)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314560006552, 314870836598)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174064825760, 174375655806)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 58)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315181666644, 315492496690)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312384196230, 312695026276)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300883484528, 301194314574)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309897555862, 310208385908)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305545935218, 305856765264)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307721745540, 308032575586)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5284110782, 5594940828)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304613445080, 304924275126)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308032575586, 308343405632)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311140876046, 311451706092)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303991784988, 304302615034)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305856765264, 306167595310)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310519215954, 310830046000)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9635731426, 9946561472)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311451706092, 311762536138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299640164344, 299950994390)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305235105172, 305545935218)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315803326736, 316114156782)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309586725816, 309897555862)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312695026276, 313005856322)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311762536138, 312073366184)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302748464804, 303059294850)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298707674206, 299018504252)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307410915494, 307721745540)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317668307012, 317979137058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308965065724, 309275895770)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 185, comm: 0, waiting: 0>>, <TaskState 'filter-3ba6e441-e3ff-431d-bcd1-551a83265f9a' long-running>, compute_duration=0.6132452487945557, stimulus_id='secede-filter-3ba6e441-e3ff-431d-bcd1-551a83265f9a-1663467883.7808378')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 178, comm: 0, waiting: 0>>, <TaskState 'filter-47059cbc-a873-40d3-b350-44a00e25a201' long-running>, compute_duration=0.027143001556396484, stimulus_id='secede-filter-47059cbc-a873-40d3-b350-44a00e25a201-1663467883.811583')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 171, comm: 0, waiting: 0>>, <TaskState 'filter-4fb37719-b857-4939-ab62-21ed9d6f15db' long-running>, compute_duration=0.030697345733642578, stimulus_id='secede-filter-4fb37719-b857-4939-ab62-21ed9d6f15db-1663467883.844137')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 169, comm: 0, waiting: 0>>, <TaskState 'filter-290041c1-f321-4503-9b81-54b16a5da824' long-running>, compute_duration=0.030255556106567383, stimulus_id='secede-filter-290041c1-f321-4503-9b81-54b16a5da824-1663467883.8755586')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 139, comm: 0, waiting: 0>>, <TaskState 'filter-bdc13ad5-3175-4c61-9c8c-91c5ad5ce54c' long-running>, compute_duration=0.02704167366027832, stimulus_id='secede-filter-bdc13ad5-3175-4c61-9c8c-91c5ad5ce54c-1663467883.9038854')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 137, comm: 0, waiting: 0>>, <TaskState 'filter-d4f5d2e0-39bf-4d90-b23b-f551cb3888e5' long-running>, compute_duration=0.04331398010253906, stimulus_id='secede-filter-d4f5d2e0-39bf-4d90-b23b-f551cb3888e5-1663467883.9483001')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 136, comm: 0, waiting: 0>>, <TaskState 'filter-1e62bd49-e6a3-42c3-9f9a-ad1850bab306' long-running>, compute_duration=0.026470422744750977, stimulus_id='secede-filter-1e62bd49-e6a3-42c3-9f9a-ad1850bab306-1663467883.9761665')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 130, comm: 0, waiting: 0>>, <TaskState 'filter-2d5bb04d-e454-4e65-8548-7984a119d16a' long-running>, compute_duration=0.029963254928588867, stimulus_id='secede-filter-2d5bb04d-e454-4e65-8548-7984a119d16a-1663467884.0074394')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 119, comm: 0, waiting: 0>>, <TaskState 'filter-e5e2b342-1604-4557-b839-3abbd3ec2532' long-running>, compute_duration=0.030226469039916992, stimulus_id='secede-filter-e5e2b342-1604-4557-b839-3abbd3ec2532-1663467884.0396743')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 118, comm: 0, waiting: 0>>, <TaskState 'filter-efaa9269-8987-4f03-88a3-95264009eace' long-running>, compute_duration=0.02792501449584961, stimulus_id='secede-filter-efaa9269-8987-4f03-88a3-95264009eace-1663467884.0687416')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 115, comm: 0, waiting: 0>>, <TaskState 'filter-16d2b3ac-ec8c-4108-9945-24625594493c' long-running>, compute_duration=0.030270814895629883, stimulus_id='secede-filter-16d2b3ac-ec8c-4108-9945-24625594493c-1663467884.100181')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 169, comm: 0, waiting: 1>>, <TaskState 'filter-7f0569f1-d07a-4bb8-ad93-d1a8ee318358' long-running>, compute_duration=0.6376407146453857, stimulus_id='secede-filter-7f0569f1-d07a-4bb8-ad93-d1a8ee318358-1663467884.1121888')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 114, comm: 0, waiting: 0>>, <TaskState 'filter-b0d7a516-7da6-4f80-a40b-070a060a2ad4' long-running>, compute_duration=0.02924513816833496, stimulus_id='secede-filter-b0d7a516-7da6-4f80-a40b-070a060a2ad4-1663467884.130548')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 166, comm: 0, waiting: 1>>, <TaskState 'filter-0b74fa5a-83c6-46b7-8a7a-753dd4623e02' long-running>, compute_duration=0.03091287612915039, stimulus_id='secede-filter-0b74fa5a-83c6-46b7-8a7a-753dd4623e02-1663467884.1470513')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 106, comm: 0, waiting: 0>>, <TaskState 'filter-915352ae-63e4-4fda-a698-9e4d5e3b395c' long-running>, compute_duration=0.030200719833374023, stimulus_id='secede-filter-915352ae-63e4-4fda-a698-9e4d5e3b395c-1663467884.162023')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 161, comm: 0, waiting: 1>>, <TaskState 'filter-c286db66-6cd7-419b-822d-b26f277890dc' long-running>, compute_duration=0.030856609344482422, stimulus_id='secede-filter-c286db66-6cd7-419b-822d-b26f277890dc-1663467884.1825104')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 104, comm: 0, waiting: 0>>, <TaskState 'filter-227604a3-7830-4aad-9b89-649a839f063d' long-running>, compute_duration=0.02914714813232422, stimulus_id='secede-filter-227604a3-7830-4aad-9b89-649a839f063d-1663467884.192288')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 157, comm: 0, waiting: 1>>, <TaskState 'filter-ce97ccac-0aea-41f3-b62a-156eaa85f3af' long-running>, compute_duration=0.02674412727355957, stimulus_id='secede-filter-ce97ccac-0aea-41f3-b62a-156eaa85f3af-1663467884.211263')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 99, comm: 0, waiting: 0>>, <TaskState 'filter-94335bfd-1ddc-4e86-8858-830791d9b132' long-running>, compute_duration=0.031859397888183594, stimulus_id='secede-filter-94335bfd-1ddc-4e86-8858-830791d9b132-1663467884.2253995')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 151, comm: 0, waiting: 1>>, <TaskState 'filter-61773e44-4a48-4e87-9bbe-9c5e86f2bb78' long-running>, compute_duration=0.027448415756225586, stimulus_id='secede-filter-61773e44-4a48-4e87-9bbe-9c5e86f2bb78-1663467884.242061')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 91, comm: 0, waiting: 0>>, <TaskState 'filter-d3c92a3b-da15-4fa3-9f30-2b19b37878f8' long-running>, compute_duration=0.02816486358642578, stimulus_id='secede-filter-d3c92a3b-da15-4fa3-9f30-2b19b37878f8-1663467884.2548804')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 145, comm: 0, waiting: 1>>, <TaskState 'filter-7a5e00e8-3217-45b0-ae81-c2487c092365' long-running>, compute_duration=0.03336834907531738, stimulus_id='secede-filter-7a5e00e8-3217-45b0-ae81-c2487c092365-1663467884.2790449')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 79, comm: 0, waiting: 0>>, <TaskState 'filter-2c05d097-0960-4365-a2e5-c602743ea6f1' long-running>, compute_duration=0.030152320861816406, stimulus_id='secede-filter-2c05d097-0960-4365-a2e5-c602743ea6f1-1663467884.286465')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 141, comm: 0, waiting: 1>>, <TaskState 'filter-3b1ae86a-cbbf-4bdc-b343-541b171e7333' long-running>, compute_duration=0.027382612228393555, stimulus_id='secede-filter-3b1ae86a-cbbf-4bdc-b343-541b171e7333-1663467884.3092782')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 75, comm: 0, waiting: 0>>, <TaskState 'filter-1d99bd57-1e14-4fe2-a7cc-65c2eab4216e' long-running>, compute_duration=0.027549028396606445, stimulus_id='secede-filter-1d99bd57-1e14-4fe2-a7cc-65c2eab4216e-1663467884.3170946')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 136, comm: 0, waiting: 1>>, <TaskState 'filter-5d2c7bd6-279b-4fa6-acb7-cd6bc97b3999' long-running>, compute_duration=0.0287930965423584, stimulus_id='secede-filter-5d2c7bd6-279b-4fa6-acb7-cd6bc97b3999-1663467884.3404458')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 69, comm: 0, waiting: 0>>, <TaskState 'filter-a32521dc-f1bf-4316-bd05-a2741dc4b614' long-running>, compute_duration=0.03366374969482422, stimulus_id='secede-filter-a32521dc-f1bf-4316-bd05-a2741dc4b614-1663467884.353228')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 113, comm: 0, waiting: 1>>, <TaskState 'filter-593f0bf3-295e-46d5-8198-944c8406c3a2' long-running>, compute_duration=0.03072357177734375, stimulus_id='secede-filter-593f0bf3-295e-46d5-8198-944c8406c3a2-1663467884.37368')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 67, comm: 0, waiting: 0>>, <TaskState 'filter-7d0ef9a7-c1d7-465f-a42f-797160fea466' long-running>, compute_duration=0.028818607330322266, stimulus_id='secede-filter-7d0ef9a7-c1d7-465f-a42f-797160fea466-1663467884.3832786')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 58, comm: 0, waiting: 0>>, <TaskState 'filter-b132ec9a-b07d-41c7-a696-62fc0f442f4c' long-running>, compute_duration=0.032209157943725586, stimulus_id='secede-filter-b132ec9a-b07d-41c7-a696-62fc0f442f4c-1663467884.4168603')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 91, comm: 0, waiting: 1>>, <TaskState 'filter-52a71158-0c51-4842-af61-02bafb3edba9' long-running>, compute_duration=0.03831076622009277, stimulus_id='secede-filter-52a71158-0c51-4842-af61-02bafb3edba9-1663467884.417885')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 57, comm: 0, waiting: 0>>, <TaskState 'filter-c1cddec7-3e7b-4dc5-871d-6d07ce6da7ea' long-running>, compute_duration=0.02751755714416504, stimulus_id='secede-filter-c1cddec7-3e7b-4dc5-871d-6d07ce6da7ea-1663467884.4454577')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 73, comm: 0, waiting: 1>>, <TaskState 'filter-f3e91a8f-26be-4272-b11e-b4cde6e306f5' long-running>, compute_duration=0.029526233673095703, stimulus_id='secede-filter-f3e91a8f-26be-4272-b11e-b4cde6e306f5-1663467884.4511805')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 50, comm: 0, waiting: 0>>, <TaskState 'filter-ccfd9e5f-3bff-4cd8-ad7d-76c9367212cc' long-running>, compute_duration=0.03602337837219238, stimulus_id='secede-filter-ccfd9e5f-3bff-4cd8-ad7d-76c9367212cc-1663467884.4826248')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 72, comm: 0, waiting: 1>>, <TaskState 'filter-fda47faf-3191-4ca1-babc-37a9a4809377' long-running>, compute_duration=0.036316633224487305, stimulus_id='secede-filter-fda47faf-3191-4ca1-babc-37a9a4809377-1663467884.4908378')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 46, comm: 0, waiting: 0>>, <TaskState 'filter-44abe501-8eb2-4195-9813-d0a9245f021e' long-running>, compute_duration=0.030818939208984375, stimulus_id='secede-filter-44abe501-8eb2-4195-9813-d0a9245f021e-1663467884.5146863')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 59, comm: 0, waiting: 1>>, <TaskState 'filter-9066acd3-833e-48cc-8951-04fb4cdb6ca5' long-running>, compute_duration=0.033724308013916016, stimulus_id='secede-filter-9066acd3-833e-48cc-8951-04fb4cdb6ca5-1663467884.528094')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 45, comm: 0, waiting: 0>>, <TaskState 'filter-d3c8cb66-c9c8-4df1-8458-abdba2bd32ba' long-running>, compute_duration=0.04073333740234375, stimulus_id='secede-filter-d3c8cb66-c9c8-4df1-8458-abdba2bd32ba-1663467884.5565991')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 53, comm: 0, waiting: 1>>, <TaskState 'filter-133959fc-598b-47ca-8599-da75de63c408' long-running>, compute_duration=0.02855658531188965, stimulus_id='secede-filter-133959fc-598b-47ca-8599-da75de63c408-1663467884.558455')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 120, comm: 0, waiting: 5>>, <TaskState 'filter-5e892267-25a3-4b19-92d4-beaed0e578a5' long-running>, compute_duration=0.5629673004150391, stimulus_id='secede-filter-5e892267-25a3-4b19-92d4-beaed0e578a5-1663467884.5612233')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 78, running: 0/1, ready: 41, comm: 0, waiting: 0>>, <TaskState 'filter-49145b30-903d-42b1-b838-3d088664a75b' long-running>, compute_duration=0.03281855583190918, stimulus_id='secede-filter-49145b30-903d-42b1-b838-3d088664a75b-1663467884.5906072')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 48, comm: 0, waiting: 1>>, <TaskState 'filter-5048f760-4257-4e95-b6db-6bc3820ec625' long-running>, compute_duration=0.029892921447753906, stimulus_id='secede-filter-5048f760-4257-4e95-b6db-6bc3820ec625-1663467884.5927598')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 116, comm: 0, waiting: 5>>, <TaskState 'filter-5a829c91-45cb-4dbc-8d37-625e8301ff11' long-running>, compute_duration=0.029760122299194336, stimulus_id='secede-filter-5a829c91-45cb-4dbc-8d37-625e8301ff11-1663467884.5924985')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 115, comm: 0, waiting: 5>>, <TaskState 'filter-13e28f58-d1d8-4f65-9fd1-aba159304a94' long-running>, compute_duration=0.03242182731628418, stimulus_id='secede-filter-13e28f58-d1d8-4f65-9fd1-aba159304a94-1663467884.627219')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 24, comm: 0, waiting: 1>>, <TaskState 'filter-f246d920-41ca-45e3-86f2-e22dc405f768' long-running>, compute_duration=0.03428196907043457, stimulus_id='secede-filter-f246d920-41ca-45e3-86f2-e22dc405f768-1663467884.6291277')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 79, running: 0/1, ready: 38, comm: 0, waiting: 0>>, <TaskState 'filter-90be4e25-e368-4830-b488-38163f1b421d' long-running>, compute_duration=0.03695988655090332, stimulus_id='secede-filter-90be4e25-e368-4830-b488-38163f1b421d-1663467884.6309333')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 114, comm: 0, waiting: 5>>, <TaskState 'filter-33f9d625-276a-4da7-9fa8-1a0cf93219a8' long-running>, compute_duration=0.02887105941772461, stimulus_id='secede-filter-33f9d625-276a-4da7-9fa8-1a0cf93219a8-1663467884.6574662')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 10, comm: 0, waiting: 1>>, <TaskState 'filter-9bf3e0ea-aefd-424b-8572-6280c53cceaf' long-running>, compute_duration=0.032106637954711914, stimulus_id='secede-filter-9bf3e0ea-aefd-424b-8572-6280c53cceaf-1663467884.6630356')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 79, running: 0/1, ready: 33, comm: 0, waiting: 0>>, <TaskState 'filter-7a800d9d-ccbe-49a5-9c00-3206735a9adf' long-running>, compute_duration=0.030940771102905273, stimulus_id='secede-filter-7a800d9d-ccbe-49a5-9c00-3206735a9adf-1663467884.6637893')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 113, comm: 0, waiting: 5>>, <TaskState 'filter-759c6456-4150-4669-bd7e-9b9bef985c12' long-running>, compute_duration=0.02740192413330078, stimulus_id='secede-filter-759c6456-4150-4669-bd7e-9b9bef985c12-1663467884.6891603')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 79, running: 0/1, ready: 19, comm: 0, waiting: 0>>, <TaskState 'filter-6cce6a58-654a-48dc-af0a-ecb9f4e7f494' long-running>, compute_duration=0.03154587745666504, stimulus_id='secede-filter-6cce6a58-654a-48dc-af0a-ecb9f4e7f494-1663467884.6965458')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 8, comm: 0, waiting: 1>>, <TaskState 'filter-c7f66627-f021-4828-9f0f-3e5d1a2d37b7' long-running>, compute_duration=0.037529945373535156, stimulus_id='secede-filter-c7f66627-f021-4828-9f0f-3e5d1a2d37b7-1663467884.7025578')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 112, comm: 0, waiting: 5>>, <TaskState 'filter-ab7c49e7-1cd7-4014-8fa3-a13e6a7d4ecb' long-running>, compute_duration=0.03294873237609863, stimulus_id='secede-filter-ab7c49e7-1cd7-4014-8fa3-a13e6a7d4ecb-1663467884.7233677')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 79, running: 0/1, ready: 7, comm: 0, waiting: 0>>, <TaskState 'filter-7faf1b1b-247e-4812-b42f-9bafd4051f2c' long-running>, compute_duration=0.03386354446411133, stimulus_id='secede-filter-7faf1b1b-247e-4812-b42f-9bafd4051f2c-1663467884.7315643')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45953', name: LSFCluster-1-12, status: running, stored: 57, running: 0/1, ready: 7, comm: 0, waiting: 1>>, <TaskState 'filter-0f7ba7e7-d953-4416-988f-a8ac96ad9865' long-running>, compute_duration=0.031005382537841797, stimulus_id='secede-filter-0f7ba7e7-d953-4416-988f-a8ac96ad9865-1663467884.7355485')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35754 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 111, comm: 0, waiting: 5>>, <TaskState 'filter-03950d20-07a8-427e-9e6d-ff84ed2037d6' long-running>, compute_duration=0.029208898544311523, stimulus_id='secede-filter-03950d20-07a8-427e-9e6d-ff84ed2037d6-1663467884.755351')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45793', name: LSFCluster-1-4, status: running, stored: 79, running: 0/1, ready: 5, comm: 0, waiting: 0>>, <TaskState 'filter-1d3883b9-c5e3-4def-8f4a-c6449c1ef5cb' long-running>, compute_duration=0.04672718048095703, stimulus_id='secede-filter-1d3883b9-c5e3-4def-8f4a-c6449c1ef5cb-1663467884.779434')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35760 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 109, comm: 0, waiting: 5>>, <TaskState 'filter-115be554-67ca-47bc-a5c6-00cb4f6d7958' long-running>, compute_duration=0.034464120864868164, stimulus_id='secede-filter-115be554-67ca-47bc-a5c6-00cb4f6d7958-1663467884.7910945')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 107, comm: 0, waiting: 5>>, <TaskState 'filter-d43c9afd-3b43-4f36-a693-6ceb1cba0fa5' long-running>, compute_duration=0.028564929962158203, stimulus_id='secede-filter-d43c9afd-3b43-4f36-a693-6ceb1cba0fa5-1663467884.8208642')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 106, comm: 0, waiting: 5>>, <TaskState 'filter-d4a208a5-17e5-4141-bfa4-f4b5b330f045' long-running>, compute_duration=0.028765439987182617, stimulus_id='secede-filter-d4a208a5-17e5-4141-bfa4-f4b5b330f045-1663467884.850775')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 104, comm: 0, waiting: 5>>, <TaskState 'filter-7654d544-9a74-4a3d-bd31-da62bb6cd714' long-running>, compute_duration=0.02800583839416504, stimulus_id='secede-filter-7654d544-9a74-4a3d-bd31-da62bb6cd714-1663467884.8799112')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 103, comm: 0, waiting: 5>>, <TaskState 'filter-59cf8282-7c1e-4242-9574-fd1eab884987' long-running>, compute_duration=0.02769637107849121, stimulus_id='secede-filter-59cf8282-7c1e-4242-9574-fd1eab884987-1663467884.9087138')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 102, comm: 0, waiting: 5>>, <TaskState 'filter-b52da4d5-4fb3-43a1-9518-09b3363e6bc5' long-running>, compute_duration=0.03260445594787598, stimulus_id='secede-filter-b52da4d5-4fb3-43a1-9518-09b3363e6bc5-1663467884.9425087')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 101, comm: 0, waiting: 5>>, <TaskState 'filter-4bdbe55d-7c97-4e63-9a14-521456868c3b' long-running>, compute_duration=0.029747724533081055, stimulus_id='secede-filter-4bdbe55d-7c97-4e63-9a14-521456868c3b-1663467884.9752283')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 100, comm: 0, waiting: 5>>, <TaskState 'filter-1dd6948e-9f50-4fff-a017-8b6bd294f286' long-running>, compute_duration=0.03290224075317383, stimulus_id='secede-filter-1dd6948e-9f50-4fff-a017-8b6bd294f286-1663467885.0092008')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 97, comm: 0, waiting: 5>>, <TaskState 'filter-9373326d-84cc-4e13-86b1-3fe6c9c71d90' long-running>, compute_duration=0.028612136840820312, stimulus_id='secede-filter-9373326d-84cc-4e13-86b1-3fe6c9c71d90-1663467885.0389378')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 96, comm: 0, waiting: 5>>, <TaskState 'filter-90479cb4-d4a9-4f9a-a982-6e6cd7b52bfe' long-running>, compute_duration=0.029415607452392578, stimulus_id='secede-filter-90479cb4-d4a9-4f9a-a982-6e6cd7b52bfe-1663467885.0695074')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 95, comm: 0, waiting: 5>>, <TaskState 'filter-2e188a9d-017e-4e16-9def-46d4ecc7259f' long-running>, compute_duration=0.03010869026184082, stimulus_id='secede-filter-2e188a9d-017e-4e16-9def-46d4ecc7259f-1663467885.1006851')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 94, comm: 0, waiting: 5>>, <TaskState 'filter-dbacf4ce-eb4d-4d06-9a8c-99fda9e25d6e' long-running>, compute_duration=0.029445648193359375, stimulus_id='secede-filter-dbacf4ce-eb4d-4d06-9a8c-99fda9e25d6e-1663467885.1313791')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 93, comm: 0, waiting: 5>>, <TaskState 'filter-4dff08a2-1226-44ae-b96d-3af3cc81fb07' long-running>, compute_duration=0.028727054595947266, stimulus_id='secede-filter-4dff08a2-1226-44ae-b96d-3af3cc81fb07-1663467885.1613154')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 91, comm: 0, waiting: 5>>, <TaskState 'filter-44510237-c6bf-4885-945c-491d9f304c44' long-running>, compute_duration=0.035346269607543945, stimulus_id='secede-filter-44510237-c6bf-4885-945c-491d9f304c44-1663467885.197825')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 88, comm: 0, waiting: 5>>, <TaskState 'filter-2856e02e-26db-442c-ac23-f5aa82396f9e' long-running>, compute_duration=0.029419898986816406, stimulus_id='secede-filter-2856e02e-26db-442c-ac23-f5aa82396f9e-1663467885.2283537')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 87, comm: 0, waiting: 5>>, <TaskState 'filter-dc8d573b-87a3-481c-a086-0d81a77e2307' long-running>, compute_duration=0.029989004135131836, stimulus_id='secede-filter-dc8d573b-87a3-481c-a086-0d81a77e2307-1663467885.2595875')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 86, comm: 0, waiting: 5>>, <TaskState 'filter-fd93b354-691e-4e80-a78d-9072505b4c25' long-running>, compute_duration=0.030201196670532227, stimulus_id='secede-filter-fd93b354-691e-4e80-a78d-9072505b4c25-1663467885.2910163')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 82, comm: 0, waiting: 5>>, <TaskState 'filter-585e4dcc-0cf8-4155-b3b2-4520f742027e' long-running>, compute_duration=0.030052900314331055, stimulus_id='secede-filter-585e4dcc-0cf8-4155-b3b2-4520f742027e-1663467885.3222833')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 73, comm: 0, waiting: 5>>, <TaskState 'filter-710b58f8-8229-4412-8a37-1bb2f0a9e538' long-running>, compute_duration=0.03311586380004883, stimulus_id='secede-filter-710b58f8-8229-4412-8a37-1bb2f0a9e538-1663467885.3566103')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 57, comm: 0, waiting: 5>>, <TaskState 'filter-5bfa23ba-2966-40ef-b91d-ae16b905a908' long-running>, compute_duration=0.03353309631347656, stimulus_id='secede-filter-5bfa23ba-2966-40ef-b91d-ae16b905a908-1663467885.3912935')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 52, comm: 0, waiting: 5>>, <TaskState 'filter-ec74c194-cda9-4645-9f9e-b7da02972b31' long-running>, compute_duration=0.0301821231842041, stimulus_id='secede-filter-ec74c194-cda9-4645-9f9e-b7da02972b31-1663467885.4225762')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 51, comm: 0, waiting: 5>>, <TaskState 'filter-163b644c-61b8-4ee9-8162-43e1b4df4017' long-running>, compute_duration=0.02579474449157715, stimulus_id='secede-filter-163b644c-61b8-4ee9-8162-43e1b4df4017-1663467885.4513357')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 33, comm: 0, waiting: 5>>, <TaskState 'filter-b84efff7-f9f2-4e82-8017-dcae8575d432' long-running>, compute_duration=0.027864694595336914, stimulus_id='secede-filter-b84efff7-f9f2-4e82-8017-dcae8575d432-1663467885.482775')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 19, comm: 0, waiting: 5>>, <TaskState 'filter-27dd6294-ea62-42b0-a772-5ac17a339434' long-running>, compute_duration=0.030896663665771484, stimulus_id='secede-filter-27dd6294-ea62-42b0-a772-5ac17a339434-1663467885.5168867')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 17, comm: 0, waiting: 5>>, <TaskState 'filter-8b1021af-d883-47cc-8650-8a6656370cd6' long-running>, compute_duration=0.02937602996826172, stimulus_id='secede-filter-8b1021af-d883-47cc-8650-8a6656370cd6-1663467885.5474901')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 16, comm: 0, waiting: 5>>, <TaskState 'filter-2c4f5f68-e896-4473-ad5c-2b89793ded67' long-running>, compute_duration=0.03285837173461914, stimulus_id='secede-filter-2c4f5f68-e896-4473-ad5c-2b89793ded67-1663467885.5813425')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 14, comm: 0, waiting: 5>>, <TaskState 'filter-f6780803-c380-4da9-8331-a0a662d59ea7' long-running>, compute_duration=0.029701948165893555, stimulus_id='secede-filter-f6780803-c380-4da9-8331-a0a662d59ea7-1663467885.612326')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 13, comm: 0, waiting: 5>>, <TaskState 'filter-3ede5888-9f38-4658-8fe5-ef71e33ebfdd' long-running>, compute_duration=0.032816171646118164, stimulus_id='secede-filter-3ede5888-9f38-4658-8fe5-ef71e33ebfdd-1663467885.6486902')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 12, comm: 0, waiting: 5>>, <TaskState 'filter-5626f1db-dd28-471d-be09-d0fba52fc005' long-running>, compute_duration=0.036881208419799805, stimulus_id='secede-filter-5626f1db-dd28-471d-be09-d0fba52fc005-1663467885.6870468')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 11, comm: 0, waiting: 5>>, <TaskState 'filter-db1518e8-c4d4-40e8-84b8-1cd47d4244e4' long-running>, compute_duration=0.02984476089477539, stimulus_id='secede-filter-db1518e8-c4d4-40e8-84b8-1cd47d4244e4-1663467885.7179837')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 10, comm: 0, waiting: 5>>, <TaskState 'filter-c980cf9e-06f1-4a67-aa93-3554e4411a99' long-running>, compute_duration=0.0316009521484375, stimulus_id='secede-filter-c980cf9e-06f1-4a67-aa93-3554e4411a99-1663467885.7508173')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 8, comm: 0, waiting: 5>>, <TaskState 'filter-d4f1ca42-2154-4ac4-898a-af5f58e29e86' long-running>, compute_duration=0.028504610061645508, stimulus_id='secede-filter-d4f1ca42-2154-4ac4-898a-af5f58e29e86-1663467885.7808619')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 7, comm: 0, waiting: 5>>, <TaskState 'filter-c81ed699-d1ef-458a-b2bf-d905f427f939' long-running>, compute_duration=0.03027510643005371, stimulus_id='secede-filter-c81ed699-d1ef-458a-b2bf-d905f427f939-1663467885.8125162')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 6, comm: 0, waiting: 5>>, <TaskState 'filter-46d16d54-7351-4382-846e-443b9bf49656' long-running>, compute_duration=0.030562877655029297, stimulus_id='secede-filter-46d16d54-7351-4382-846e-443b9bf49656-1663467885.8481915')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270111309974, 270422140020)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 99)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274462930618, 274773760664)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 100)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 5, comm: 0, waiting: 5>>, <TaskState 'filter-9987deaf-90a5-4cfe-98f3-6901fc351c17' long-running>, compute_duration=0.03790616989135742, stimulus_id='secede-filter-9987deaf-90a5-4cfe-98f3-6901fc351c17-1663467885.887526')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215094391832, 215405221878)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 81)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197687909256, 197998739302)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 73)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:36541', name: LSFCluster-1-13, status: running, stored: 57, running: 0/1, ready: 4, comm: 0, waiting: 5>>, <TaskState 'filter-7112c4cc-312b-4735-a8e5-466bfe1ff126' long-running>, compute_duration=0.030920982360839844, stimulus_id='secede-filter-7112c4cc-312b-4735-a8e5-466bfe1ff126-1663467885.9197514')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35770 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196133759026, 196444589072)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 72)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (195822928980, 196133759026)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 72)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189606328060, 189917158106)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 68)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (181835576910, 182146406956)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 64)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179659766588, 179970596634)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 63)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187430517738, 187741347784)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 67)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184943877370, 185254707416)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 66)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184011387232, 184322217278)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 65)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171889015438, 172199845484)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 55)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68693440166, 69004270212)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69315100258, 69625930304)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73355890856, 73666720902)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53773597958, 54084428004)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52530277774, 52841107820)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53151937866, 53462767912)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52841107820, 53151937866)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59057708740, 59368538786)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58125218602, 58436048648)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43516206440, 43827036486)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43827036486, 44137866532)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (44448696578, 44759526624)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42583716302, 42894546348)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42894546348, 43205376394)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41651226164, 41962056210)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41962056210, 42272886256)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46935336946, 47246166992)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49421977314, 49732807360)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49111147268, 49421977314)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99465614720, 99776444766)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142670991114, 142981821160)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 31)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147022611758, 147333441804)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 36)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148576761988, 148887592034)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 37)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 96, comm: 0, waiting: 3>>, <TaskState 'filter-2eca1d40-2c0e-4ad9-8114-8f6cb8cf4c47' long-running>, compute_duration=0.649986982345581, stimulus_id='secede-filter-2eca1d40-2c0e-4ad9-8114-8f6cb8cf4c47-1663467886.946232')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 95, comm: 0, waiting: 3>>, <TaskState 'filter-5fa56ee9-d781-4a45-98e6-da72fda44cd0' long-running>, compute_duration=0.026339292526245117, stimulus_id='secede-filter-5fa56ee9-d781-4a45-98e6-da72fda44cd0-1663467886.9745576')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 94, comm: 0, waiting: 3>>, <TaskState 'filter-14530444-7c59-42c9-a20a-be988de519a1' long-running>, compute_duration=0.025491714477539062, stimulus_id='secede-filter-14530444-7c59-42c9-a20a-be988de519a1-1663467887.0011222')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 92, comm: 0, waiting: 3>>, <TaskState 'filter-0b7eb337-7341-4cc3-ac1f-4470e5b86c45' long-running>, compute_duration=0.02633523941040039, stimulus_id='secede-filter-0b7eb337-7341-4cc3-ac1f-4470e5b86c45-1663467887.0286562')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 91, comm: 0, waiting: 3>>, <TaskState 'filter-79641b5f-02bc-460d-877f-43fa14d72cc4' long-running>, compute_duration=0.02619767189025879, stimulus_id='secede-filter-79641b5f-02bc-460d-877f-43fa14d72cc4-1663467887.0560327')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 89, comm: 0, waiting: 3>>, <TaskState 'filter-0bdf7958-737a-4335-a296-76fbc7304b31' long-running>, compute_duration=0.026851654052734375, stimulus_id='secede-filter-0bdf7958-737a-4335-a296-76fbc7304b31-1663467887.0839918')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 87, comm: 0, waiting: 3>>, <TaskState 'filter-2838e1e9-8abc-4e01-99a4-3f5b066ed64d' long-running>, compute_duration=0.028406858444213867, stimulus_id='secede-filter-2838e1e9-8abc-4e01-99a4-3f5b066ed64d-1663467887.1134942')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 83, comm: 0, waiting: 3>>, <TaskState 'filter-1eb0e704-4d46-4ff8-bf5a-d877bfadbeaa' long-running>, compute_duration=0.026607036590576172, stimulus_id='secede-filter-1eb0e704-4d46-4ff8-bf5a-d877bfadbeaa-1663467887.1419823')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 72, comm: 0, waiting: 3>>, <TaskState 'filter-25591c8e-1602-43fb-9f3e-9da3e859d400' long-running>, compute_duration=0.027048587799072266, stimulus_id='secede-filter-25591c8e-1602-43fb-9f3e-9da3e859d400-1663467887.1711245')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 71, comm: 0, waiting: 3>>, <TaskState 'filter-612aea8e-59d4-4644-8218-26457f203ee6' long-running>, compute_duration=0.026465654373168945, stimulus_id='secede-filter-612aea8e-59d4-4644-8218-26457f203ee6-1663467887.2003348')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 70, comm: 0, waiting: 3>>, <TaskState 'filter-0ab8b232-6a7a-40ff-be7b-4ef82612143c' long-running>, compute_duration=0.026815176010131836, stimulus_id='secede-filter-0ab8b232-6a7a-40ff-be7b-4ef82612143c-1663467887.2282164')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 69, comm: 0, waiting: 3>>, <TaskState 'filter-50000836-9150-433e-a779-b57a9707ffb2' long-running>, compute_duration=0.026508331298828125, stimulus_id='secede-filter-50000836-9150-433e-a779-b57a9707ffb2-1663467887.2557888')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 63, comm: 0, waiting: 3>>, <TaskState 'filter-f8b9d6f1-77c6-46a5-b4df-13301b337bf5' long-running>, compute_duration=0.028029680252075195, stimulus_id='secede-filter-f8b9d6f1-77c6-46a5-b4df-13301b337bf5-1663467887.28497')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 61, comm: 0, waiting: 3>>, <TaskState 'filter-bd9e1dc0-e64d-42e5-95f3-f83c84d115f4' long-running>, compute_duration=0.028787612915039062, stimulus_id='secede-filter-bd9e1dc0-e64d-42e5-95f3-f83c84d115f4-1663467887.3162417')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 58, comm: 0, waiting: 3>>, <TaskState 'filter-fcc821ad-d325-47e9-97b6-62112fa30342' long-running>, compute_duration=0.028080463409423828, stimulus_id='secede-filter-fcc821ad-d325-47e9-97b6-62112fa30342-1663467887.3455884')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 53, comm: 0, waiting: 3>>, <TaskState 'filter-c61a8983-9595-4da9-b31e-8edfa42569a7' long-running>, compute_duration=0.027832508087158203, stimulus_id='secede-filter-c61a8983-9595-4da9-b31e-8edfa42569a7-1663467887.3746476')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 41, comm: 0, waiting: 3>>, <TaskState 'filter-0ceb6377-c9bc-4f87-be4a-fa4bab1e4d68' long-running>, compute_duration=0.02625870704650879, stimulus_id='secede-filter-0ceb6377-c9bc-4f87-be4a-fa4bab1e4d68-1663467887.4021192')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 33, comm: 0, waiting: 3>>, <TaskState 'filter-7a4736a4-2ad3-42bb-b5fe-09f9329decbf' long-running>, compute_duration=0.027194976806640625, stimulus_id='secede-filter-7a4736a4-2ad3-42bb-b5fe-09f9329decbf-1663467887.4317133')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 29, comm: 0, waiting: 3>>, <TaskState 'filter-fcb7aae2-157b-4393-9031-7571817afd5d' long-running>, compute_duration=0.027319908142089844, stimulus_id='secede-filter-fcb7aae2-157b-4393-9031-7571817afd5d-1663467887.4614248')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 27, comm: 0, waiting: 3>>, <TaskState 'filter-7cecaf48-d181-4edc-974c-a51fc19ac011' long-running>, compute_duration=0.02939009666442871, stimulus_id='secede-filter-7cecaf48-d181-4edc-974c-a51fc19ac011-1663467887.4918923')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 13, comm: 0, waiting: 3>>, <TaskState 'filter-bbce972d-be98-492e-b69e-95b9498c12d3' long-running>, compute_duration=0.02775287628173828, stimulus_id='secede-filter-bbce972d-be98-492e-b69e-95b9498c12d3-1663467887.5221286')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 12, comm: 0, waiting: 3>>, <TaskState 'filter-eb3c53ed-eca3-45eb-9724-f355d523fc8a' long-running>, compute_duration=0.0276339054107666, stimulus_id='secede-filter-eb3c53ed-eca3-45eb-9724-f355d523fc8a-1663467887.5523248')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 10, comm: 0, waiting: 3>>, <TaskState 'filter-dbc4adb1-4522-44bd-a85b-6090ef7c1463' long-running>, compute_duration=0.029853105545043945, stimulus_id='secede-filter-dbc4adb1-4522-44bd-a85b-6090ef7c1463-1663467887.5848172')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 8, comm: 0, waiting: 3>>, <TaskState 'filter-850464d4-8ea2-4e85-b5d2-47b3d3b0d291' long-running>, compute_duration=0.02998661994934082, stimulus_id='secede-filter-850464d4-8ea2-4e85-b5d2-47b3d3b0d291-1663467887.6160264')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 5, comm: 0, waiting: 3>>, <TaskState 'filter-e2967651-f7b1-4477-9631-97f30eb18fee' long-running>, compute_duration=0.029752492904663086, stimulus_id='secede-filter-e2967651-f7b1-4477-9631-97f30eb18fee-1663467887.6469731')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:34877', name: LSFCluster-1-1, status: running, stored: 49, running: 0/1, ready: 1, comm: 0, waiting: 3>>, <TaskState 'filter-fa86541d-dfe4-4e17-b92a-4bf2d2a1328d' long-running>, compute_duration=0.02854299545288086, stimulus_id='secede-filter-fa86541d-dfe4-4e17-b92a-4bf2d2a1328d-1663467887.6765559')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35750 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 96, comm: 0, waiting: 1>>, <TaskState 'filter-6ede1f83-12a7-42c8-a51d-0a6ff928d423' long-running>, compute_duration=0.56746506690979, stimulus_id='secede-filter-6ede1f83-12a7-42c8-a51d-0a6ff928d423-1663467888.081952')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 94, comm: 0, waiting: 1>>, <TaskState 'filter-b128870a-17b6-4d4c-99b8-2cc95c2911e6' long-running>, compute_duration=0.026697635650634766, stimulus_id='secede-filter-b128870a-17b6-4d4c-99b8-2cc95c2911e6-1663467888.1103659')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 93, comm: 0, waiting: 1>>, <TaskState 'filter-fb6a2872-bc50-4bfb-872d-c6a722d03f10' long-running>, compute_duration=0.026093721389770508, stimulus_id='secede-filter-fb6a2872-bc50-4bfb-872d-c6a722d03f10-1663467888.1378088')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (259543088410, 259853918456)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 97)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 91, comm: 0, waiting: 1>>, <TaskState 'filter-dec43da0-77cc-4207-87b6-06f65e94505a' long-running>, compute_duration=0.026058197021484375, stimulus_id='secede-filter-dec43da0-77cc-4207-87b6-06f65e94505a-1663467888.1661603')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210742771188, 211053601234)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 79)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 86, comm: 0, waiting: 1>>, <TaskState 'filter-b5c3e0ae-9790-4f6a-87b9-5605dc50f144' long-running>, compute_duration=0.026260852813720703, stimulus_id='secede-filter-b5c3e0ae-9790-4f6a-87b9-5605dc50f144-1663467888.194154')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209810281050, 210121111096)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 79)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 85, comm: 0, waiting: 1>>, <TaskState 'filter-4ef261b6-87c8-41c3-92c6-9884a98833f0' long-running>, compute_duration=0.028785228729248047, stimulus_id='secede-filter-4ef261b6-87c8-41c3-92c6-9884a98833f0-1663467888.2240677')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (212607751464, 212918581510)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 80)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217891862246, 218202692292)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 82)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 84, comm: 0, waiting: 1>>, <TaskState 'filter-e1797223-e113-481e-8ebc-480531fbac2b' long-running>, compute_duration=0.028934478759765625, stimulus_id='secede-filter-e1797223-e113-481e-8ebc-480531fbac2b-1663467888.254104')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216959372108, 217270202154)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 81)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 83, comm: 0, waiting: 1>>, <TaskState 'filter-8ea46518-9232-4ec3-818c-63cb0fa02536' long-running>, compute_duration=0.026596784591674805, stimulus_id='secede-filter-8ea46518-9232-4ec3-818c-63cb0fa02536-1663467888.2817826')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214472731740, 214783561786)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 80)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 75, comm: 0, waiting: 1>>, <TaskState 'filter-7f6dc646-80c3-4f6a-b4e1-b06f5c5df8fd' long-running>, compute_duration=0.027384042739868164, stimulus_id='secede-filter-7f6dc646-80c3-4f6a-b4e1-b06f5c5df8fd-1663467888.311229')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214783561786, 215094391832)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 80)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202972020038, 203282850084)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 76)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 74, comm: 0, waiting: 1>>, <TaskState 'filter-0389eb5e-1b3d-4b2a-a90b-9bec878fd9b2' long-running>, compute_duration=0.02792978286743164, stimulus_id='secede-filter-0389eb5e-1b3d-4b2a-a90b-9bec878fd9b2-1663467888.3404417')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198931229440, 199242059486)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 74)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 69, comm: 0, waiting: 1>>, <TaskState 'filter-a9728d26-94a8-4fff-91cb-4ab9b2679a77' long-running>, compute_duration=0.027346134185791016, stimulus_id='secede-filter-a9728d26-94a8-4fff-91cb-4ab9b2679a77-1663467888.368905')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201107039762, 201417869808)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 75)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 63, comm: 0, waiting: 1>>, <TaskState 'filter-f951aad0-2fa8-4e67-80f8-5da33498bf83' long-running>, compute_duration=0.027804851531982422, stimulus_id='secede-filter-f951aad0-2fa8-4e67-80f8-5da33498bf83-1663467888.398713')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206080320498, 206391150544)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 77)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207634470728, 207945300774)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 78)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 53, comm: 0, waiting: 1>>, <TaskState 'filter-83fbd19e-8180-4237-b9a9-8d114fa7b485' long-running>, compute_duration=0.026668310165405273, stimulus_id='secede-filter-83fbd19e-8180-4237-b9a9-8d114fa7b485-1663467888.4276516')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208566960866, 208877790912)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 78)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 32, comm: 0, waiting: 1>>, <TaskState 'filter-7472118f-93ed-4637-813b-69c6e4515f88' long-running>, compute_duration=0.028324604034423828, stimulus_id='secede-filter-7472118f-93ed-4637-813b-69c6e4515f88-1663467888.4590383')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207323640682, 207634470728)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 78)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170956525300, 171267355346)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 55)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:37633', name: LSFCluster-1-7, status: running, stored: 69, running: 0/1, ready: 0, comm: 0, waiting: 1>>, <TaskState 'filter-cfde3e02-3c57-44ce-b2d8-41347bc9ebfd' long-running>, compute_duration=0.028179407119750977, stimulus_id='secede-filter-cfde3e02-3c57-44ce-b2d8-41347bc9ebfd-1663467888.4884064')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35768 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173132335622, 173443165668)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 57)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (178727276450, 179038106496)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 63)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166915734702, 167226564748)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 53)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166294074610, 166604904656)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 52)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (159455813598, 159766643644)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 46)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163496604196, 163807434242)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 49)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28907194278, 29218024324)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5905770874, 6216600920)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9324901380, 9635731426)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12122371794, 12433201840)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18338972714, 18649802760)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61855179154, 62166009200)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (67450119982, 67760950028)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (79261661730, 79572491776)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73977550948, 74288380994)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58746878694, 59057708740)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57814388556, 58125218602)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57192728464, 57503558510)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55327748188, 55638578234)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (45381186716, 45692016762)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (84545772512, 84856602558)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116250437204, 116561267250)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 11)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115317947066, 115628777112)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 11)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101019764950, 101330594996)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 1)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155415023000, 155725853046)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 43)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158523323460, 158834153506)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 46)")'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44157 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:38594 remote=tcp://192.168.64.232:44157>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44157 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 101, comm: 0, waiting: 0>>, <TaskState 'filter-cefb6acf-5949-4bd5-ba77-722fdf7c81fe' long-running>, compute_duration=0.9539065361022949, stimulus_id='secede-filter-cefb6acf-5949-4bd5-ba77-722fdf7c81fe-1663467891.036412')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 96, comm: 0, waiting: 0>>, <TaskState 'filter-5c7d88b1-a570-45a3-9a89-6436256f1226' long-running>, compute_duration=0.0254669189453125, stimulus_id='secede-filter-5c7d88b1-a570-45a3-9a89-6436256f1226-1663467891.0648458')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 94, comm: 0, waiting: 0>>, <TaskState 'filter-547ad544-c86e-4084-858d-7966a547a255' long-running>, compute_duration=0.025629758834838867, stimulus_id='secede-filter-547ad544-c86e-4084-858d-7966a547a255-1663467891.0917149')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 90, comm: 0, waiting: 0>>, <TaskState 'filter-86c3ad6d-cbc2-4614-94d2-a2297aaa79bf' long-running>, compute_duration=0.026094436645507812, stimulus_id='secede-filter-86c3ad6d-cbc2-4614-94d2-a2297aaa79bf-1663467891.119837')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 89, comm: 0, waiting: 0>>, <TaskState 'filter-719c7043-9c01-4fe7-ba1e-19d970ae4af9' long-running>, compute_duration=0.031124353408813477, stimulus_id='secede-filter-719c7043-9c01-4fe7-ba1e-19d970ae4af9-1663467891.1520872')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 88, comm: 0, waiting: 0>>, <TaskState 'filter-2691167a-4ef4-4bd7-80f6-68d314b67763' long-running>, compute_duration=0.026770830154418945, stimulus_id='secede-filter-2691167a-4ef4-4bd7-80f6-68d314b67763-1663467891.1799626')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 85, comm: 0, waiting: 0>>, <TaskState 'filter-b8878f1e-5ff7-4174-a919-b365ee2ef7cd' long-running>, compute_duration=0.025713682174682617, stimulus_id='secede-filter-b8878f1e-5ff7-4174-a919-b365ee2ef7cd-1663467891.2073035')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 83, comm: 0, waiting: 0>>, <TaskState 'filter-143d316a-7ef7-4048-8245-972f38698880' long-running>, compute_duration=0.026339054107666016, stimulus_id='secede-filter-143d316a-7ef7-4048-8245-972f38698880-1663467891.2361896')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 67, comm: 0, waiting: 0>>, <TaskState 'filter-d9412c03-0ca5-4b91-9a5a-06b681557271' long-running>, compute_duration=0.026594161987304688, stimulus_id='secede-filter-d9412c03-0ca5-4b91-9a5a-06b681557271-1663467891.2639616')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 64, comm: 0, waiting: 0>>, <TaskState 'filter-2f4c3291-41c5-4c19-b9e4-886eb387971d' long-running>, compute_duration=0.02601337432861328, stimulus_id='secede-filter-2f4c3291-41c5-4c19-b9e4-886eb387971d-1663467891.291924')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 63, comm: 0, waiting: 0>>, <TaskState 'filter-97c9ae6f-40fa-4df3-a066-b59589096d9c' long-running>, compute_duration=0.026564359664916992, stimulus_id='secede-filter-97c9ae6f-40fa-4df3-a066-b59589096d9c-1663467891.320227')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 58, comm: 0, waiting: 0>>, <TaskState 'filter-0654cdb2-1eff-4326-9f28-a576ec975f3f' long-running>, compute_duration=0.02908611297607422, stimulus_id='secede-filter-0654cdb2-1eff-4326-9f28-a576ec975f3f-1663467891.350757')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 54, comm: 0, waiting: 0>>, <TaskState 'filter-b91e0561-fef1-4b90-bfd9-eb222b36c8f3' long-running>, compute_duration=0.027627229690551758, stimulus_id='secede-filter-b91e0561-fef1-4b90-bfd9-eb222b36c8f3-1663467891.379603')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 53, comm: 0, waiting: 0>>, <TaskState 'filter-f039b199-3992-4c10-a540-f65c56e4f67f' long-running>, compute_duration=0.0271608829498291, stimulus_id='secede-filter-f039b199-3992-4c10-a540-f65c56e4f67f-1663467891.4079502')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 51, comm: 0, waiting: 0>>, <TaskState 'filter-9a9aea43-c7bc-4d63-aae6-298fd65b0e9f' long-running>, compute_duration=0.026900291442871094, stimulus_id='secede-filter-9a9aea43-c7bc-4d63-aae6-298fd65b0e9f-1663467891.437007')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 50, comm: 0, waiting: 0>>, <TaskState 'filter-b7ff3544-6774-4f33-ac88-74fb70bae9d1' long-running>, compute_duration=0.028234004974365234, stimulus_id='secede-filter-b7ff3544-6774-4f33-ac88-74fb70bae9d1-1663467891.4663155')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 44, comm: 0, waiting: 0>>, <TaskState 'filter-14110094-d734-4311-9177-e16c042facb3' long-running>, compute_duration=0.026272296905517578, stimulus_id='secede-filter-14110094-d734-4311-9177-e16c042facb3-1663467891.4958248')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 144, comm: 0, waiting: 0>>, <TaskState 'filter-f6b7d7ba-474e-48d2-b5b2-ba386f937b1b' long-running>, compute_duration=0.5811071395874023, stimulus_id='secede-filter-f6b7d7ba-474e-48d2-b5b2-ba386f937b1b-1663467891.5123973')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 39, comm: 0, waiting: 0>>, <TaskState 'filter-568c8154-b231-4474-8958-0bbd2cd828c7' long-running>, compute_duration=0.029709339141845703, stimulus_id='secede-filter-568c8154-b231-4474-8958-0bbd2cd828c7-1663467891.526657')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 139, comm: 0, waiting: 0>>, <TaskState 'filter-e76339bb-7e67-4763-9ad5-fbddb003a0db' long-running>, compute_duration=0.03265976905822754, stimulus_id='secede-filter-e76339bb-7e67-4763-9ad5-fbddb003a0db-1663467891.54661')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 36, comm: 0, waiting: 0>>, <TaskState 'filter-6798b123-0b8e-4579-98d3-880e96ca9618' long-running>, compute_duration=0.030037879943847656, stimulus_id='secede-filter-6798b123-0b8e-4579-98d3-880e96ca9618-1663467891.5579243')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 135, comm: 0, waiting: 0>>, <TaskState 'filter-d392ed38-3a90-4199-adf1-3f9a3222c828' long-running>, compute_duration=0.026050806045532227, stimulus_id='secede-filter-d392ed38-3a90-4199-adf1-3f9a3222c828-1663467891.5740547')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 35, comm: 0, waiting: 0>>, <TaskState 'filter-b9ebb56a-e8f8-4c38-8bda-86a08442e11f' long-running>, compute_duration=0.029268503189086914, stimulus_id='secede-filter-b9ebb56a-e8f8-4c38-8bda-86a08442e11f-1663467891.5897946')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 133, comm: 0, waiting: 0>>, <TaskState 'filter-0f9ee93c-9180-44bd-a329-f8a21c01bacd' long-running>, compute_duration=0.0256650447845459, stimulus_id='secede-filter-0f9ee93c-9180-44bd-a329-f8a21c01bacd-1663467891.6011715')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 28, comm: 0, waiting: 0>>, <TaskState 'filter-1ad3e620-d1d2-4242-9402-e411bf629a7e' long-running>, compute_duration=0.027548551559448242, stimulus_id='secede-filter-1ad3e620-d1d2-4242-9402-e411bf629a7e-1663467891.6184633')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 131, comm: 0, waiting: 0>>, <TaskState 'filter-d841d13a-fa9d-4b53-a850-b22e1ec5d852' long-running>, compute_duration=0.026209115982055664, stimulus_id='secede-filter-d841d13a-fa9d-4b53-a850-b22e1ec5d852-1663467891.6295767')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 11, comm: 0, waiting: 0>>, <TaskState 'filter-09498579-761d-4de0-9c41-b566bba13bf0' long-running>, compute_duration=0.030283451080322266, stimulus_id='secede-filter-09498579-761d-4de0-9c41-b566bba13bf0-1663467891.651564')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 130, comm: 0, waiting: 0>>, <TaskState 'filter-4d8207bd-32f0-40cf-830e-25534bf9c779' long-running>, compute_duration=0.026144981384277344, stimulus_id='secede-filter-4d8207bd-32f0-40cf-830e-25534bf9c779-1663467891.656928')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 8, comm: 0, waiting: 0>>, <TaskState 'filter-9104e613-8304-40b8-a667-122983415a99' long-running>, compute_duration=0.031403303146362305, stimulus_id='secede-filter-9104e613-8304-40b8-a667-122983415a99-1663467891.684181')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 129, comm: 0, waiting: 0>>, <TaskState 'filter-b1ea1b0d-a2ef-4f07-a75a-eba6334d0ae6' long-running>, compute_duration=0.027684688568115234, stimulus_id='secede-filter-b1ea1b0d-a2ef-4f07-a75a-eba6334d0ae6-1663467891.685801')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 5, comm: 0, waiting: 0>>, <TaskState 'filter-69f16272-92cd-45a9-80c3-38e9be8b6740' long-running>, compute_duration=0.03059697151184082, stimulus_id='secede-filter-69f16272-92cd-45a9-80c3-38e9be8b6740-1663467891.71602')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 128, comm: 0, waiting: 0>>, <TaskState 'filter-c44728eb-15b7-450f-a5b5-4b983ad309ca' long-running>, compute_duration=0.029320240020751953, stimulus_id='secede-filter-c44728eb-15b7-450f-a5b5-4b983ad309ca-1663467891.7163782')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 127, comm: 0, waiting: 0>>, <TaskState 'filter-702002a9-3c1b-421c-90e7-ac001e72d717' long-running>, compute_duration=0.02704477310180664, stimulus_id='secede-filter-702002a9-3c1b-421c-90e7-ac001e72d717-1663467891.7445052')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45151', name: LSFCluster-1-9, status: running, stored: 92, running: 0/1, ready: 3, comm: 0, waiting: 0>>, <TaskState 'filter-a566ec4e-69f2-467e-9522-18ddbe042980' long-running>, compute_duration=0.029351472854614258, stimulus_id='secede-filter-a566ec4e-69f2-467e-9522-18ddbe042980-1663467891.746499')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35758 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 126, comm: 0, waiting: 0>>, <TaskState 'filter-9f9a8228-509c-4ad8-9665-a625a8cafb47' long-running>, compute_duration=0.02711200714111328, stimulus_id='secede-filter-9f9a8228-509c-4ad8-9665-a625a8cafb47-1663467891.7729607')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 125, comm: 0, waiting: 0>>, <TaskState 'filter-48e4a72f-267e-4c0b-9606-847452f67026' long-running>, compute_duration=0.03189802169799805, stimulus_id='secede-filter-48e4a72f-267e-4c0b-9606-847452f67026-1663467891.8067343')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 121, comm: 0, waiting: 0>>, <TaskState 'filter-43f43c0d-a73e-454a-b6bc-af8a4b6374f1' long-running>, compute_duration=0.03014063835144043, stimulus_id='secede-filter-43f43c0d-a73e-454a-b6bc-af8a4b6374f1-1663467891.8379843')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 116, comm: 0, waiting: 0>>, <TaskState 'filter-dee589c0-e761-4913-9642-c5d793703667' long-running>, compute_duration=0.027815818786621094, stimulus_id='secede-filter-dee589c0-e761-4913-9642-c5d793703667-1663467891.8668938')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 112, comm: 0, waiting: 0>>, <TaskState 'filter-79166142-baf2-483c-883b-cb1bdd914817' long-running>, compute_duration=0.029369115829467773, stimulus_id='secede-filter-79166142-baf2-483c-883b-cb1bdd914817-1663467891.897407')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 105, comm: 0, waiting: 0>>, <TaskState 'filter-1de8e6f7-2285-4d7c-814f-b7983a7fe32c' long-running>, compute_duration=0.0282742977142334, stimulus_id='secede-filter-1de8e6f7-2285-4d7c-814f-b7983a7fe32c-1663467891.92802')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 103, comm: 0, waiting: 0>>, <TaskState 'filter-faeff420-e6b1-448f-a22b-2d2b213a92ff' long-running>, compute_duration=0.028948545455932617, stimulus_id='secede-filter-faeff420-e6b1-448f-a22b-2d2b213a92ff-1663467891.958082')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 101, comm: 0, waiting: 0>>, <TaskState 'filter-9e65fb75-9869-4795-8339-5912e458ce1b' long-running>, compute_duration=0.02809453010559082, stimulus_id='secede-filter-9e65fb75-9869-4795-8339-5912e458ce1b-1663467891.9872584')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 100, comm: 0, waiting: 0>>, <TaskState 'filter-dacc6ead-2636-46d2-a5c8-40151e1aff0c' long-running>, compute_duration=0.02984023094177246, stimulus_id='secede-filter-dacc6ead-2636-46d2-a5c8-40151e1aff0c-1663467892.018246')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 99, comm: 0, waiting: 0>>, <TaskState 'filter-558b05eb-c6d3-4eb0-9f12-abcd9b6f9543' long-running>, compute_duration=0.031084299087524414, stimulus_id='secede-filter-558b05eb-c6d3-4eb0-9f12-abcd9b6f9543-1663467892.0505605')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 98, comm: 0, waiting: 0>>, <TaskState 'filter-1e4fa834-7ce2-4066-bde8-b11d7a4234df' long-running>, compute_duration=0.028981447219848633, stimulus_id='secede-filter-1e4fa834-7ce2-4066-bde8-b11d7a4234df-1663467892.0807676')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 97, comm: 0, waiting: 0>>, <TaskState 'filter-9bb048c7-6ce4-4d4a-9461-006cb0273a0d' long-running>, compute_duration=0.025370359420776367, stimulus_id='secede-filter-9bb048c7-6ce4-4d4a-9461-006cb0273a0d-1663467892.108695')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 96, comm: 0, waiting: 0>>, <TaskState 'filter-d62d2356-1b5a-4b1e-b3c1-54208c11bdb4' long-running>, compute_duration=0.03397226333618164, stimulus_id='secede-filter-d62d2356-1b5a-4b1e-b3c1-54208c11bdb4-1663467892.147763')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 95, comm: 0, waiting: 0>>, <TaskState 'filter-d4befcf2-1fc0-4f10-b46e-0e52738fef07' long-running>, compute_duration=0.02994370460510254, stimulus_id='secede-filter-d4befcf2-1fc0-4f10-b46e-0e52738fef07-1663467892.1789432')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 94, comm: 0, waiting: 0>>, <TaskState 'filter-a3b06225-5eb2-471a-81aa-568d91a0be86' long-running>, compute_duration=0.029862642288208008, stimulus_id='secede-filter-a3b06225-5eb2-471a-81aa-568d91a0be86-1663467892.2100291')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 93, comm: 0, waiting: 0>>, <TaskState 'filter-18fd1581-4045-4be1-b2ae-3c0aba45b9c3' long-running>, compute_duration=0.030431747436523438, stimulus_id='secede-filter-18fd1581-4045-4be1-b2ae-3c0aba45b9c3-1663467892.2416258')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 92, comm: 0, waiting: 0>>, <TaskState 'filter-bc7e2203-d960-4a2d-ab0f-7ebb25f0fae9' long-running>, compute_duration=0.030431270599365234, stimulus_id='secede-filter-bc7e2203-d960-4a2d-ab0f-7ebb25f0fae9-1663467892.2749038')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 91, comm: 0, waiting: 0>>, <TaskState 'filter-3ef3a24c-73bc-4567-82bf-8659322c9ae1' long-running>, compute_duration=0.032044410705566406, stimulus_id='secede-filter-3ef3a24c-73bc-4567-82bf-8659322c9ae1-1663467892.3081043')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 88, comm: 0, waiting: 0>>, <TaskState 'filter-1dd527e9-9199-4e48-a5d9-4d2492e279ad' long-running>, compute_duration=0.03114151954650879, stimulus_id='secede-filter-1dd527e9-9199-4e48-a5d9-4d2492e279ad-1663467892.3403943')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 86, comm: 0, waiting: 0>>, <TaskState 'filter-7c7ac36e-4c5c-4918-8bb2-43317dbda8d8' long-running>, compute_duration=0.031130313873291016, stimulus_id='secede-filter-7c7ac36e-4c5c-4918-8bb2-43317dbda8d8-1663467892.372682')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 85, comm: 0, waiting: 0>>, <TaskState 'filter-aac79bac-a8ac-4b2d-8bff-2726564ec9c5' long-running>, compute_duration=0.03102850914001465, stimulus_id='secede-filter-aac79bac-a8ac-4b2d-8bff-2726564ec9c5-1663467892.4066203')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 83, comm: 0, waiting: 0>>, <TaskState 'filter-5cd79b43-de25-425e-adcf-7a0bec2c3502' long-running>, compute_duration=0.03650522232055664, stimulus_id='secede-filter-5cd79b43-de25-425e-adcf-7a0bec2c3502-1663467892.4443045')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 82, comm: 0, waiting: 0>>, <TaskState 'filter-53084c2b-55bc-46f5-9fda-7193d123391d' long-running>, compute_duration=0.03159523010253906, stimulus_id='secede-filter-53084c2b-55bc-46f5-9fda-7193d123391d-1663467892.477052')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 81, comm: 0, waiting: 0>>, <TaskState 'filter-b552f981-3e37-4421-a14b-babfcfef1930' long-running>, compute_duration=0.031039714813232422, stimulus_id='secede-filter-b552f981-3e37-4421-a14b-babfcfef1930-1663467892.5092683')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 79, comm: 0, waiting: 0>>, <TaskState 'filter-e84b5d15-0f9a-4d1e-b754-af3d2cb95b8c' long-running>, compute_duration=0.02980780601501465, stimulus_id='secede-filter-e84b5d15-0f9a-4d1e-b754-af3d2cb95b8c-1663467892.5403347')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 78, comm: 0, waiting: 0>>, <TaskState 'filter-746ce148-158c-4618-b536-3895773af94e' long-running>, compute_duration=0.030192852020263672, stimulus_id='secede-filter-746ce148-158c-4618-b536-3895773af94e-1663467892.5717547')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 77, comm: 0, waiting: 0>>, <TaskState 'filter-ef9908ad-5264-4b07-b078-19182f58f244' long-running>, compute_duration=0.025705337524414062, stimulus_id='secede-filter-ef9908ad-5264-4b07-b078-19182f58f244-1663467892.6010103')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 76, comm: 0, waiting: 0>>, <TaskState 'filter-76a94282-425b-4b62-8118-eb5cf2eacd00' long-running>, compute_duration=0.03233647346496582, stimulus_id='secede-filter-76a94282-425b-4b62-8118-eb5cf2eacd00-1663467892.6377273')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 75, comm: 0, waiting: 0>>, <TaskState 'filter-c7fce498-1e18-45de-b635-daf3ce2afad7' long-running>, compute_duration=0.0327298641204834, stimulus_id='secede-filter-c7fce498-1e18-45de-b635-daf3ce2afad7-1663467892.6715982')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 73, comm: 0, waiting: 0>>, <TaskState 'filter-33bebe5c-4a7c-4b31-b501-3c7941794c20' long-running>, compute_duration=0.03433513641357422, stimulus_id='secede-filter-33bebe5c-4a7c-4b31-b501-3c7941794c20-1663467892.7097087')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 70, comm: 0, waiting: 0>>, <TaskState 'filter-932f7936-3f22-44a0-934c-865449543b39' long-running>, compute_duration=0.03543376922607422, stimulus_id='secede-filter-932f7936-3f22-44a0-934c-865449543b39-1663467892.7463114')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 67, comm: 0, waiting: 0>>, <TaskState 'filter-02a3a433-7009-4a7f-849b-1448c2789609' long-running>, compute_duration=0.03343939781188965, stimulus_id='secede-filter-02a3a433-7009-4a7f-849b-1448c2789609-1663467892.780936')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 66, comm: 0, waiting: 0>>, <TaskState 'filter-15762cf8-fb9d-4125-b607-a8f153ddea86' long-running>, compute_duration=0.03099226951599121, stimulus_id='secede-filter-15762cf8-fb9d-4125-b607-a8f153ddea86-1663467892.8161094')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 65, comm: 0, waiting: 0>>, <TaskState 'filter-4b7f9ffc-aa45-4f98-aa20-91825f71f06d' long-running>, compute_duration=0.03720664978027344, stimulus_id='secede-filter-4b7f9ffc-aa45-4f98-aa20-91825f71f06d-1663467892.8575275')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 61, comm: 0, waiting: 0>>, <TaskState 'filter-969cfe9b-16b4-4986-a23f-1a2056358452' long-running>, compute_duration=0.03355669975280762, stimulus_id='secede-filter-969cfe9b-16b4-4986-a23f-1a2056358452-1663467892.8925307')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 60, comm: 0, waiting: 0>>, <TaskState 'filter-2f79559f-80fb-4238-8608-a55c7abba9ef' long-running>, compute_duration=0.033669471740722656, stimulus_id='secede-filter-2f79559f-80fb-4238-8608-a55c7abba9ef-1663467892.9304795')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 58, comm: 0, waiting: 0>>, <TaskState 'filter-16a74a31-bfee-4a19-8b69-06db0779379d' long-running>, compute_duration=0.03835272789001465, stimulus_id='secede-filter-16a74a31-bfee-4a19-8b69-06db0779379d-1663467892.9700983')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 57, comm: 0, waiting: 0>>, <TaskState 'filter-675c2309-470c-4d33-8284-89ca3e0c7960' long-running>, compute_duration=0.03739762306213379, stimulus_id='secede-filter-675c2309-470c-4d33-8284-89ca3e0c7960-1663467893.008785')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 55, comm: 0, waiting: 0>>, <TaskState 'filter-90cfe2ec-8c1e-4ddb-a1b8-76b1ea9a4123' long-running>, compute_duration=0.03427600860595703, stimulus_id='secede-filter-90cfe2ec-8c1e-4ddb-a1b8-76b1ea9a4123-1663467893.0441897')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 53, comm: 0, waiting: 0>>, <TaskState 'filter-62baf99f-6be3-4760-9bcd-e617fc460a0e' long-running>, compute_duration=0.03135418891906738, stimulus_id='secede-filter-62baf99f-6be3-4760-9bcd-e617fc460a0e-1663467893.0798352')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 52, comm: 0, waiting: 0>>, <TaskState 'filter-2996abdc-3c63-4f17-a66e-f83d85b247a7' long-running>, compute_duration=0.026215076446533203, stimulus_id='secede-filter-2996abdc-3c63-4f17-a66e-f83d85b247a7-1663467893.1077647')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 49, comm: 0, waiting: 0>>, <TaskState 'filter-e0524eea-d996-4cee-86c0-88df7e923e25' long-running>, compute_duration=0.028800487518310547, stimulus_id='secede-filter-e0524eea-d996-4cee-86c0-88df7e923e25-1663467893.1422694')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 41, comm: 0, waiting: 0>>, <TaskState 'filter-e9eb003b-79ab-4dcd-8f01-74e953335646' long-running>, compute_duration=0.033469200134277344, stimulus_id='secede-filter-e9eb003b-79ab-4dcd-8f01-74e953335646-1663467893.1803184')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 40, comm: 0, waiting: 0>>, <TaskState 'filter-88768a3c-d34a-49f5-a665-5abf241311be' long-running>, compute_duration=0.03236579895019531, stimulus_id='secede-filter-88768a3c-d34a-49f5-a665-5abf241311be-1663467893.214057')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 39, comm: 0, waiting: 0>>, <TaskState 'filter-bc7324ea-95ba-4571-9bbd-dee89869ece0' long-running>, compute_duration=0.03224444389343262, stimulus_id='secede-filter-bc7324ea-95ba-4571-9bbd-dee89869ece0-1663467893.251132')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 38, comm: 0, waiting: 0>>, <TaskState 'filter-4d990e27-6d69-4b53-a3ca-f59391fe082c' long-running>, compute_duration=0.032187461853027344, stimulus_id='secede-filter-4d990e27-6d69-4b53-a3ca-f59391fe082c-1663467893.2881708')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 37, comm: 0, waiting: 0>>, <TaskState 'filter-f6639c39-ad42-44c4-84d4-10c6620655d8' long-running>, compute_duration=0.03235673904418945, stimulus_id='secede-filter-f6639c39-ad42-44c4-84d4-10c6620655d8-1663467893.3218327')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 35, comm: 0, waiting: 0>>, <TaskState 'filter-9db13ed0-d88d-4ace-8df9-65b052964ba1' long-running>, compute_duration=0.03243231773376465, stimulus_id='secede-filter-9db13ed0-d88d-4ace-8df9-65b052964ba1-1663467893.359186')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 32, comm: 0, waiting: 0>>, <TaskState 'filter-4ba5643d-2497-4e86-ae3c-f771eb66beea' long-running>, compute_duration=0.03981184959411621, stimulus_id='secede-filter-4ba5643d-2497-4e86-ae3c-f771eb66beea-1663467893.400345')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 30, comm: 0, waiting: 0>>, <TaskState 'filter-585c41c1-a5d3-4593-b635-e7fe40d99c0f' long-running>, compute_duration=0.03687024116516113, stimulus_id='secede-filter-585c41c1-a5d3-4593-b635-e7fe40d99c0f-1663467893.4384375')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 28, comm: 0, waiting: 0>>, <TaskState 'filter-3bab012f-3bad-43b0-a3c6-a5dfc47f4233' long-running>, compute_duration=0.037709951400756836, stimulus_id='secede-filter-3bab012f-3bad-43b0-a3c6-a5dfc47f4233-1663467893.4773464')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 26, comm: 0, waiting: 0>>, <TaskState 'filter-576159db-63f5-4ceb-9316-0fb82b1982aa' long-running>, compute_duration=0.029415607452392578, stimulus_id='secede-filter-576159db-63f5-4ceb-9316-0fb82b1982aa-1663467893.507999')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 22, comm: 0, waiting: 0>>, <TaskState 'filter-616d374f-5bd4-46fb-94f5-163328d9655e' long-running>, compute_duration=0.02960824966430664, stimulus_id='secede-filter-616d374f-5bd4-46fb-94f5-163328d9655e-1663467893.5428567')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 20, comm: 0, waiting: 0>>, <TaskState 'filter-0b0bc3a3-e641-4328-a177-3d112d38e81f' long-running>, compute_duration=0.0335993766784668, stimulus_id='secede-filter-0b0bc3a3-e641-4328-a177-3d112d38e81f-1663467893.5818143')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 18, comm: 0, waiting: 0>>, <TaskState 'filter-81c9a06e-8db9-4b71-b9b4-1f4fe9ba8fd8' long-running>, compute_duration=0.03347444534301758, stimulus_id='secede-filter-81c9a06e-8db9-4b71-b9b4-1f4fe9ba8fd8-1663467893.6216183')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 16, comm: 0, waiting: 0>>, <TaskState 'filter-a38fd658-82e5-478c-bafb-78ea0338ba85' long-running>, compute_duration=0.10567665100097656, stimulus_id='secede-filter-a38fd658-82e5-478c-bafb-78ea0338ba85-1663467893.7340467')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 15, comm: 0, waiting: 0>>, <TaskState 'filter-debd5e46-aeb8-4853-9306-0ca499ba1cd4' long-running>, compute_duration=0.03790593147277832, stimulus_id='secede-filter-debd5e46-aeb8-4853-9306-0ca499ba1cd4-1663467893.7774787')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 14, comm: 0, waiting: 0>>, <TaskState 'filter-1de46bb7-9309-4cd1-a010-e51e890a2aaf' long-running>, compute_duration=0.03395342826843262, stimulus_id='secede-filter-1de46bb7-9309-4cd1-a010-e51e890a2aaf-1663467893.8128712')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 13, comm: 0, waiting: 0>>, <TaskState 'filter-a4dd3526-47f9-49da-8f63-76b936927986' long-running>, compute_duration=0.03371930122375488, stimulus_id='secede-filter-a4dd3526-47f9-49da-8f63-76b936927986-1663467893.8523343')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 11, comm: 0, waiting: 0>>, <TaskState 'filter-5956b6d5-ad5a-4e40-8182-4f62042715e8' long-running>, compute_duration=0.03404092788696289, stimulus_id='secede-filter-5956b6d5-ad5a-4e40-8182-4f62042715e8-1663467893.8918505')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 10, comm: 0, waiting: 0>>, <TaskState 'filter-190f6a65-7695-4880-a69a-da1ce9182a97' long-running>, compute_duration=0.03513383865356445, stimulus_id='secede-filter-190f6a65-7695-4880-a69a-da1ce9182a97-1663467893.9327433')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 7, comm: 0, waiting: 0>>, <TaskState 'filter-39670fa9-be56-4d5d-9927-ec5f1ec91527' long-running>, compute_duration=0.03926968574523926, stimulus_id='secede-filter-39670fa9-be56-4d5d-9927-ec5f1ec91527-1663467893.977695')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 6, comm: 0, waiting: 0>>, <TaskState 'filter-6e6dbc0e-ea58-4678-a846-10769fc1d05a' long-running>, compute_duration=0.03868699073791504, stimulus_id='secede-filter-6e6dbc0e-ea58-4678-a846-10769fc1d05a-1663467894.0176163')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method Worker.maybe_transition_long_running of <Worker 'tcp://192.168.64.29:45971', name: LSFCluster-1-11, status: running, stored: 50, running: 0/1, ready: 4, comm: 0, waiting: 0>>, <TaskState 'filter-31e92a70-08eb-4f24-a7cd-8cb53e2e1fd8' long-running>, compute_duration=0.036754608154296875, stimulus_id='secede-filter-31e92a70-08eb-4f24-a7cd-8cb53e2e1fd8-1663467894.0556502')
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2723, in maybe_transition_long_running
    stimulus_id=stimulus_id,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 2682, in transition
    self.batched_stream.send(msg)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.29:35780 remote=tcp://192.168.64.232:44357> already closed.
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256123957904, 256434787950)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 96)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256745617996, 257056448042)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 96)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248974866846, 249285696892)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 94)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245866566386, 246177396432)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 93)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221621822798, 221932652844)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 84)")'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (220067672568, 220378502614)
kwargs:    {}
Exception: 'CancelledError("(\'sort_index-5236dc1a4fdc7dcc69d64af629cdbae1\', 83)")'

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:42031
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:42573
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:45079
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:36203
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:40433
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:35691
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:39497
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:36347
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:46419
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:40483
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:38621
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:43447
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35693
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33857
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33373
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:37317
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:34095
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:41471
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:36805
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:40543
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:45501
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:46139
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:37585
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:36881
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:34877
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:42715
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:39299
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:46879
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:33331
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:35107
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:40075
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:41929
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:38485
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36541
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:43365
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44985
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:45793
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:37633
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:45151
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:41581
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:45953
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:35081
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44981
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:39297
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240271625558, 240582455604)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172199845484, 172510675530)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171267355346, 171578185392)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199552889532, 199863719578)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262962218916, 263273048962)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271043800112, 271354630158)
kwargs:    {}
Exception: 'CancelledError()'

distributed.nanny - INFO - Worker closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175308145944, 175618975990)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (265138029238, 265448859284)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276017080848, 276327910894)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135211070010, 135521900056)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (932490138, 1243320184)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211675261326, 211986091372)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1864980276, 2175810322)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11811541748, 12122371794)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (168780714978, 169091545024)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15852332346, 16163162392)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (2486640368, 2797470414)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227216763626, 227527593672)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7770751150, 8081581196)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15230672254, 15541502300)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169091545024, 169402375070)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55949408280, 56260238326)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68071780074, 68382610120)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (8703241288, 9014071334)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (72734230764, 73045060810)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238406645282, 238717475328)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29218024324, 29528854370)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1243320184, 1554150230)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (177483956266, 177794786312)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36571
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124021188354, 124332018400)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7459921104, 7770751150)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257988938180, 258299768226)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229392573948, 229703403994)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9014071334, 9324901380)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (226284273488, 226595103534)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235298344822, 235609174868)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13054861932, 13365691978)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92316523662, 92627353708)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110033836284, 110344666330)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44517
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11500711702, 11811541748)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138630200516, 138941030562)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119358737664, 119669567710)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20825613082, 21136443128)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133656919780, 133967749826)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20203952990, 20514783036)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35745455290, 36056285336)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132413599596, 132724429642)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130548619320, 130859449366)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (120602057848, 120912887894)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248042376708, 248353206754)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132724429642, 133035259688)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244623246202, 244934076248)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32947984876, 33258814922)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22690593358, 23001423404)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6216600920, 6527430966)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239339135420, 239649965466)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138941030562, 139251860608)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250839847122, 251150677168)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130859449366, 131170279412)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255813127858, 256123957904)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (127751148906, 128061978952)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247420716616, 247731546662)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121845378032, 122156208078)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194890438842, 195201268888)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133346089734, 133656919780)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36056285336, 36367115382)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32015494738, 32326324784)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18960632806, 19271462852)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (93249013800, 93559843846)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280057871446, 280368701492)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13987352070, 14298182116)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240893285650, 241204115696)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122467038124, 122777868170)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249907356984, 250218187030)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135832730102, 136143560148)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244312416156, 244623246202)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (129926959228, 130237789274)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125886168630, 126196998676)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (137697710378, 138008540424)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131170279412, 131481109458)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (266381349422, 266692179468)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23933913542, 24244743588)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254880637720, 255191467766)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217270202154, 217581032200)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12744031886, 13054861932)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244934076248, 245244906294)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82991622282, 83302452328)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122156208078, 122467038124)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (37299605520, 37610435566)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292491073286, 292801903332)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78950831684, 79261661730)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260475578548, 260786408594)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (10257391518, 10568221564)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (0, 310830046)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245244906294, 245555736340)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250218187030, 250529017076)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121223717940, 121534547986)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75531701178, 75842531224)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (176862296174, 177173126220)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249596526938, 249907356984)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292180243240, 292491073286)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288761112734, 289071942780)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166604904656, 166915734702)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288139452642, 288450282688)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59679368832, 59990198878)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (90140713340, 90451543386)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125264508538, 125575338584)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (222554312936, 222865142982)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156036683092, 156347513138)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294666883608, 294977713654)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206701980590, 207012810636)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237163325098, 237474155144)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128683639044, 128994469090)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160699133782, 161009963828)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (94803164030, 95113994076)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64963479614, 65274309660)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (140495180792, 140806010838)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (252083167306, 252393997352)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228770913856, 229081743902)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149820082172, 150130912218)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221000162706, 221310992752)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (218824352384, 219135182430)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128994469090, 129305299136)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74910041086, 75220871132)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231257554224, 231568384270)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (258299768226, 258610598272)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242136605834, 242447435880)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (233122534500, 233433364546)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62166009200, 62476839246)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291869413194, 292180243240)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (224108463166, 224419293212)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (85167432604, 85478262650)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193336288612, 193647118658)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157901663368, 158212493414)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138319370470, 138630200516)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238095815236, 238406645282)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73045060810, 73355890856)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123088698216, 123399528262)
kwargs:    {}
Exception: "RuntimeError('cannot schedule new futures after shutdown')"

distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:38981'
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:39463'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:45041'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:43973'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:33963'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:38551'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:45947'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:42981'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:46533'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:39907'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:37261'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:42125'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:36039'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:41209'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:37669'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:33763'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:36099'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:38093'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:33199'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:37637'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:45611'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35387'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:41451'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:46859'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:45545'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:33739'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:37341'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:45539'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36221'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41629'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:41151'
distributed.dask_worker - INFO - End worker
logout
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:45971
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:32935'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:33567'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42009'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42691'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:41307'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46303'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:41597'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:35303'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46235'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:44199'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:35817'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42629'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46481'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:44207'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:43529'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:44489'
distributed.dask_worker - INFO - End worker
logout
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:35149
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:35089'
distributed.dask_worker - INFO - End worker
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:40623'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:34653'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:44253'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:41661'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:38999'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:34833'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:35747'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:45281'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:46667'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:34073'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:36473'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:36033'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:42503'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:38605'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:42605'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:43331'
distributed.dask_worker - INFO - Timed out starting worker
distributed.dask_worker - INFO - End worker
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:37707'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:36279'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:43617'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:41967'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:43289'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:38745'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:40049'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:38603'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:35383'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:33755'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:36021'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:34765'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:44687'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:34475'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:46393'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.18:44297'
distributed.dask_worker - INFO - Timed out starting worker
distributed.dask_worker - INFO - End worker
logout

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920262: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:24:59 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:25:02 2022
                            <40*lassen28>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:25:02 2022
Terminated at Sat Sep 17 19:25:39 2022
Results reported at Sat Sep 17 19:25:39 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:45847 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.16 sec.
    Max Memory :                                 -
    Average Memory :                             -
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              -
    Max Threads :                                -
    Run time :                                   37 sec.
    Turnaround time :                            40 sec.

The output (if any) is above this job summary.

distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:44307'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43511'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43707'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:35537'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:40109'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:37301'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:39865'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:37701'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:33031'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43499'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:34005'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:41221'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43449'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:43435'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:37265'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.27:45231'
distributed.dask_worker - INFO - Timed out starting worker
distributed.dask_worker - INFO - End worker
logout

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920259: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:23:26 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:23:29 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:23:29 2022
Terminated at Sat Sep 17 19:25:42 2022
Results reported at Sat Sep 17 19:25:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44357 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.17 sec.
    Max Memory :                                 59 MB
    Average Memory :                             56.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   133 sec.
    Turnaround time :                            136 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920258: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:23:26 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:23:29 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:23:29 2022
Terminated at Sat Sep 17 19:25:42 2022
Results reported at Sat Sep 17 19:25:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44157 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.21 sec.
    Max Memory :                                 59 MB
    Average Memory :                             56.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   133 sec.
    Turnaround time :                            136 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920257: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:23:26 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:23:29 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:23:29 2022
Terminated at Sat Sep 17 19:25:42 2022
Results reported at Sat Sep 17 19:25:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:45701 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.18 sec.
    Max Memory :                                 59 MB
    Average Memory :                             56.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   133 sec.
    Turnaround time :                            136 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920264: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:24:59 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:25:02 2022
                            <40*lassen18>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:25:02 2022
Terminated at Sat Sep 17 19:26:13 2022
Results reported at Sat Sep 17 19:26:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:34579 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.14 sec.
    Max Memory :                                 59 MB
    Average Memory :                             52.57 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   70 sec.
    Turnaround time :                            74 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920263: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:24:59 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:25:02 2022
                            <40*lassen27>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:25:02 2022
Terminated at Sat Sep 17 19:26:13 2022
Results reported at Sat Sep 17 19:26:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:46661 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   0.20 sec.
    Max Memory :                                 59 MB
    Average Memory :                             52.57 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   70 sec.
    Turnaround time :                            74 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:32801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:37113'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45395'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38939'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44397'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36871
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43621
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36871
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43621
distributed.worker - INFO -          dashboard at:        192.168.64.30:46807
distributed.worker - INFO -          dashboard at:        192.168.64.30:46213
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-b9og9wzc
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q4p_vheh
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45715
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37923
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45715
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34217
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37923
distributed.worker - INFO -          dashboard at:        192.168.64.30:45659
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34217
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -          dashboard at:        192.168.64.30:43335
distributed.worker - INFO -          dashboard at:        192.168.64.30:43779
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42525
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42525
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-n5kmunaa
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fj52w2zs
distributed.worker - INFO -          dashboard at:        192.168.64.30:36809
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-geno86vl
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39395
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33273
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39395
distributed.worker - INFO -          dashboard at:        192.168.64.30:45767
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33273
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -          dashboard at:        192.168.64.30:43199
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44591
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2lk_ep0c
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44591
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:41397
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44079
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a37rcs35
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44079
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:37117
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-j37lcuw0
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-b3tayyqs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-cr25mcf6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:46271
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42853
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34453
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:46271
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42853
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34453
distributed.worker - INFO -          dashboard at:        192.168.64.30:37755
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36863
distributed.worker - INFO -          dashboard at:        192.168.64.30:32983
distributed.worker - INFO -          dashboard at:        192.168.64.30:40673
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33955
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36863
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -          dashboard at:        192.168.64.30:34219
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36181
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36181
distributed.worker - INFO -          dashboard at:        192.168.64.30:44595
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wdqq6u4f
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33955
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8l6s6mst
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:36387
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z6zpgme3
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:45847
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hj63zm9f
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-dtdxpg1t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-07b0peow
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:45847
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41197'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:32931'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33083'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37933'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42249'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40599'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40117'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46503'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37631'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41119'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41985'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33343'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40945
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34843
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40945
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33877
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34843
distributed.worker - INFO -          dashboard at:        192.168.64.29:45893
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33877
distributed.worker - INFO -          dashboard at:        192.168.64.29:44823
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO -          dashboard at:        192.168.64.29:46593
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-px9ke1_d
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44539
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-puo8erkz
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44539
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45805
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42669
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45805
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38689
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34429
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42669
distributed.worker - INFO -          dashboard at:        192.168.64.29:43529
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34429
distributed.worker - INFO -          dashboard at:        192.168.64.29:33499
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38689
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:41943
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46607
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46607
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-a8wqal0x
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36795
distributed.worker - INFO -          dashboard at:        192.168.64.29:36691
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-nvdh92hb
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36795
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43163
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:40891
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ry2mpfgx
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43857
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43163
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:46853
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:36243
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO -          dashboard at:        192.168.64.29:40143
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44519
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xlxarc83
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44519
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oqbbgv3q
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:42419
distributed.worker - INFO -          dashboard at:        192.168.64.29:36455
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2u6lyqvr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fmohg6kv
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9sh8l6rk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-vxera5i3
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hwcdu5nw
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-irlbbuzz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:32903
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:32903
distributed.worker - INFO -          dashboard at:        192.168.64.29:46239
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_m_7dk3q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34223
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34223
distributed.worker - INFO -          dashboard at:        192.168.64.29:46521
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42011
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42011
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-e23eaxai
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:39683
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-i85welhz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:34579
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39155
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46549
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39155
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46549
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35581
distributed.worker - INFO -          dashboard at:        192.168.64.32:43251
distributed.worker - INFO -          dashboard at:        192.168.64.32:36217
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35581
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40269
distributed.worker - INFO -          dashboard at:        192.168.64.32:40911
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40269
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:43481
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46289
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35911
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46289
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35911
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:45325
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46615
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mo8_wlns
distributed.worker - INFO -          dashboard at:        192.168.64.32:43867
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-cm6l40nf
distributed.worker - INFO -          dashboard at:        192.168.64.32:44989
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:45325
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46615
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43119
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-d2bfmok1
distributed.worker - INFO -          dashboard at:        192.168.64.32:35051
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:39353
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43119
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:40231
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_gzex8g9
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46609
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35337
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46609
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35337
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-crp_mphl
distributed.worker - INFO -          dashboard at:        192.168.64.32:46465
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:41357
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5a_71g3o
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ns46dgb3
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-fdn8fafy
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-k_g4ypwf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0ax8mzob
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nodwwspa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33915
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33915
distributed.worker - INFO -          dashboard at:        192.168.64.32:38643
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35021
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35021
distributed.worker - INFO -          dashboard at:        192.168.64.32:37423
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38003
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38833
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38003
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38833
distributed.worker - INFO -          dashboard at:        192.168.64.32:41693
distributed.worker - INFO -          dashboard at:        192.168.64.32:35649
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41453
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41453
distributed.worker - INFO -          dashboard at:        192.168.64.32:44829
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6b86hemp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6ko8gj7d
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m2_iih0t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ydm_y6ao
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rxbw2f9a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 755.69 MiB from 463 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.52s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 24.81 MiB from 304 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 10.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 93735 was killed by signal 9
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39040 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4372, in _get_data
    await comm.write("OK")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:38964 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:38003 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41106>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Event loop was unresponsive in Worker for 3.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39382 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:41106'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41106>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:35581 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:35581 remote=tcp://192.168.64.32:51802>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - WARNING - Restarting worker
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:51802'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:35581 remote=tcp://192.168.64.32:51802>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:46609 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:46609 remote=tcp://192.168.64.32:37714>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Event loop was unresponsive in Worker for 11.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:37714'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:46609 remote=tcp://192.168.64.32:37714>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:46615 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:46615 remote=tcp://192.168.64.32:42258>: BrokenPipeError: [Errno 32] Broken pipe
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39288 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:42258'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:46615 remote=tcp://192.168.64.32:42258>: BrokenPipeError: [Errno 32] Broken pipe
distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:38992 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 8.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 672, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39318 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:40269 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:40269 remote=tcp://192.168.64.32:33498>: BrokenPipeError: [Errno 32] Broken pipe
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39236 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4372, in _get_data
    await comm.write("OK")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39360 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 672, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39278 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:43119 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43119 remote=tcp://192.168.64.32:56754>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Event loop was unresponsive in Worker for 8.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:56754'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43119 remote=tcp://192.168.64.32:56754>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 672, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39228 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 11.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4372, in _get_data
    await comm.write("OK")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39370 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:33498'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:40269 remote=tcp://192.168.64.32:33498>: BrokenPipeError: [Errno 32] Broken pipe
distributed.core - INFO - Event loop was unresponsive in Worker for 3.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37535
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37535
distributed.worker - INFO -          dashboard at:        192.168.64.32:36393
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5bq_z5kx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:38833 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38833 remote=tcp://192.168.64.32:42622>: BrokenPipeError: [Errno 32] Broken pipe
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4372, in _get_data
    await comm.write("OK")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:39022 remote=tcp://192.168.64.32:46549>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:42622'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38833 remote=tcp://192.168.64.32:42622>: BrokenPipeError: [Errno 32] Broken pipe
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:35337 -> tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:35337 remote=tcp://192.168.64.32:54086>: BrokenPipeError: [Errno 32] Broken pipe
distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:54086'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
BrokenPipeError: [Errno 32] Broken pipe

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:35337 remote=tcp://192.168.64.32:54086>: BrokenPipeError: [Errno 32] Broken pipe
distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 93738 was killed by signal 9
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35021
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4372, in _get_data
    await comm.write("OK")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:37708 remote=tcp://192.168.64.32:35021>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 5.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - WARNING - Restarting worker
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35021
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4372, in _get_data
    await comm.write("OK")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:37872 remote=tcp://192.168.64.32:35021>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 291, in connect
    timeout=min(intermediate_cap, time_left()),
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 423, in wait_for
    raise futures.TimeoutError()
concurrent.futures._base.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4354, in _get_data
    comm = await rpc.connect(worker)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 317, in connect
    ) from active_exception
OSError: Timed out trying to connect to tcp://192.168.64.32:46549 after 30 s
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34711
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34711
distributed.worker - INFO -          dashboard at:        192.168.64.32:42563
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-pn26p7l3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 291, in connect
    timeout=min(intermediate_cap, time_left()),
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 423, in wait_for
    raise futures.TimeoutError()
concurrent.futures._base.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4354, in _get_data
    comm = await rpc.connect(worker)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 317, in connect
    ) from active_exception
OSError: Timed out trying to connect to tcp://192.168.64.32:46549 after 30 s
distributed.client - WARNING - Couldn't gather 6 keys, rescheduling {"('series-sum-agg-b96761d4b9935052c2dc403929f4145c', 0)": (), "('unique-agg-8d66cbd277cf18ca684b4211e266a52c', 0)": (), "('series-count-agg-67dbcf46dab2e6450eef66fa63b4011b', 0)": (), "('unique-agg-009c4e6d0775abc9a368719140a55754', 0)": (), "('series-sum-agg-ff1c1e8df9137355d4b6c2e112ebf6e4', 0)": (), "('series-sum-agg-a36abe34653a91307c42b38bc58cb43b', 0)": ()}
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46549
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 291, in connect
    timeout=min(intermediate_cap, time_left()),
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 423, in wait_for
    raise futures.TimeoutError()
concurrent.futures._base.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4354, in _get_data
    comm = await rpc.connect(worker)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 317, in connect
    ) from active_exception
OSError: Timed out trying to connect to tcp://192.168.64.32:46549 after 30 s
distributed.core - INFO - Event loop was unresponsive in Worker for 8.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 93742 was killed by signal 9
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:46609 -> tcp://192.168.64.32:35337
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:46609 remote=tcp://192.168.64.32:37996>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:37996'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:46609 remote=tcp://192.168.64.32:37996>: Stream is closed
distributed.nanny - WARNING - Restarting worker
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35337
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:54064 remote=tcp://192.168.64.32:35337>: Stream is closed
distributed.client - WARNING - Couldn't gather 1 keys, rescheduling {"('unique-agg-1113a7f2dedbbe9cb93770702450d0de', 0)": ()}
distributed.core - INFO - Event loop was unresponsive in Worker for 11.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35337
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:54226 remote=tcp://192.168.64.32:35337>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 5.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.client - WARNING - Couldn't gather 5 keys, rescheduling {"('series-sum-agg-29850d28277cc293c9f2e74f70ea7004', 0)": (), "('unique-agg-2e8527375850a3cad6fdbfae35e0a5c3', 0)": (), "('series-count-agg-c7f1e069fb2e1ef93a422eb60dc62131', 0)": (), "('series-sum-agg-3ecf56e4a3048cf6159ca696f8ed7b3e', 0)": (), "('series-sum-agg-bbc260aa5b016ada32c62fa026fa6b1f', 0)": ()}
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35789
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35789
distributed.worker - INFO -          dashboard at:        192.168.64.32:32815
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-aaj4exkc
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 4.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35021
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 291, in connect
    timeout=min(intermediate_cap, time_left()),
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 438, in connect
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x2001d4dcd748>: ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4354, in _get_data
    comm = await rpc.connect(worker)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 317, in connect
    ) from active_exception
OSError: Timed out trying to connect to tcp://192.168.64.32:35021 after 30 s
distributed.nanny - INFO - Worker process 93750 was killed by signal 9
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35581
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:52192 remote=tcp://192.168.64.32:35581>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35581
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:51970 remote=tcp://192.168.64.32:35581>: Stream is closed
distributed.nanny - WARNING - Restarting worker
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35581
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:52168 remote=tcp://192.168.64.32:35581>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 4.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:45799
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:45799
distributed.worker - INFO -          dashboard at:        192.168.64.32:45365
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-dy6_ue4n
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 9.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 93775 was killed by signal 9
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46615
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:42358 remote=tcp://192.168.64.32:46615>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:38003 -> tcp://192.168.64.32:46615
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41524>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:41524'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41524>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:37535 -> tcp://192.168.64.32:46615
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:37535 remote=tcp://192.168.64.32:54574>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:54574'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:37535 remote=tcp://192.168.64.32:54574>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:46615
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43174 remote=tcp://192.168.64.32:46615>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4354, in _get_data
    comm = await rpc.connect(worker)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.32:46615 after 30 s
distributed.nanny - WARNING - Restarting worker
distributed.core - INFO - Event loop was unresponsive in Worker for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40673
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40673
distributed.worker - INFO -          dashboard at:        192.168.64.32:38185
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-k0_xfd1u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 728.25 MiB from 494 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - WARNING - Compute Failed
Function:  subgraph_callable-de390e8a-3159-46f0-9d73-4b666aa1
args:      (         index   proc  rank  thread_id  cat  io_cat        tstart          tend  ...                                           filename    size   bandwidth  duration          tmid              file_id              proc_id  _partitions
0       710756  53820     0   31131248    0       2  12293.506836  12293.506836  ...  /p/gpfs1/iopp/temp/1000-genome-haridev/scratch...     956    7.541048  0.000121  122935072585 -7496962032399953040 -8961932103327414197           16
1       710757  53820     0   31131248    0       3  12293.507812  12293.507812  ...  /p/gpfs1/iopp/temp/1000-genome-haridev/scratch...       0    0.000000  0.000008  122935073370 -7496962032399953040 -8961932103327414197           16
2       710758  53820     0   31131248    0       3  12293.507812  12293.507812  ...                       chr10n-235001//chr10.NA19391       0    0.000000  0.000231  122935075294 -9016530150434512095 -8961932103327414197           16
3       710759  53820     0   31131248    0       3  12293.
kwargs:    {}
Exception: "MemoryError((7, 9744965), dtype('int64'))"

distributed.nanny - INFO - Worker process 93759 was killed by signal 9
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:37535 -> tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:37535 remote=tcp://192.168.64.32:54634>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:34318 remote=tcp://192.168.64.32:40269>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:34428 remote=tcp://192.168.64.32:40269>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:54634'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:37535 remote=tcp://192.168.64.32:54634>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:33530 remote=tcp://192.168.64.32:40269>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:38833 -> tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38833 remote=tcp://192.168.64.32:42640>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:33454 remote=tcp://192.168.64.32:40269>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:42640'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38833 remote=tcp://192.168.64.32:42640>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:33730 remote=tcp://192.168.64.32:40269>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:33408 remote=tcp://192.168.64.32:40269>: Stream is closed
distributed.nanny - WARNING - Restarting worker
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:43119 -> tcp://192.168.64.32:40269
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43119 remote=tcp://192.168.64.32:56814>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:56814'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43119 remote=tcp://192.168.64.32:56814>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123088698216, 123399528262)
kwargs:    {}
Exception: "MemoryError((7, 9744965), dtype('int64'))"

distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41425
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41425
distributed.worker - INFO -          dashboard at:        192.168.64.32:35085
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-bqfqg9dh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 6.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://192.168.64.32:58398 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 93768 was killed by signal 9
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:45325 -> tcp://192.168.64.32:35911
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:45325 remote=tcp://192.168.64.32:41244>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:41244'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 973, in _handle_write
    num_bytes = self.write_to_fd(self._write_buffer.peek(size))
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1146, in write_to_fd
    return self.socket.send(data)  # type: ignore
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:45325 remote=tcp://192.168.64.32:41244>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.nanny - WARNING - Restarting worker
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.32:35911
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.32:58760 remote=tcp://192.168.64.32:35911>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:43119 -> tcp://192.168.64.32:35911
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43119 remote=tcp://192.168.64.32:56808>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:56808'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43119 remote=tcp://192.168.64.32:56808>: Stream is closed
distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:38003 -> tcp://192.168.64.32:35911
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41168>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:41168'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41168>: Stream is closed
distributed.comm.tcp - INFO - Connection from tcp://192.168.64.32:42144 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 4.51s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39757
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39757
distributed.worker - INFO -          dashboard at:        192.168.64.32:43981
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q6e7wjfm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:45799 -> tcp://192.168.64.32:39155
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:45799 remote=tcp://192.168.64.32:60194>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:33915 -> tcp://192.168.64.32:39155
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:33915 remote=tcp://192.168.64.32:43794>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:60194'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:45799 remote=tcp://192.168.64.32:60194>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:38003 -> tcp://192.168.64.32:39155
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41188>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:43794'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:33915 remote=tcp://192.168.64.32:43794>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:41188'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38003 remote=tcp://192.168.64.32:41188>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.nanny - INFO - Worker process 93743 was killed by signal 9
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:34711 -> tcp://192.168.64.32:39155
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:34711 remote=tcp://192.168.64.32:58408>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:58408'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:34711 remote=tcp://192.168.64.32:58408>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:45325 -> tcp://192.168.64.32:39155
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:45325 remote=tcp://192.168.64.32:41264>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:41264'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:45325 remote=tcp://192.168.64.32:41264>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - failed during get data with tcp://192.168.64.32:38833 -> tcp://192.168.64.32:39155
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38833 remote=tcp://192.168.64.32:42706>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.core - INFO - Lost connection to 'tcp://192.168.64.32:42706'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:38833 remote=tcp://192.168.64.32:42706>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35357
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35357
distributed.worker - INFO -          dashboard at:        192.168.64.32:41221
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f6zosinb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46661
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920269: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:26:39 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:26:43 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:26:43 2022
Terminated at Sat Sep 17 19:34:32 2022
Results reported at Sat Sep 17 19:34:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:34579 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.79 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1233 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   470 sec.
    Turnaround time :                            473 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920268: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:26:39 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:26:43 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:26:43 2022
Terminated at Sat Sep 17 19:34:32 2022
Results reported at Sat Sep 17 19:34:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:45847 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.83 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   470 sec.
    Turnaround time :                            473 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920267: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 19:26:39 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 19:26:43 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 19:26:43 2022
Terminated at Sat Sep 17 19:34:32 2022
Results reported at Sat Sep 17 19:34:32 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:46661 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.83 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   470 sec.
    Turnaround time :                            473 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45497'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37683'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45865'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37791'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41381'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45513'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34821'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34295'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:43047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:33753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44057'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42825'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:32781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:37947'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33067'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38503'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40913'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39191'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44027'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8l6s6mst', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fj52w2zs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-j37lcuw0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a37rcs35', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wdqq6u4f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-n5kmunaa', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q4p_vheh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hj63zm9f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-cr25mcf6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-dtdxpg1t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-b3tayyqs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z6zpgme3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2lk_ep0c', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-b9og9wzc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-geno86vl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-07b0peow', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5bq_z5kx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6b86hemp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-dy6_ue4n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5a_71g3o', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q6e7wjfm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-aaj4exkc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-nodwwspa', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6ko8gj7d', purging
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-fdn8fafy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m2_iih0t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-bqfqg9dh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-pn26p7l3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-k0_xfd1u', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f6zosinb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ydm_y6ao', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0ax8mzob', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:34391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:33633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39859'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:41407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:33229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:40479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:39039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:36169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:34261'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s3skyykv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-92u1jc8i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-govzgwrb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6skt9hzl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ga401cg1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8mdfep4n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q58amsci', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6xivxwpt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-x0axfk8r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-90v4num1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sqc3_2p4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-j2blzdv4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-00bgmwc_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m7v0w3cx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-omtm9baq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ol63_hx1', purging
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:46369'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39033'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:34461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42455'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:38285'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:38047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:34485'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45095'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42459'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:36315'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35247'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44213'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37715
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:39695
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:39695
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36733
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37715
distributed.worker - INFO -          dashboard at:        192.168.64.31:35051
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36733
distributed.worker - INFO -          dashboard at:        192.168.64.31:34975
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -          dashboard at:        192.168.64.31:36195
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40883
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40883
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:40911
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q58amsci
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:44561
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6xivxwpt
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6skt9hzl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:44561
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34147'
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:32961
distributed.worker - INFO -          dashboard at:        192.168.64.31:43701
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:32961
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:39679
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:39943
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ol63_hx1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40901
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42797
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44887
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37185
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:39943
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40901
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42797
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37185
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44887
distributed.worker - INFO -          dashboard at:        192.168.64.30:44707
distributed.worker - INFO -          dashboard at:        192.168.64.30:44105
distributed.worker - INFO -          dashboard at:        192.168.64.30:41365
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44259'
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:42817
distributed.worker - INFO -          dashboard at:        192.168.64.31:37419
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-x0axfk8r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37003
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40455
distributed.worker - INFO -               Threads:                          1
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34295'
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s3skyykv
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:32991
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40455
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ohkcn6ru
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:32991
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37241
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wtn9p9jd
distributed.worker - INFO -          dashboard at:        192.168.64.30:44999
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37003
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-up1vz8jw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:33109
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42873'
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-92u1jc8i
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-govzgwrb
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -          dashboard at:        192.168.64.30:35373
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:33471
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45817'
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33455
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33455
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:46231
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wbon0tfi
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36249'
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-okubf255
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:33797
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:46231
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-weepy_ps
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42927'
distributed.worker - INFO -          dashboard at:        192.168.64.30:36231
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2qs1s8pi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36313
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34921'
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xqj6dbpr
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36313
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:36323
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36259'
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l51s7wtq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35677'
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l0wuu3a5
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39657'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41209
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41209
distributed.worker - INFO -          dashboard at:        192.168.64.31:41145
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-j2blzdv4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:43985
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45889
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:43985
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45889
distributed.worker - INFO -          dashboard at:        192.168.64.31:35141
distributed.worker - INFO -          dashboard at:        192.168.64.31:36991
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40869
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40869
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.31:40291
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-omtm9baq
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ga401cg1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:44965
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:44965
distributed.worker - INFO -          dashboard at:        192.168.64.31:40409
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8mdfep4n
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36665
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m7v0w3cx
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36665
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:34757
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37991
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37991
distributed.worker - INFO -          dashboard at:        192.168.64.31:35969
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sqc3_2p4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-00bgmwc_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41811
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41811
distributed.worker - INFO -          dashboard at:        192.168.64.31:44081
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-90v4num1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:46709
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:46709
distributed.worker - INFO -          dashboard at:        192.168.64.30:43597
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:46345
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:46345
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:40951
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v6m9y63a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fplrnukj
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44039
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44039
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42057
distributed.worker - INFO -          dashboard at:        192.168.64.30:35955
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42057
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37055
distributed.worker - INFO -          dashboard at:        192.168.64.30:42847
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37055
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:36879
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_nftncg4
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-10mr9g20
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-g1a8sdd5
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34535
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34535
distributed.worker - INFO -          dashboard at:        192.168.64.30:45259
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e794zxpt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-puo8erkz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oqbbgv3q', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-nvdh92hb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-e23eaxai', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ry2mpfgx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-vxera5i3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-a8wqal0x', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-i85welhz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9sh8l6rk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2u6lyqvr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-px9ke1_d', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_m_7dk3q', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hwcdu5nw', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-irlbbuzz', purging
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fmohg6kv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xlxarc83', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:40977'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:40479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:33441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:39981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:39163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:38831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:38883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:42587'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:41315'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:45645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:40559'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:46073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:39165'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:34321
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:34321
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43569
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35717
distributed.worker - INFO -          dashboard at:        192.168.64.28:41171
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43569
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35717
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -          dashboard at:        192.168.64.28:43737
distributed.worker - INFO -          dashboard at:        192.168.64.28:36771
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40563
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40563
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:34171
distributed.worker - INFO -          dashboard at:        192.168.64.28:37283
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:34171
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-58iq3auk
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -          dashboard at:        192.168.64.28:35521
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7uxvnnfy
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5onh8ut3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:37615
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:37615
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43631
distributed.worker - INFO -          dashboard at:        192.168.64.28:41415
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43631
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:37405
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5l3ao5kr
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.28:42101
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40287
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:37405
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-1t9x6n84
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:44617
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:38623
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:33905
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45677
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:41431
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45677
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:33905
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:41431
distributed.worker - INFO -          dashboard at:        192.168.64.28:39201
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-aqmq811i
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:36381
distributed.worker - INFO -          dashboard at:        192.168.64.28:41357
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-af23v4p4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45083
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45083
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ouwltgwo
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:35493
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-iuvz21wc
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-85bpo6x7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f764__y0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-g03iv8bi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gkj6fixs
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43705
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43705
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:45265
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q_au32yo
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40639
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40639
distributed.worker - INFO -          dashboard at:        192.168.64.28:45625
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44665
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44665
distributed.worker - INFO -          dashboard at:        192.168.64.28:42819
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35367
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-cy_lcrdf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0r8rgn5c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43895'
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35367
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_nftncg4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fplrnukj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-up1vz8jw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e794zxpt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xmbqorly', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-weepy_ps', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rqf_m_kl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v6m9y63a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l51s7wtq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wtn9p9jd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xqj6dbpr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-16rwd8k1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l0wuu3a5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4m34i4sn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0_4r3cke', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fo8d3i2m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dz8nd_ur', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gjfrxhbk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rf1t6m3a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ohkcn6ru', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-g1a8sdd5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wbon0tfi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2qs1s8pi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-10mr9g20', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-okubf255', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yhshf999', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lwacqlio', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rdi4yphx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hcyo_r7q', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3ho42sqi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hhrlyf97', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-do2hg4iv', purging
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43331
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44099
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43331
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44099
distributed.worker - INFO -          dashboard at:        192.168.64.29:32817
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -          dashboard at:        192.168.64.29:45285
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39425
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37473
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39425
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41423
distributed.worker - INFO -          dashboard at:        192.168.64.29:45499
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39109
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hhrlyf97
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39109
distributed.worker - INFO -          dashboard at:        192.168.64.29:42793
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35815
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33611
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35815
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35753
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33611
distributed.worker - INFO -          dashboard at:        192.168.64.29:43281
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35753
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yhshf999
distributed.worker - INFO -          dashboard at:        192.168.64.29:46343
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:41481
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37205
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41963
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lwacqlio
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37205
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:46067
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41963
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37473
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-do2hg4iv
distributed.worker - INFO -          dashboard at:        192.168.64.29:42869
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:38255
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43733
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gjfrxhbk
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4m34i4sn
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3ho42sqi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43733
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:41549
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41423
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-16rwd8k1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fo8d3i2m
distributed.worker - INFO -          dashboard at:        192.168.64.29:38757
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rf1t6m3a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xmbqorly
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rqf_m_kl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38425
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44163
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44163
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38425
distributed.worker - INFO -          dashboard at:        192.168.64.29:33047
distributed.worker - INFO -          dashboard at:        192.168.64.29:40011
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rdi4yphx
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hcyo_r7q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36761
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36761
distributed.worker - INFO -          dashboard at:        192.168.64.29:36443
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45269
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45269
distributed.worker - INFO -          dashboard at:        192.168.64.29:34775
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0_4r3cke
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dz8nd_ur
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46233
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46233
distributed.worker - INFO -          dashboard at:        192.168.64.32:43181
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40087
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40087
distributed.worker - INFO -          dashboard at:        192.168.64.32:34993
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xwe6m628
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35337
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35337
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46799
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-56hji0z6
distributed.worker - INFO -          dashboard at:        192.168.64.32:40239
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:39817
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44443
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44443
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:40071
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ama90rqz
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39171
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34499
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-vhyb0jz3
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-icpbr57h
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39171
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34499
distributed.worker - INFO -          dashboard at:        192.168.64.32:38449
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:45411
distributed.worker - INFO -          dashboard at:        192.168.64.32:36983
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1p08v4kt
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-tww80pvx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43901
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43901
distributed.worker - INFO -          dashboard at:        192.168.64.32:33801
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-cqdv9dpq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-budjmd67
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33113
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33113
distributed.worker - INFO -          dashboard at:        192.168.64.32:42945
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fva5ptjn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38025
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:45633
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:45633
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40385
distributed.worker - INFO -          dashboard at:        192.168.64.34:40583
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40385
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38025
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -          dashboard at:        192.168.64.34:46171
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:32835
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-h2ovyw0d
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-p6u07qws
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7h0_1adu
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38809
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38809
distributed.worker - INFO -          dashboard at:        192.168.64.34:45281
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:36099
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:36099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:42619
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:43479
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:45923
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:42619
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:35007
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40287
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:43479
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:37409
distributed.worker - INFO -          dashboard at:        192.168.64.34:40817
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:37409
distributed.worker - INFO -          dashboard at:        192.168.64.34:37641
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:46095
distributed.worker - INFO -          dashboard at:        192.168.64.34:38029
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:42213
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44723
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:42213
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hzg6ot7k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38727
distributed.worker - INFO -          dashboard at:        192.168.64.34:45041
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44723
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:46095
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38727
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.34:33029
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.34:35923
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:33627
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-w3_u_3gh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:40357
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bij4n0n4
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-54fhlfle
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-l6d6fnu4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40329
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:33627
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ttoqr4sa
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40329
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yl0vxh6u
distributed.worker - INFO -          dashboard at:        192.168.64.34:35019
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:37505
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-g4mlx8d4
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-j2880ow2
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fw102eg2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fyb4n1ax
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ptm6k0z_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40481
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40481
distributed.worker - INFO -          dashboard at:        192.168.64.34:36305
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-wfzwik7h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35065
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35065
distributed.worker - INFO -          dashboard at:        192.168.64.32:41975
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-h2opler6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35863
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35863
distributed.worker - INFO -          dashboard at:        192.168.64.32:43409
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-a5pt7ezv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44267
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44267
distributed.worker - INFO -          dashboard at:        192.168.64.32:37269
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6pevnf21
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33537
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33537
distributed.worker - INFO -          dashboard at:        192.168.64.32:33455
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7jtquekt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44757
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44757
distributed.worker - INFO -          dashboard at:        192.168.64.32:34073
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nbjp9tq1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35761
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35761
distributed.worker - INFO -          dashboard at:        192.168.64.32:33005
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ows2ys1j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38857
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44019
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 763.19 MiB from 529 reference cycles (threshold: 9.54 MiB)
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:56540 remote=tcp://192.168.64.232:35367>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:56560 remote=tcp://192.168.64.232:35367>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:56574 remote=tcp://192.168.64.232:35367>: Stream is closed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.28:56330 remote=tcp://192.168.64.232:35367>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:42964 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:42966 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:42962 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:42960 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.32:42968 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4a8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38288 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f438>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38286 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f438>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38292 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38290 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f3c8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38278 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42984 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f358>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42994 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f470>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38296 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70278>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38282 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70240>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38294 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f2e8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38300 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001fb6048>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
  tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f550>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42990 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f1d0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42988 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f3c8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43000 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b5b828>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43002 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
  convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38312 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f438>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38298 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70320>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38280 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38306 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001d07c50>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38304 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001d08c18>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38310 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f438>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.34:38302 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f2e8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42996 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f550>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42992 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:43004 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4a8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42998 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70438>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.32:42986 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37644 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37642 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37656 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37646 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37650 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37654 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37658 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37652 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.30:37648 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250218187030, 250529017076)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:37686 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f3c8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:37680 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4e0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:37690 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f2e8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:37682 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:37688 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70358>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.30:37684 remote=tcp://192.168.64.232:44019>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:44019 after 30 s
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254569807674, 254880637720)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (54084428004, 54395258050)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36056285336, 36367115382)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249285696892, 249596526938)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257678108134, 257988938180)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33258814922, 33569644968)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306478425356, 306789255402)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55949408280, 56260238326)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28907194278, 29218024324)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272287120296, 272597950342)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288761112734, 289071942780)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36367115382, 36677945428)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (251461507214, 251772337260)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 282, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 905, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 672, in send_recv
    await comm.write(msg, serializers=serializers, on_error="raise")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 298, in write
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://192.168.64.31:49868 remote=tcp://192.168.64.232:35367>: Stream is closed
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203282850084, 203593680130)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309586725816, 309897555862)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (103817235364, 104128065410)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274773760664, 275084590710)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (139873520700, 140184350746)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304613445080, 304924275126)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (67450119982, 67760950028)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (66828459890, 67139289936)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255502297812, 255813127858)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62787669292, 63098499338)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206391150544, 206701980590)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128994469090, 129305299136)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228149253764, 228460083810)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276638740940, 276949570986)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136143560148, 136454390194)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64963479614, 65274309660)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253326487490, 253637317536)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99465614720, 99776444766)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (19893122944, 20203952990)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239028305374, 239339135420)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51597787636, 51908617682)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204215340222, 204526170268)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (141738500976, 142049331022)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (153860872770, 154171702816)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74599211040, 74910041086)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158834153506, 159144983552)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123088698216, 123399528262)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f550>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33122 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b702b0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33144 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33126 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b702e8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33130 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4a8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33150 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4a8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33134 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f470>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33132 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33120 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f240>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33128 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f470>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33140 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f4e0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33146 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f550>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33148 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f2b0>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33142 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f588>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33138 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (219446012476, 219756842522)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317668307012, 317979137058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170334865208, 170645695254)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (25488063772, 25798893818)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30150514462, 30461344508)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (63409329384, 63720159430)
kwargs:    {}
Exception: 'CancelledError()'

tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f3c8>>, <Task finished coro=<Worker.heartbeat() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:1321> exception=OSError('Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 869, in _read_to_buffer
    bytes_read = self.read_from_fd(buf)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/iostream.py", line 1138, in read_from_fd
    return self.socket.recv_into(buf, len(buf))
ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 326, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/usr/tce/packages/python/python-3.7.2/lib/python3.7/asyncio/tasks.py", line 416, in wait_for
    return fut.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 127, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.29:33136 remote=tcp://192.168.64.232:38857>: ConnectionResetError: [Errno 104] Connection reset by peer

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1370, in heartbeat
    raise e
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1336, in heartbeat
    for key in self.active_keys
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 902, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 1072, in connect
    comm = await fut
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/core.py", line 333, in connect
    ) from exc
OSError: Timed out during handshake while connecting to tcp://192.168.64.232:38857 after 30 s
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (273530440480, 273841270526)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253015657444, 253326487490)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (103195575272, 103506405318)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (129926959228, 130237789274)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51908617682, 52219447728)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (178416446404, 178727276450)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (38853755750, 39164585796)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (222243482890, 222554312936)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (269800479928, 270111309974)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122467038124, 122777868170)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (70869250488, 71180080534)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260786408594, 261097238640)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308965065724, 309275895770)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301505144620, 301815974666)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (258610598272, 258921428318)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (88897393156, 89208223202)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208566960866, 208877790912)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64030989476, 64341819522)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227838423718, 228149253764)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130548619320, 130859449366)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245244906294, 245555736340)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217891862246, 218202692292)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276949570986, 277260401032)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106303875732, 106614705778)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123399528262, 123710358308)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207323640682, 207634470728)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (259853918456, 260164748502)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175618975990, 175929806036)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (205458660406, 205769490452)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111277156468, 111587986514)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50354467452, 50665297498)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313627516414, 313938346460)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227216763626, 227527593672)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (162253284012, 162564114058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (246799056524, 247109886570)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147955101896, 148265931942)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (141116840884, 141427670930)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (65895969752, 66206799798)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (71490910580, 71801740626)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69936760350, 70247590396)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299950994390, 300261824436)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (129305299136, 129616129182)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154793362908, 155104192954)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214472731740, 214783561786)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309897555862, 310208385908)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171889015438, 172199845484)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59990198878, 60301028924)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (85789092696, 86099922742)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201107039762, 201417869808)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242447435880, 242758265926)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36988775474, 37299605520)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262651388870, 262962218916)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22690593358, 23001423404)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198620399394, 198931229440)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (227527593672, 227838423718)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144846801436, 145157631482)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197998739302, 198309569348)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111587986514, 111898816560)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (273219610434, 273530440480)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56571068372, 56881898418)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115007117020, 115317947066)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101952255088, 102263085134)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (192714628520, 193025458566)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211675261326, 211986091372)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303370124896, 303680954942)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (102573915180, 102884745226)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (120602057848, 120912887894)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49732807360, 50043637406)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138941030562, 139251860608)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (71180080534, 71490910580)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (223797633120, 224108463166)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268246329698, 268557159744)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (178727276450, 179038106496)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (168469884932, 168780714978)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188984667968, 189295498014)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305235105172, 305545935218)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305856765264, 306167595310)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (190227988152, 190538818198)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (243379926018, 243690756064)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74288380994, 74599211040)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73977550948, 74288380994)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (84856602558, 85167432604)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271976290250, 272287120296)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (277882061124, 278192891170)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (192403798474, 192714628520)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (224730123258, 225040953304)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30461344508, 30772174554)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (104128065410, 104438895456)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (65585139706, 65895969752)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (54395258050, 54706088096)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156347513138, 156658343184)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199863719578, 200174549624)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (137697710378, 138008540424)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (139562690654, 139873520700)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196755419118, 197066249164)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169713205116, 170024035162)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300572654482, 300883484528)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296531863884, 296842693930)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289071942780, 289382772826)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125886168630, 126196998676)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (118737077572, 119047907618)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (141427670930, 141738500976)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298086014114, 298396844160)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288450282688, 288761112734)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165050754426, 165361584472)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (226284273488, 226595103534)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291558583148, 291869413194)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216959372108, 217270202154)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290004432918, 290315262964)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31083004600, 31393834646)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33880475014, 34191305060)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53151937866, 53462767912)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202661189992, 202972020038)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268867989790, 269178819836)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (190538818198, 190849648244)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297464354022, 297775184068)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73666720902, 73977550948)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (108168856008, 108479686054)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301194314574, 301505144620)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261718898732, 262029728778)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68693440166, 69004270212)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130859449366, 131170279412)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158523323460, 158834153506)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (219756842522, 220067672568)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169402375070, 169713205116)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115628777112, 115939607158)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (80815811960, 81126642006)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247109886570, 247420716616)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261097238640, 261408068686)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295599373746, 295910203792)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210121111096, 210431941142)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193647118658, 193957948704)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272908780388, 273219610434)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201417869808, 201728699854)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73355890856, 73666720902)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (195201268888, 195512098934)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28285534186, 28596364232)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (107236365870, 107547195916)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64652649568, 64963479614)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (129616129182, 129926959228)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (38232095658, 38542925704)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307100085448, 307410915494)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68071780074, 68382610120)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39475415842, 39786245888)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (130237789274, 130548619320)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (169091545024, 169402375070)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245555736340, 245866566386)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (243069095972, 243379926018)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43827036486, 44137866532)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207945300774, 208256130820)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297153523976, 297464354022)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294666883608, 294977713654)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (192092968428, 192403798474)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (277260401032, 277571231078)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58436048648, 58746878694)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (150752572310, 151063402356)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147644271850, 147955101896)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105060555548, 105371385594)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (258921428318, 259232258364)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24866403680, 25177233726)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216648542062, 216959372108)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (224419293212, 224730123258)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26731383956, 27042214002)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87654072972, 87964903018)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145157631482, 145468461528)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221000162706, 221310992752)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173132335622, 173443165668)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (68382610120, 68693440166)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206701980590, 207012810636)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (71801740626, 72112570672)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78018341546, 78329171592)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133656919780, 133967749826)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101019764950, 101330594996)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303991784988, 304302615034)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (222865142982, 223175973028)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (40718736026, 41029566072)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241514945742, 241825775788)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203904510176, 204215340222)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112209646606, 112520476652)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116872097296, 117182927342)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292801903332, 293112733378)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315803326736, 316114156782)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313938346460, 314249176506)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82059132144, 82369962190)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314870836598, 315181666644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271043800112, 271354630158)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156036683092, 156347513138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69004270212, 69315100258)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276327910894, 276638740940)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313005856322, 313316686368)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312073366184, 312384196230)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171578185392, 171889015438)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197066249164, 197377079210)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160077473690, 160388303736)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47246166992, 47556997038)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99776444766, 100087274812)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (246177396432, 246488226478)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (158212493414, 158523323460)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (54706088096, 55016918142)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (153239212678, 153550042724)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27974704140, 28285534186)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217270202154, 217581032200)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (140806010838, 141116840884)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (195822928980, 196133759026)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210431941142, 210742771188)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225662613396, 225973443442)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113763796836, 114074626882)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78640001638, 78950831684)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (218202692292, 218513522338)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174375655806, 174686485852)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194890438842, 195201268888)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82991622282, 83302452328)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61855179154, 62166009200)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35123795198, 35434625244)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (200796209716, 201107039762)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135832730102, 136143560148)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122156208078, 122467038124)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23623083496, 23933913542)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298707674206, 299018504252)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291869413194, 292180243240)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157280003276, 157590833322)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142360161068, 142670991114)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247731546662, 248042376708)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275706250802, 276017080848)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20203952990, 20514783036)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294977713654, 295288543700)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225351783350, 225662613396)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109412176192, 109723006238)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (79261661730, 79572491776)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (83302452328, 83613282374)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119980397756, 120291227802)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267003009514, 267313839560)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128683639044, 128994469090)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35434625244, 35745455290)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316424986828, 316735816874)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (35745455290, 36056285336)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (127129488814, 127440318860)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (127751148906, 128061978952)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138630200516, 138941030562)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61544349108, 61855179154)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (51286957590, 51597787636)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31704664692, 32015494738)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302748464804, 303059294850)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56260238326, 56571068372)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174064825760, 174375655806)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208877790912, 209188620958)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221932652844, 222243482890)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (79572491776, 79883321822)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303680954942, 303991784988)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (263273048962, 263583879008)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (159766643644, 160077473690)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126196998676, 126507828722)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75531701178, 75842531224)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (60922689016, 61233519062)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124642848446, 124953678492)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290936923056, 291247753102)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (167537394794, 167848224840)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310208385908, 310519215954)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148576761988, 148887592034)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75842531224, 76153361270)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257367278088, 257678108134)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42583716302, 42894546348)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198931229440, 199242059486)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (268557159744, 268867989790)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191471308336, 191782138382)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (213540241602, 213851071648)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221310992752, 221621822798)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (176240636082, 176551466128)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125264508538, 125575338584)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239649965466, 239960795512)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59368538786, 59679368832)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113452966790, 113763796836)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (246488226478, 246799056524)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199242059486, 199552889532)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32947984876, 33258814922)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267313839560, 267624669606)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308032575586, 308343405632)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23312253450, 23623083496)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (203593680130, 203904510176)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (88586563110, 88897393156)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46002846808, 46313676854)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46313676854, 46624506900)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (61233519062, 61544349108)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255813127858, 256123957904)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (34812965152, 35123795198)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191782138382, 192092968428)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (76464191316, 76775021362)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (146400951666, 146711781712)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (153550042724, 153860872770)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157590833322, 157901663368)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (201728699854, 202039529900)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161631623920, 161942453966)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189295498014, 189606328060)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175308145944, 175618975990)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306789255402, 307100085448)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (297775184068, 298086014114)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214161901694, 214472731740)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (264827199192, 265138029238)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311451706092, 311762536138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41651226164, 41962056210)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135521900056, 135832730102)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (272597950342, 272908780388)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119047907618, 119358737664)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225040953304, 225351783350)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197687909256, 197998739302)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121845378032, 122156208078)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (79883321822, 80194151868)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238717475328, 239028305374)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294356053562, 294666883608)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311140876046, 311451706092)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (198309569348, 198620399394)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (265759689330, 266070519376)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (36677945428, 36988775474)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310830046000, 311140876046)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (21136443128, 21447273174)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300261824436, 300572654482)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152928382632, 153239212678)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144535971390, 144846801436)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50043637406, 50354467452)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (206080320498, 206391150544)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (137076050286, 137386880332)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58746878694, 59057708740)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (60611858970, 60922689016)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43516206440, 43827036486)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (83924112420, 84234942466)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (60301028924, 60611858970)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121223717940, 121534547986)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (150441742264, 150752572310)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193025458566, 193336288612)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274462930618, 274773760664)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241204115696, 241514945742)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (114696286974, 115007117020)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (263583879008, 263894709054)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (226905933580, 227216763626)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302126804712, 302437634758)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (177173126220, 177483956266)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (140495180792, 140806010838)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (93559843846, 93870673892)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (127440318860, 127751148906)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (293112733378, 293423563424)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (226595103534, 226905933580)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (183389727140, 183700557186)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (190849648244, 191160478290)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316735816874, 317046646920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52841107820, 53151937866)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100087274812, 100398104858)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62166009200, 62476839246)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (85167432604, 85478262650)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (45692016762, 46002846808)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239339135420, 239649965466)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271665460204, 271976290250)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115939607158, 116250437204)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (271354630158, 271665460204)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170956525300, 171267355346)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (264205539100, 264516369146)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (125575338584, 125886168630)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (73045060810, 73355890856)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (72734230764, 73045060810)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (303059294850, 303370124896)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131170279412, 131481109458)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278192891170, 278503721216)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100398104858, 100708934904)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29839684416, 30150514462)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289382772826, 289693602872)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (205769490452, 206080320498)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (107858025962, 108168856008)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (159455813598, 159766643644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216337712016, 216648542062)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (107547195916, 107858025962)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112520476652, 112831306698)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57503558510, 57814388556)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (104749725502, 105060555548)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (84234942466, 84545772512)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (186498027600, 186808857646)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32015494738, 32326324784)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (72112570672, 72423400718)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (104438895456, 104749725502)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240271625558, 240582455604)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133035259688, 133346089734)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (102263085134, 102573915180)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (289693602872, 290004432918)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299329334298, 299640164344)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315181666644, 315492496690)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173443165668, 173753995714)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166915734702, 167226564748)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (65274309660, 65585139706)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (81437472052, 81748302098)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69315100258, 69625930304)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (205147830360, 205458660406)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267935499652, 268246329698)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (265138029238, 265448859284)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196133759026, 196444589072)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209810281050, 210121111096)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57192728464, 57503558510)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204526170268, 204837000314)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (118426247526, 118737077572)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (143914311298, 144225141344)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (72423400718, 72734230764)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242136605834, 242447435880)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101641425042, 101952255088)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (212918581510, 213229411556)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (195512098934, 195822928980)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184633047324, 184943877370)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (223486803074, 223797633120)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (86721582834, 87032412880)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (302437634758, 302748464804)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (34502135106, 34812965152)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (225973443442, 226284273488)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310519215954, 310830046000)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (261408068686, 261718898732)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165983244564, 166294074610)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (58125218602, 58436048648)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (93249013800, 93559843846)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96357314260, 96668144306)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133346089734, 133656919780)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (243690756064, 244001586110)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (200174549624, 200485379670)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (117804587434, 118115417480)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290315262964, 290626093010)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194268778750, 194579608796)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172821505576, 173132335622)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172199845484, 172510675530)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59679368832, 59990198878)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (301815974666, 302126804712)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (180903086772, 181213916818)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106925535824, 107236365870)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (70247590396, 70558420442)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (84545772512, 84856602558)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (122777868170, 123088698216)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (21758103220, 22068933266)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26109723864, 26420553910)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29218024324, 29528854370)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215405221878, 215716051924)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27663874094, 27974704140)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (259232258364, 259543088410)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287828622596, 288139452642)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312695026276, 313005856322)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (220378502614, 220689332660)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55327748188, 55638578234)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96978974352, 97289804398)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (245866566386, 246177396432)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161942453966, 162253284012)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (223175973028, 223486803074)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110655496376, 110966326422)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (208256130820, 208566960866)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (106614705778, 106925535824)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142670991114, 142981821160)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (74910041086, 75220871132)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (306167595310, 306478425356)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193336288612, 193647118658)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174686485852, 174997315898)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87343242926, 87654072972)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23001423404, 23312253450)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189917158106, 190227988152)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27042214002, 27353044048)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32326324784, 32637154830)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (63720159430, 64030989476)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215716051924, 216026881970)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308343405632, 308654235678)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (294045223516, 294356053562)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (37299605520, 37610435566)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (185565537462, 185876367508)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (103506405318, 103817235364)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (76153361270, 76464191316)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260164748502, 260475578548)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292180243240, 292491073286)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (134900239964, 135211070010)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39164585796, 39475415842)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (266381349422, 266692179468)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (191160478290, 191471308336)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (222554312936, 222865142982)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (298396844160, 298707674206)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (69625930304, 69936760350)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138319370470, 138630200516)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (242758265926, 243069095972)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119358737664, 119669567710)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (111898816560, 112209646606)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142049331022, 142360161068)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (175929806036, 176240636082)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (162874944104, 163185774150)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (159144983552, 159455813598)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (25177233726, 25488063772)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (146711781712, 147022611758)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82680792236, 82991622282)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (64341819522, 64652649568)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296221033838, 296531863884)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207012810636, 207323640682)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209188620958, 209499451004)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (194579608796, 194890438842)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (224108463166, 224419293212)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78950831684, 79261661730)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (171267355346, 171578185392)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (77396681454, 77707511500)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (120291227802, 120602057848)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (100708934904, 101019764950)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (134589409918, 134900239964)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (89208223202, 89519053248)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (113142136744, 113452966790)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (66517629844, 66828459890)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148887592034, 149198422080)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211364431280, 211675261326)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (316114156782, 316424986828)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262029728778, 262340558824)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126818658768, 127129488814)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248353206754, 248664036800)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (126507828722, 126818658768)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22379763312, 22690593358)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188052177830, 188363007876)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (300883484528, 301194314574)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295910203792, 296221033838)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (76775021362, 77085851408)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13054861932, 13365691978)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13365691978, 13676522024)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (204837000314, 205147830360)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (40097075934, 40407905980)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (93870673892, 94181503938)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15541502300, 15852332346)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11811541748, 12122371794)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (14919842208, 15230672254)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (266692179468, 267003009514)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17095652530, 17406482576)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (219135182430, 219446012476)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314249176506, 314560006552)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (114074626882, 114385456928)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87964903018, 88275733064)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (62476839246, 62787669292)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202350359946, 202661189992)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170024035162, 170334865208)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (30772174554, 31083004600)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (214783561786, 215094391832)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (33569644968, 33880475014)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (296842693930, 297153523976)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (77707511500, 78018341546)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136454390194, 136765220240)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275395420756, 275706250802)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (140184350746, 140495180792)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270732970066, 271043800112)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (186808857646, 187119687692)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (263894709054, 264205539100)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (182457237002, 182768067048)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20825613082, 21136443128)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (305545935218, 305856765264)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163807434242, 164118264288)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (197377079210, 197687909256)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270111309974, 270422140020)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (108790516100, 109101346146)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (269178819836, 269489649882)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (45070356670, 45381186716)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (276017080848, 276327910894)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (277571231078, 277882061124)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (274152100572, 274462930618)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47556997038, 47867827084)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41340396118, 41651226164)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124021188354, 124332018400)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (43205376394, 43516206440)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (82369962190, 82680792236)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (275084590710, 275395420756)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116561267250, 116872097296)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (193957948704, 194268778750)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (177794786312, 178105616358)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (37610435566, 37921265612)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105682215640, 105993045686)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (67760950028, 68071780074)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (220067672568, 220378502614)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (292491073286, 292801903332)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (28596364232, 28907194278)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211986091372, 212296921418)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (101330594996, 101641425042)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (167226564748, 167537394794)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253948147582, 254258977628)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (143292651206, 143603481252)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131481109458, 131791939504)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (77085851408, 77396681454)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (80194151868, 80504981914)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (10568221564, 10879051610)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188363007876, 188673837922)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (196444589072, 196755419118)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (308654235678, 308965065724)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (37921265612, 38232095658)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (311762536138, 312073366184)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (75220871132, 75531701178)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (273841270526, 274152100572)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53462767912, 53773597958)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (32637154830, 32947984876)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179038106496, 179348936542)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (27353044048, 27663874094)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (213851071648, 214161901694)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (264516369146, 264827199192)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (53773597958, 54084428004)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110344666330, 110655496376)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48178657130, 48489487176)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55016918142, 55327748188)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (57814388556, 58125218602)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (59057708740, 59368538786)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124332018400, 124642848446)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (81748302098, 82059132144)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (63098499338, 63409329384)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (120912887894, 121223717940)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184011387232, 184322217278)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262962218916, 263273048962)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (16784822484, 17095652530)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (189606328060, 189917158106)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317979137058, 318289967104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166294074610, 166604904656)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50976127544, 51286957590)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304302615034, 304613445080)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105993045686, 106303875732)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (108479686054, 108790516100)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (176551466128, 176862296174)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (34191305060, 34502135106)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (170645695254, 170956525300)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24244743588, 24555573634)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (260475578548, 260786408594)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (209499451004, 209810281050)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (181524746864, 181835576910)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109723006238, 110033836284)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41962056210, 42272886256)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (270422140020, 270732970066)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (133967749826, 134278579872)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (66206799798, 66517629844)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147333441804, 147644271850)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (312384196230, 312695026276)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160699133782, 161009963828)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145779291574, 146090121620)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (135211070010, 135521900056)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (86410752788, 86721582834)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52219447728, 52530277774)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (70558420442, 70869250488)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (38542925704, 38853755750)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (172510675530, 172821505576)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (78329171592, 78640001638)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128372808998, 128683639044)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46935336946, 47246166992)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179970596634, 180281426680)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (22068933266, 22379763312)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (267624669606, 267935499652)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17406482576, 17717312622)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (313316686368, 313627516414)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (146090121620, 146400951666)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (124953678492, 125264508538)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (81126642006, 81437472052)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110966326422, 111277156468)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (138008540424, 138319370470)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (67139289936, 67450119982)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (174997315898, 175308145944)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187119687692, 187430517738)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (266070519376, 266381349422)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49111147268, 49421977314)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (102884745226, 103195575272)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (109101346146, 109412176192)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (31393834646, 31704664692)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (25798893818, 26109723864)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (166604904656, 166915734702)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (87032412880, 87343242926)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132724429642, 133035259688)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (309275895770, 309586725816)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (10257391518, 10568221564)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (139251860608, 139562690654)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (56881898418, 57192728464)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (265448859284, 265759689330)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (47867827084, 48178657130)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (50665297498, 50976127544)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (114385456928, 114696286974)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (23933913542, 24244743588)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (178105616358, 178416446404)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (181835576910, 182146406956)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (44137866532, 44448696578)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (168780714978, 169091545024)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (80504981914, 80815811960)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (121534547986, 121845378032)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92938183754, 93249013800)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (134278579872, 134589409918)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (105371385594, 105682215640)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (20514783036, 20825613082)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13676522024, 13987352070)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (212607751464, 212918581510)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (52530277774, 52841107820)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (55638578234, 55949408280)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (85478262650, 85789092696)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (259543088410, 259853918456)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (164118264288, 164429094334)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (115317947066, 115628777112)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (148265931942, 148576761988)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (142981821160, 143292651206)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (44759526624, 45070356670)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (181213916818, 181524746864)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (176862296174, 177173126220)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (128061978952, 128372808998)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132413599596, 132724429642)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (112831306698, 113142136744)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (88275733064, 88586563110)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (123710358308, 124021188354)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (26420553910, 26731383956)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (262340558824, 262651388870)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (14298182116, 14609012162)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (173753995714, 174064825760)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249596526938, 249907356984)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165361584472, 165672414518)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (183700557186, 184011387232)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (143603481252, 143914311298)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280368701492, 280679531538)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (137386880332, 137697710378)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (83613282374, 83924112420)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48800317222, 49111147268)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (29528854370, 29839684416)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (216026881970, 216337712016)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (218513522338, 218824352384)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (117493757388, 117804587434)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (182146406956, 182457237002)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18028142668, 18338972714)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (210742771188, 211053601234)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (212296921418, 212607751464)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (211053601234, 211364431280)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12122371794, 12433201840)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (119669567710, 119980397756)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (144225141344, 144535971390)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (290626093010, 290936923056)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (131791939504, 132102769550)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (24555573634, 24866403680)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (117182927342, 117493757388)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (41029566072, 41340396118)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (86099922742, 86410752788)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (177483956266, 177794786312)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (91694863570, 92005693616)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (147022611758, 147333441804)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (136765220240, 137076050286)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (213229411556, 213540241602)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (110033836284, 110344666330)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42272886256, 42583716302)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (21447273174, 21758103220)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179659766588, 179970596634)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (180592256726, 180903086772)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (16473992438, 16784822484)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163496604196, 163807434242)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (132102769550, 132413599596)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (145468461528, 145779291574)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (118115417480, 118426247526)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (44448696578, 44759526624)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (164739924380, 165050754426)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (167848224840, 168159054886)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184322217278, 184633047324)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (19271462852, 19582292898)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (49421977314, 49732807360)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (215094391832, 215405221878)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (116250437204, 116561267250)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (186187197554, 186498027600)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (217581032200, 217891862246)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (162564114058, 162874944104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (39786245888, 40097075934)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (188673837922, 188984667968)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (17717312622, 18028142668)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (163185774150, 163496604196)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (42894546348, 43205376394)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256123957904, 256434787950)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (160388303736, 160699133782)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (315492496690, 315803326736)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (221621822798, 221932652844)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (45381186716, 45692016762)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9946561472, 10257391518)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (48489487176, 48800317222)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11189881656, 11500711702)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (168159054886, 168469884932)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (199552889532, 199863719578)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12433201840, 12744031886)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (165672414518, 165983244564)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254880637720, 255191467766)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15852332346, 16163162392)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (40407905980, 40718736026)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (46624506900, 46935336946)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (184943877370, 185254707416)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (185254707416, 185565537462)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250529017076, 250839847122)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161320793874, 161631623920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (182768067048, 183078897094)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (183078897094, 183389727140)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (180281426680, 180592256726)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187430517738, 187741347784)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (164429094334, 164739924380)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (179348936542, 179659766588)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18338972714, 18649802760)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (12744031886, 13054861932)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244623246202, 244934076248)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (187741347784, 188052177830)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286585302412, 286896132458)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (161009963828, 161320793874)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (185876367508, 186187197554)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (15230672254, 15541502300)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (11500711702, 11811541748)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96046484214, 96357314260)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (249907356984, 250218187030)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (16163162392, 16473992438)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (19582292898, 19893122944)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (10879051610, 11189881656)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (252083167306, 252393997352)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240582455604, 240893285650)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (13987352070, 14298182116)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18649802760, 18960632806)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (18960632806, 19271462852)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (14609012162, 14919842208)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (269489649882, 269800479928)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (293734393470, 294045223516)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202972020038, 203282850084)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (95113994076, 95424824122)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (252704827398, 253015657444)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (239960795512, 240271625558)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244001586110, 244312416156)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (293423563424, 293734393470)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (90762373432, 91073203478)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256434787950, 256745617996)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (251772337260, 252083167306)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (256745617996, 257056448042)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (254258977628, 254569807674)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (255191467766, 255502297812)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (241825775788, 242136605834)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248042376708, 248353206754)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307410915494, 307721745540)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299018504252, 299329334298)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152617552586, 152928382632)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (220689332660, 221000162706)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151685062448, 151995892494)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (207634470728, 207945300774)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155725853046, 156036683092)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (94181503938, 94492333984)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (91384033524, 91694863570)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156658343184, 156969173230)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (98222294536, 98533124582)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (150130912218, 150441742264)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (97911464490, 98222294536)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (152306722540, 152617552586)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248664036800, 248974866846)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154482532862, 154793362908)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (251150677168, 251461507214)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151374232402, 151685062448)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151995892494, 152306722540)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155104192954, 155415023000)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (253637317536, 253948147582)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (154171702816, 154482532862)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244312416156, 244623246202)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287206962504, 287517792550)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (156969173230, 157280003276)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (157901663368, 158212493414)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149820082172, 150130912218)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (291247753102, 291558583148)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (151063402356, 151374232402)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317357476966, 317668307012)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (218824352384, 219135182430)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1864980276, 2175810322)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (155415023000, 155725853046)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149198422080, 149509252126)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5594940828, 5905770874)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (200485379670, 200796209716)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (149509252126, 149820082172)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4973280736, 5284110782)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (89829883294, 90140713340)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (3419130506, 3729960552)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (96668144306, 96978974352)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4040790598, 4351620644)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (95424824122, 95735654168)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4351620644, 4662450690)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (99154784674, 99465614720)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (4662450690, 4973280736)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (250839847122, 251150677168)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (240893285650, 241204115696)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (8392411242, 8703241288)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (247420716616, 247731546662)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284098662044, 284409492090)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7770751150, 8081581196)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282544511814, 282855341860)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286896132458, 287206962504)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9014071334, 9324901380)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280057871446, 280368701492)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9635731426, 9946561472)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (307721745540, 308032575586)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (621660092, 932490138)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (286274472366, 286585302412)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (314560006552, 314870836598)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (932490138, 1243320184)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (2486640368, 2797470414)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92005693616, 92316523662)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (2797470414, 3108300460)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (90451543386, 90762373432)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282855341860, 283166171906)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284409492090, 284720322136)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (3108300460, 3419130506)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (9324901380, 9635731426)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (98843954628, 99154784674)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5905770874, 6216600920)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (288139452642, 288450282688)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283477001952, 283787831998)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281922851722, 282233681768)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (98533124582, 98843954628)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1243320184, 1554150230)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280990361584, 281301191630)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (97289804398, 97600634444)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (318289967104, 318600797150)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278814551262, 279125381308)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (310830046, 621660092)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (3729960552, 4040790598)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (252393997352, 252704827398)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (5284110782, 5594940828)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285031152182, 285341982228)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (258299768226, 258610598272)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7149091058, 7459921104)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (244934076248, 245244906294)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6216600920, 6527430966)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (295288543700, 295599373746)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (284720322136, 285031152182)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (1554150230, 1864980276)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281301191630, 281612021676)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283787831998, 284098662044)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (317046646920, 317357476966)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (8081581196, 8392411242)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (280679531538, 280990361584)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92316523662, 92627353708)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (7459921104, 7770751150)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (94803164030, 95113994076)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285341982228, 285652812274)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (8703241288, 9014071334)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (282233681768, 282544511814)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (94492333984, 94803164030)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (2175810322, 2486640368)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (281612021676, 281922851722)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285963642320, 286274472366)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (283166171906, 283477001952)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (0, 310830046)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279125381308, 279436211354)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257056448042, 257367278088)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (278503721216, 278814551262)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (257988938180, 258299768226)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (299640164344, 299950994390)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6838261012, 7149091058)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (6527430966, 6838261012)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (202039529900, 202350359946)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (285652812274, 285963642320)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (287517792550, 287828622596)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (90140713340, 90451543386)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279436211354, 279747041400)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (91073203478, 91384033524)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (279747041400, 280057871446)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (97600634444, 97911464490)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (248974866846, 249285696892)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (89519053248, 89829883294)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (95735654168, 96046484214)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (304924275126, 305235105172)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228460083810, 228770913856)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (92627353708, 92938183754)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236230834960, 236541665006)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231879214316, 232190044362)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236541665006, 236852495052)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235609174868, 235920004914)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234365854684, 234676684730)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (232190044362, 232500874408)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (228770913856, 229081743902)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230635894132, 230946724178)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (236852495052, 237163325098)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229081743902, 229392573948)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234676684730, 234987514776)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (232811704454, 233122534500)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230014234040, 230325064086)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231257554224, 231568384270)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (231568384270, 231879214316)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237163325098, 237474155144)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237784985190, 238095815236)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235298344822, 235609174868)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (235920004914, 236230834960)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234055024638, 234365854684)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (234987514776, 235298344822)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (232500874408, 232811704454)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (233433364546, 233744194592)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229392573948, 229703403994)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230946724178, 231257554224)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (233122534500, 233433364546)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (229703403994, 230014234040)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (237474155144, 237784985190)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (233744194592, 234055024638)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (230325064086, 230635894132)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238406645282, 238717475328)
kwargs:    {}
Exception: 'CancelledError()'

distributed.worker - WARNING - Compute Failed
Function:  filter
args:      (238095815236, 238406645282)
kwargs:    {}
Exception: 'CancelledError()'

distributed.utils_perf - INFO - full garbage collection released 53.31 MiB from 25117 reference cycles (threshold: 9.54 MiB)
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb598>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49820 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49822 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37596 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb840>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49824 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb488>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37594 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49826 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37600 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49828 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37602 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361bd08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49830 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37604 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49832 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37606 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031be6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49834 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37608 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49836 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37610 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49838 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031be510>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37612 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49840 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb488>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37614 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb950>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49842 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37616 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361a2f0>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49844 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37618 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49846 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37620 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49848 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37622 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37624 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361c158>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.30:37626 remote=tcp://192.168.64.232:44019> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.31:49850 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56300 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361ad08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56298 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56294 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb488>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56296 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56302 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56304 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb620>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56306 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56308 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56310 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56312 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031be378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56314 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x20000361ad08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56316 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb6a8>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56318 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56320 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x2000031bb378>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56322 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback <function Worker.__init__.<locals>.<lambda> at 0x200003643d08>
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 921, in _run
    val = self.callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1008, in <lambda>
    lambda: self.batched_stream.send({"op": "keep-alive"}), 60000
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56324 remote=tcp://192.168.64.232:35367> already closed.
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:43733
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:42797
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:40901
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:43985
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xmbqorly' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xmbqorly'
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ohkcn6ru' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ohkcn6ru'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:44757
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:44267
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35863
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33537
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35761
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:43631
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:45083
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:43569
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:39695
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:40869
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:36665
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:34321
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:37615
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:44665
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:39943
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:40883
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:46231
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:33455
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:37055
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:46709
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:34535
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:36733
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:46345
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:42057
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:36313
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:41431
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:44039
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:34171
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:37991
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:40287
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:41811
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:40639
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:32961
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:40563
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:35717
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:45677
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:45889
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:43705
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-g1a8sdd5' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-g1a8sdd5'
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fplrnukj' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fplrnukj'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_nftncg4' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_nftncg4'
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-10mr9g20' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-10mr9g20'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:37715
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l0wuu3a5' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l0wuu3a5'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v6m9y63a' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v6m9y63a'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:37405
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xqj6dbpr' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xqj6dbpr'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e794zxpt' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e794zxpt'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l51s7wtq' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l51s7wtq'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sqc3_2p4' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sqc3_2p4'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6skt9hzl' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6skt9hzl'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q58amsci' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q58amsci'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-90v4num1' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-90v4num1'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-omtm9baq' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-omtm9baq'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8mdfep4n' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8mdfep4n'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ol63_hx1' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ol63_hx1'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s3skyykv' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s3skyykv'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45983'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-00bgmwc_' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-00bgmwc_'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-govzgwrb' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-govzgwrb'
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:41917'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36445'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36259'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:44213'
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-92u1jc8i' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-92u1jc8i'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:44259'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:39657'
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:44561
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:55104 remote=tcp://192.168.64.31:44561>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:41209
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:51398 remote=tcp://192.168.64.31:41209>: Stream is closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:44561 -> tcp://192.168.64.28:41431
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:44561 remote=tcp://192.168.64.28:55104>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:40883
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:44782 remote=tcp://192.168.64.31:40883>: Stream is closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:41209 -> tcp://192.168.64.28:41431
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:41209 remote=tcp://192.168.64.28:51398>: Stream is closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:40883 -> tcp://192.168.64.28:41431
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:40883 remote=tcp://192.168.64.28:44782>: Stream is closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:43331
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70320>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56316 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56316 remote=tcp://192.168.64.232:35367> already closed.
distributed.core - INFO - Lost connection to 'tcp://192.168.64.28:55104'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:44561 remote=tcp://192.168.64.28:55104>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.28:51398'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:41209 remote=tcp://192.168.64.28:51398>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70320>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56316 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56316 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70320>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56316 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56316 remote=tcp://192.168.64.232:35367> already closed.
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hhrlyf97' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hhrlyf97'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:39109
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ga401cg1' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ga401cg1'
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lwacqlio' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lwacqlio'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40913'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:35753
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:38503'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gjfrxhbk' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gjfrxhbk'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:42825'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40353'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44027'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:35815
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:46277'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-do2hg4iv' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-do2hg4iv'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:34003'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:39425
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yhshf999' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yhshf999'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:41963
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-16rwd8k1' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-16rwd8k1'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44099
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3ho42sqi' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3ho42sqi'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:37473
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:37205
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6xivxwpt' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6xivxwpt'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:45889
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:36178 remote=tcp://192.168.64.31:45889>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:44965
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:48396 remote=tcp://192.168.64.31:44965>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:32961
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:46178 remote=tcp://192.168.64.31:32961>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b5b7f0>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56322 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56322 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56298 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56298 remote=tcp://192.168.64.232:35367> already closed.
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f400>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56298 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56298 remote=tcp://192.168.64.232:35367> already closed.
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:45889 -> tcp://192.168.64.28:40563
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:45889 remote=tcp://192.168.64.28:36178>: Stream is closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:32961 -> tcp://192.168.64.28:44665
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:32961 remote=tcp://192.168.64.28:46178>: Stream is closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:44965 -> tcp://192.168.64.28:40563
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:44965 remote=tcp://192.168.64.28:48396>: Stream is closed
distributed.core - INFO - Lost connection to 'tcp://192.168.64.28:48396'
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 525, in handle_comm
    result = await result
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:44965 remote=tcp://192.168.64.28:48396>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:32961
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:46148 remote=tcp://192.168.64.31:32961>: Stream is closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:32961 -> tcp://192.168.64.28:37615
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:32961 remote=tcp://192.168.64.28:46148>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b70198>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56304 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56304 remote=tcp://192.168.64.232:35367> already closed.
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:41423
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fo8d3i2m' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fo8d3i2m'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rf1t6m3a' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rf1t6m3a'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - Worker stream died during communication: tcp://192.168.64.31:32961
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3072, in gather_dep
    self.rpc, to_gather_keys, worker, who=self.address
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4377, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 390, in retry_operation
    operation=operation,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/utils_comm.py", line 370, in retry
    return await coro()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 4364, in _get_data
    max_connections=max_connections,
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/core.py", line 674, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://192.168.64.28:45944 remote=tcp://192.168.64.31:32961>: Stream is closed
distributed.nanny - INFO - Worker closed
distributed.worker - ERROR - failed during get data with tcp://192.168.64.31:32961 -> tcp://192.168.64.28:43705
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 1814, in get_data
    response = await comm.read(deserializers=serializers)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/comm/tcp.py", line 129, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://192.168.64.31:32961 remote=tcp://192.168.64.28:45944>: Stream is closed
tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x200001b6f390>>, <Task finished coro=<Worker.gather_dep() done, defined at /usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py:3015> exception=CommClosedError('Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56320 remote=tcp://192.168.64.232:35367> already closed.')>)
Traceback (most recent call last):
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 740, in _run_callback
    ret = callback()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/tornado/ioloop.py", line 764, in _discard_future_result
    future.result()
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/worker.py", line 3138, in gather_dep
    {"op": "missing-data", "errant_worker": worker, "key": d}
  File "/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/batched.py", line 137, in send
    raise CommClosedError(f"Comm {self.comm!r} already closed.")
distributed.comm.core.CommClosedError: Comm <TCP (closed) Worker->Scheduler local=tcp://192.168.64.28:56320 remote=tcp://192.168.64.232:35367> already closed.
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rqf_m_kl' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rqf_m_kl'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:44057'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:33067'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:33611
distributed.worker - INFO - Stopping worker at tcp://192.168.64.28:33905
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:38425
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4m34i4sn' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4m34i4sn'
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hcyo_r7q' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hcyo_r7q'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:38285'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:44561
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:44163
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rdi4yphx' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rdi4yphx'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:44965
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:36761
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0_4r3cke' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0_4r3cke'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.29:45269
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:34485'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:46369'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:38047'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dz8nd_ur' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dz8nd_ur'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:39731'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:40463'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:41187'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:45095'
distributed.nanny - INFO - Worker closed
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:34461'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-x0axfk8r' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-x0axfk8r'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:44887
distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m7v0w3cx' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m7v0w3cx'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-up1vz8jw' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-up1vz8jw'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42455'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:36621'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:37441'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:46233
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:36315'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:42459'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:40087
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35337
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:44443
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:46799
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.29:39033'
distributed.dask_worker - INFO - End worker
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:37185
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:34499
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:39171
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wtn9p9jd' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wtn9p9jd'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:43047'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:43901
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
logout
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:37003
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:39191'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-okubf255' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-okubf255'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:37365
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:32991
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:33113
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-weepy_ps' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-weepy_ps'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:45633
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:40385
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:45221'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:40455
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:43479
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:34295'
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wbon0tfi' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wbon0tfi'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:38043'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:36099
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35247'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:38903'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:45497'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:46399'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:37947'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:42325'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:36249'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.32:35065
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:35677'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:45817'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:38025
distributed.worker - INFO - Stopping worker at tcp://192.168.64.30:37241
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2qs1s8pi' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2qs1s8pi'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:42927'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.31:41209
distributed.nanny - INFO - Worker closed
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-j2blzdv4' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-j2blzdv4'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40233'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:40523'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:40415'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:42873'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:34921'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:37409
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:42587'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:33013'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:38809
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:45865'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:40977'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:42213
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:39981'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:39165'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:45237'
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:38727
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:46095
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:41381'
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:44723
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39287'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.30:32781'
distributed.dask_worker - INFO - End worker
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:42619
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:46073'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.32:34147'
distributed.dask_worker - INFO - End worker
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:33627
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:40287
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:40329
distributed.nanny - INFO - Worker closed
distributed.worker - INFO - Stopping worker at tcp://192.168.64.34:40481
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:34391'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:38883'
logout
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:33633'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:39163'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39859'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:35317'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:36187'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:45645'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:34295'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:35149'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39579'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:36169'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:44897'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:38831'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:37791'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:43895'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:41315'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:44399'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:40559'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:33441'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.34:40479'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:40479'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:41407'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:45345'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:45513'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:37683'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:33753'
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:39039'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:45829'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:34261'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:33229'
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.31:34821'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Worker closed
logout
logout
distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.64.28:45595'
distributed.dask_worker - INFO - End worker

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920308: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:06:57 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:07:01 2022
                            <40*lassen28>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:07:01 2022
Terminated at Sat Sep 17 20:11:44 2022
Results reported at Sat Sep 17 20:11:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:35367 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.21 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.09 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   283 sec.
    Turnaround time :                            287 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920307: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:06:57 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:07:01 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:07:01 2022
Terminated at Sat Sep 17 20:11:44 2022
Results reported at Sat Sep 17 20:11:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:38857 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.24 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.09 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   283 sec.
    Turnaround time :                            287 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920306: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:06:56 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:06:59 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:06:59 2022
Terminated at Sat Sep 17 20:11:44 2022
Results reported at Sat Sep 17 20:11:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44019 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.22 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.09 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   287 sec.
    Turnaround time :                            288 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920304: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:06:55 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:06:59 2022
                            <40*lassen31>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:06:59 2022
Terminated at Sat Sep 17 20:11:44 2022
Results reported at Sat Sep 17 20:11:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:35367 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.30 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.09 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   287 sec.
    Turnaround time :                            289 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920303: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:06:55 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:06:59 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:06:59 2022
Terminated at Sat Sep 17 20:11:44 2022
Results reported at Sat Sep 17 20:11:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44019 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.27 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.09 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   287 sec.
    Turnaround time :                            289 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920302: <dask-worker> in cluster <lassen> Done

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:06:55 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:06:59 2022
                            <40*lassen34>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:06:59 2022
Terminated at Sat Sep 17 20:11:44 2022
Results reported at Sat Sep 17 20:11:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:38857 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   0.21 sec.
    Max Memory :                                 59 MB
    Average Memory :                             58.09 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   287 sec.
    Turnaround time :                            289 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37807'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44785'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45703'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:40123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45207'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38247'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:35347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:46221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37111'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45107'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44985'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:46107
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35189
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35189
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36485
distributed.worker - INFO -          dashboard at:        192.168.64.28:37173
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36485
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:46107
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          dashboard at:        192.168.64.28:32769
distributed.worker - INFO -          dashboard at:        192.168.64.28:46447
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:46053
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:46053
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:44447
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wu6gn4ed
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35663
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:46317
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kekh6ajn
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35663
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f041o4pa
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:46317
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:39033
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:44471
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:39637
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f4hzehli
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:34171
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:34395
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:44471
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:34171
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:34395
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:41353
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:34453
distributed.worker - INFO -          dashboard at:        192.168.64.28:34341
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-k820ejwm
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-p62g2c_l
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35279
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35279
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-v0skmka8
distributed.worker - INFO -          dashboard at:        192.168.64.28:39119
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-n1a09j_b
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gp4_s2yv
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45467
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45467
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:39697
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40963
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_zhue34a
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40963
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-fehe8mjo
distributed.worker - INFO -          dashboard at:        192.168.64.28:46765
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-l9s0hga7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43297
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43297
distributed.worker - INFO -          dashboard at:        192.168.64.28:40501
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43741
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43741
distributed.worker - INFO -          dashboard at:        192.168.64.28:39159
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-825thw90
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:38045
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:38045
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rd7wehna
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:40717
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36767
distributed.worker - INFO -          dashboard at:        192.168.64.28:34309
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uhq1t6jh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0h95d9st
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:33423'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:35209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:46215'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:42731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:38169'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:42345'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37605'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36767'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:38155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37941'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45247'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:34303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39707'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37199'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33097'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:38299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44119'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42835'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f041o4pa', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kekh6ajn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uhq1t6jh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gp4_s2yv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-n1a09j_b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_zhue34a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-p62g2c_l', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rd7wehna', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-v0skmka8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-k820ejwm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-l9s0hga7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-fehe8mjo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-825thw90', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f4hzehli', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0h95d9st', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wu6gn4ed', purging
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39995'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:38591'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:36021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:46679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:44261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36095'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:41641'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:33153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44637'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:34747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43721'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:42709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:34317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34545'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40497
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40497
distributed.worker - INFO -          dashboard at:        192.168.64.29:38477
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42929
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42545
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42929
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42545
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42959
distributed.worker - INFO -          dashboard at:        192.168.64.29:43627
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:41221
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42959
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-1tsb6ryp
distributed.worker - INFO -          dashboard at:        192.168.64.29:39463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34807
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43111
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34807
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41335
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:46771
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43111
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41335
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -          dashboard at:        192.168.64.29:37507
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-x7j80xnm
distributed.worker - INFO -          dashboard at:        192.168.64.29:38791
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8e34sj5n
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rmd1sedb
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-utkpbkif
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fbcpp8jm
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-v8_ztdc9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39187
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39187
distributed.worker - INFO -          dashboard at:        192.168.64.29:42509
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39447
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39447
distributed.worker - INFO -          dashboard at:        192.168.64.29:37579
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2nl7dch8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-tsd8au68
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46523
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46523
distributed.worker - INFO -          dashboard at:        192.168.64.29:39849
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9jbltu8f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43197
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43197
distributed.worker - INFO -          dashboard at:        192.168.64.29:37375
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kqd7vhu3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36479
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36479
distributed.worker - INFO -          dashboard at:        192.168.64.29:39091
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-53dth9dn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40449
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40449
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO -          dashboard at:        192.168.64.29:46399
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-e6fxmyvv
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43075
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43075
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42121
distributed.worker - INFO -          dashboard at:        192.168.64.29:36877
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42121
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:41575
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-km72j77h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xfc0ot18
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40653
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40653
distributed.worker - INFO -          dashboard at:        192.168.64.29:46373
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8vqa38f3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37603
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37603
distributed.worker - INFO -          dashboard at:        192.168.64.31:35221
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40101
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37073
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40101
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37073
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42165
distributed.worker - INFO -          dashboard at:        192.168.64.31:44391
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:39821
distributed.worker - INFO -          dashboard at:        192.168.64.31:38023
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40473
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-lpo8r4w6
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36703
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:39821
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40473
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36703
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:46113
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40815
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42543
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36375
distributed.worker - INFO -          dashboard at:        192.168.64.31:40491
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:35523
distributed.worker - INFO -          dashboard at:        192.168.64.31:43123
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40815
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:46113
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42543
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42165
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36375
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:34069
distributed.worker - INFO -          dashboard at:        192.168.64.31:35457
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:35523
distributed.worker - INFO -          dashboard at:        192.168.64.31:42307
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:35943
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:39881
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          dashboard at:        192.168.64.31:38587
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-07i1ars7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jzfnulus
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_5p1qt30
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-g2n_zx8h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-r4pl3mt6
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-qvrpjekm
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wx5y4q21
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m1doyern
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-p3wgxdn5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:34775
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:34775
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38979
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tda6ijrd
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36245
distributed.worker - INFO -          dashboard at:        192.168.64.31:33519
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38979
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:42133
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42053
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:45853
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42053
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36245
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:33361
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -          dashboard at:        192.168.64.31:44007
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0asm4v5f
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ir35cabp
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uwyyca03
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-9h0ebodv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ux61wokc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fbcpp8jm', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-e6fxmyvv', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-x7j80xnm', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xfc0ot18', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kqd7vhu3', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-1tsb6ryp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-utkpbkif', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9jbltu8f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-km72j77h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2nl7dch8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rmd1sedb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-tsd8au68', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8e34sj5n', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-53dth9dn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-v8_ztdc9', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8vqa38f3', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:39227
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41635'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35755'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35835'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41301'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42699'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43859'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45153'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35147'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42599'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38515'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-w74wrwtv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fdycnhvc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qql_wp8z', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0_371q5g', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngb1_s1n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bze2yabg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-pyornnnc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-cpore6p1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5_ifjfun', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hpkiirrl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ghn4r1ne', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_s2lw9k2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qmuojngj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-s4m0j6s7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-16q2qhbj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mu4izugo', purging
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44687
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44687
distributed.worker - INFO -          dashboard at:        192.168.64.30:42373
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:32887
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40327
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41423
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41443
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36819
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40327
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:32887
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38809
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41443
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36819
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41423
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:36461
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35079
distributed.worker - INFO -          dashboard at:        192.168.64.30:34895
distributed.worker - INFO -          dashboard at:        192.168.64.30:40247
distributed.worker - INFO -          dashboard at:        192.168.64.30:43907
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mu4izugo
distributed.worker - INFO -          dashboard at:        192.168.64.30:44419
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40389
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45963
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35079
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38809
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40389
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40957
distributed.worker - INFO -          dashboard at:        192.168.64.30:45959
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45963
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:43275
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40957
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42623
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -          dashboard at:        192.168.64.30:42005
distributed.worker - INFO -          dashboard at:        192.168.64.30:44143
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -          dashboard at:        192.168.64.30:40315
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42623
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35149
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -          dashboard at:        192.168.64.30:42469
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35149
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qmuojngj
distributed.worker - INFO -          dashboard at:        192.168.64.30:38607
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-16q2qhbj
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-pyornnnc
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bze2yabg
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-hpkiirrl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngb1_s1n
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0_371q5g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-cpore6p1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38055
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-fdycnhvc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38055
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:38789
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36445
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_s2lw9k2
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-w74wrwtv
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qql_wp8z
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36445
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:37747
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:32933
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:32933
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:37167
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-s4m0j6s7
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5_ifjfun
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ghn4r1ne
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:34901
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:34901
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39103
distributed.worker - INFO -          dashboard at:        192.168.64.34:41935
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39103
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -          dashboard at:        192.168.64.34:41803
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-546kmq3t
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gwvk0gz_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:33891
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:33891
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38363
distributed.worker - INFO -          dashboard at:        192.168.64.34:42687
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38363
distributed.worker - INFO -          dashboard at:        192.168.64.34:39915
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-pmi4rvg8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fcicnm4w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:35557
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:35557
distributed.worker - INFO -          dashboard at:        192.168.64.34:43557
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6n33xias
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:41865
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44733
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:41865
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44733
distributed.worker - INFO -          dashboard at:        192.168.64.34:39271
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40967
distributed.worker - INFO -          dashboard at:        192.168.64.34:39247
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40967
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:37519
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oh017acr
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0xqc4hae
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-nvovvyub
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:42295
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:42295
distributed.worker - INFO -          dashboard at:        192.168.64.34:37855
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-elvn8tyx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44503
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44503
distributed.worker - INFO -          dashboard at:        192.168.64.34:36227
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:37723
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:37723
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.34:37041
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gz8wrk1e
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-jz608n0p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:46261
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:46261
distributed.worker - INFO -          dashboard at:        192.168.64.34:36763
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-end6u62r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44619
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44619
distributed.worker - INFO -          dashboard at:        192.168.64.34:37357
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bnvnx8zp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:35591
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:35591
distributed.worker - INFO -          dashboard at:        192.168.64.34:39457
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6inls811
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:45875
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:45875
distributed.worker - INFO -          dashboard at:        192.168.64.34:43407
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dcqshi52
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38475
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38475
distributed.worker - INFO -          dashboard at:        192.168.64.34:45343
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ko01_bio
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:44365
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35757
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34095
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34095
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35757
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41589
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41263
distributed.worker - INFO -          dashboard at:        192.168.64.32:38597
distributed.worker - INFO -          dashboard at:        192.168.64.32:42379
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41589
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -          dashboard at:        192.168.64.32:40829
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34555
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41263
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38067
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34925
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:45131
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44787
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34555
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34925
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38067
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.32:35067
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:43959
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-b5dmhhvs
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v1earu3y
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -          dashboard at:        192.168.64.32:33091
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-eig_fyqt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44787
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34059
distributed.worker - INFO -          dashboard at:        192.168.64.32:36995
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34059
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35241
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v8in0epw
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-c7erzuj1
distributed.worker - INFO -          dashboard at:        192.168.64.32:44463
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34103
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35241
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34103
distributed.worker - INFO -          dashboard at:        192.168.64.32:37367
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37405
distributed.worker - INFO -          dashboard at:        192.168.64.32:41377
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8nq7w784
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-li32pxwe
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37405
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:42519
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-igicgvwq
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-m0gdxrdl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i4tnzajg
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-130gun7v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-uxs_r4au
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43475
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43475
distributed.worker - INFO -          dashboard at:        192.168.64.32:45931
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-vpfhzbqm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34499
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34499
distributed.worker - INFO -          dashboard at:        192.168.64.32:42335
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-lfjtzev1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44847
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44847
distributed.worker - INFO -          dashboard at:        192.168.64.32:35945
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6uledcc8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37895
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37895
distributed.worker - INFO -          dashboard at:        192.168.64.32:46005
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-srfl51wv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37355
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920320: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:12:15 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:12:18 2022
                            <40*lassen28>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:12:18 2022
Terminated at Sat Sep 17 20:13:17 2022
Results reported at Sat Sep 17 20:13:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:39227 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.13 sec.
    Max Memory :                                 138 MB
    Average Memory :                             138.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   934 MB
    Max Processes :                              1
    Max Threads :                                2
    Run time :                                   59 sec.
    Turnaround time :                            62 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920319: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:12:15 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:12:18 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:12:18 2022
Terminated at Sat Sep 17 20:13:17 2022
Results reported at Sat Sep 17 20:13:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44365 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.16 sec.
    Max Memory :                                 137 MB
    Average Memory :                             137.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   934 MB
    Max Processes :                              1
    Max Threads :                                2
    Run time :                                   59 sec.
    Turnaround time :                            62 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920318: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:12:15 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:12:18 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:12:18 2022
Terminated at Sat Sep 17 20:13:17 2022
Results reported at Sat Sep 17 20:13:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37355 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.21 sec.
    Max Memory :                                 137 MB
    Average Memory :                             137.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   934 MB
    Max Processes :                              1
    Max Threads :                                2
    Run time :                                   59 sec.
    Turnaround time :                            62 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920317: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:12:15 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:12:18 2022
                            <40*lassen31>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:12:18 2022
Terminated at Sat Sep 17 20:13:17 2022
Results reported at Sat Sep 17 20:13:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:39227 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.20 sec.
    Max Memory :                                 137 MB
    Average Memory :                             137.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   934 MB
    Max Processes :                              1
    Max Threads :                                2
    Run time :                                   59 sec.
    Turnaround time :                            62 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920316: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:12:15 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:12:18 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:12:18 2022
Terminated at Sat Sep 17 20:13:17 2022
Results reported at Sat Sep 17 20:13:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37355 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.19 sec.
    Max Memory :                                 137 MB
    Average Memory :                             137.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   934 MB
    Max Processes :                              1
    Max Threads :                                2
    Run time :                                   59 sec.
    Turnaround time :                            62 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920315: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:12:15 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:12:18 2022
                            <40*lassen34>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:12:18 2022
Terminated at Sat Sep 17 20:13:24 2022
Results reported at Sat Sep 17 20:13:24 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:44365 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.17 sec.
    Max Memory :                                 138 MB
    Average Memory :                             138.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   934 MB
    Max Processes :                              1
    Max Threads :                                2
    Run time :                                   59 sec.
    Turnaround time :                            69 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:45999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:37583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:42017'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:45337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:40845'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:37871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38315'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:34709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:41335'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:36273'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33545'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:43469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39883'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40287'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42335'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:35941'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46695'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41319'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:32849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:35993'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36487'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-p3wgxdn5', purging
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jzfnulus', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ux61wokc', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-r4pl3mt6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-qvrpjekm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ir35cabp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tda6ijrd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wx5y4q21', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-m1doyern', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-07i1ars7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-9h0ebodv', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36367'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_5p1qt30', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:43891'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:42477'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0asm4v5f', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:33569'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:38245'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:35125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34351'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-g2n_zx8h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uwyyca03', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:43517'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:42453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44955'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:40653'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34203'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-lpo8r4w6', purging
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36219'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:46835'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:44931'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:39905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:45563'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35421'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:38909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:41375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43117'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:36855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:38043'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:41773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:39099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:46077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:32985'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39913'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33279'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42143'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45061'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37039'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33837'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:33719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33381'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35465'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41685'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41341'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:32821'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41223'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37387'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42997'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37331'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6nzfjlin', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5303lawd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kxa1ql7y', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-i620hdjn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ap9sf8ym', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mezi9xw3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kdo722xo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-a5ympw2s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-grg85nny', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-d9xcwceb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-c4r76xw_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hzcpzmf1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-lm8m_3ge', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-l3j9t7gu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-h442nx6a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-zxsfio76', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fcicnm4w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6n33xias', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-546kmq3t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-nvovvyub', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-dcqshi52', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gz8wrk1e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bnvnx8zp', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gwvk0gz_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-pmi4rvg8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0xqc4hae', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6inls811', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-elvn8tyx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-oh017acr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-jz608n0p', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ko01_bio', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-end6u62r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fnfn7w4i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i4tnzajg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i4tnzajg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6bqgmct8', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i4tnzajg' (failed in <built-in function rmdir>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i4tnzajg'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_n6avfti', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8nq7w784', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8nq7w784', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xhoa8l67', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4mai_fck', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4llau2jo', purging
distributed.diskutils - ERROR - Failed to remove '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8nq7w784' (failed in <built-in function lstat>): [Errno 2] No such file or directory: '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8nq7w784'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-732a7y_e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-lfjtzev1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-n358987r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_2nx_bj4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-k0md9035', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v1earu3y', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-705a1rus', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0_um326n', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-c7erzuj1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5ynagbkw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-130gun7v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-b5dmhhvs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-srfl51wv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-s1f7vy25', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-eig_fyqt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-y5m3_j8h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-m0gdxrdl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9cc184gz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-v8in0epw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-li32pxwe', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-uxs_r4au', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6uledcc8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-igicgvwq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-vpfhzbqm', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:34993
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:34993
distributed.worker - INFO -          dashboard at:        192.168.64.33:40799
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:42075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:42075
distributed.worker - INFO -          dashboard at:        192.168.64.33:44973
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:38205
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:38205
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-a5ympw2s
distributed.worker - INFO -          dashboard at:        192.168.64.33:43689
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44511
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5303lawd
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:42607
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:34207
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44511
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:42607
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40489
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:34207
distributed.worker - INFO -          dashboard at:        192.168.64.33:42567
distributed.worker - INFO -          dashboard at:        192.168.64.33:39595
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40489
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-i620hdjn
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:45231
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -          dashboard at:        192.168.64.33:40443
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:33189
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:45231
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:43479
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:42947
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:43479
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:45695
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -          dashboard at:        192.168.64.33:43183
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:41753
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:45695
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:41753
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-grg85nny
distributed.worker - INFO -          dashboard at:        192.168.64.33:45801
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:41533
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kxa1ql7y
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-zxsfio76
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -          dashboard at:        192.168.64.33:40297
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:38871
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ap9sf8ym
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:41533
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:39391
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-h442nx6a
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:38871
distributed.worker - INFO -          dashboard at:        192.168.64.33:36653
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:39391
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.33:44471
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-d9xcwceb
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:43475
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-l3j9t7gu
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-c4r76xw_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-lm8m_3ge
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kdo722xo
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-6nzfjlin
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:46783
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:46783
distributed.worker - INFO -          dashboard at:        192.168.64.33:38251
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:35451
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:35451
distributed.worker - INFO -          dashboard at:        192.168.64.33:39049
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hzcpzmf1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mezi9xw3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33911
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33911
distributed.worker - INFO -          dashboard at:        192.168.64.30:34661
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34611
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36029
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36029
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34611
distributed.worker - INFO -          dashboard at:        192.168.64.30:36549
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -          dashboard at:        192.168.64.30:39791
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s0y450dk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38617
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45439
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33647
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44725
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-e1hp73ta
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:46683
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35789
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36853
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45439
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33647
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44725
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36853
distributed.worker - INFO -          dashboard at:        192.168.64.30:44659
distributed.worker - INFO -          dashboard at:        192.168.64.30:43873
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38617
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uqol73qv
distributed.worker - INFO -          dashboard at:        192.168.64.30:37579
distributed.worker - INFO -          dashboard at:        192.168.64.30:44907
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:46683
distributed.worker - INFO -          dashboard at:        192.168.64.30:40295
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35789
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37377
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:32983
distributed.worker - INFO -          dashboard at:        192.168.64.30:45141
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37377
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -          dashboard at:        192.168.64.30:33117
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33887
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-qypl8ovr
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-o8fe_9ar
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-xoyzqt7e
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-r472eeia
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33887
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uicypzki
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:45423
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mx2pz2x1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-apqoyvzr
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37329
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37329
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:34575
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q0pjme_x
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5yufsd8z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f4x8dw3e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43083
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43083
distributed.worker - INFO -          dashboard at:        192.168.64.30:36635
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2xxyz6lj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42449
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42449
distributed.worker - INFO -          dashboard at:        192.168.64.30:46703
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35023
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35023
distributed.worker - INFO -          dashboard at:        192.168.64.30:39161
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-dnpotpx1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tuar5yps
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36041
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45263
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:34599
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:34599
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:46279
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45263
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:33465
distributed.worker - INFO -          dashboard at:        192.168.64.31:38375
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:46279
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:33465
distributed.worker - INFO -          dashboard at:        192.168.64.31:38269
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          dashboard at:        192.168.64.31:41159
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          dashboard at:        192.168.64.31:46247
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4llau2jo
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xhoa8l67
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:34735
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:32917
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:39687
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6bqgmct8
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-705a1rus
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:34735
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:32917
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:39687
distributed.worker - INFO -          dashboard at:        192.168.64.31:33001
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:35933
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:44057
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:35589
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:35589
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:46493
distributed.worker - INFO -          dashboard at:        192.168.64.31:35343
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:46493
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          dashboard at:        192.168.64.31:36443
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-y5m3_j8h
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4mai_fck
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0_um326n
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38241
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38241
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5ynagbkw
distributed.worker - INFO -          dashboard at:        192.168.64.31:43595
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-s1f7vy25
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45433
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38137
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:34791
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45433
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-732a7y_e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:37743
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38131
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38131
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:36441
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41813
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41813
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fnfn7w4i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:37615
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-n358987r
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:39709
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:39709
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:42299
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:44543
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:44543
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_n6avfti
distributed.worker - INFO -          dashboard at:        192.168.64.31:43741
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-k0md9035
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-9cc184gz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_2nx_bj4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40803
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34185
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41571
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34185
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33253
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40803
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41571
distributed.worker - INFO -          dashboard at:        192.168.64.32:34383
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33253
distributed.worker - INFO -          dashboard at:        192.168.64.32:33465
distributed.worker - INFO -          dashboard at:        192.168.64.32:45763
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          dashboard at:        192.168.64.32:36949
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43861
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41663
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36787
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39325
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41663
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9qn0od96
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40209
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z4s3q2li
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39325
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z3qzzogg
distributed.worker - INFO -          dashboard at:        192.168.64.32:45699
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i54d2nf8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43861
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          dashboard at:        192.168.64.32:33809
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36787
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35113
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40091
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40209
distributed.worker - INFO -          dashboard at:        192.168.64.32:33595
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39027
distributed.worker - INFO -          dashboard at:        192.168.64.32:46081
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          dashboard at:        192.168.64.32:39725
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35113
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33515
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39027
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33811
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33515
distributed.worker - INFO -          dashboard at:        192.168.64.32:37257
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33811
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:34661
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40091
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:38957
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          dashboard at:        192.168.64.32:40371
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l13_k8wm
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-txrwyvlx
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:45743
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          dashboard at:        192.168.64.32:37005
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:45743
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37949
distributed.worker - INFO -          dashboard at:        192.168.64.32:41369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-r5oc5aap
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e8ti53bv
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37949
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mdnoppen
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:38259
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8sshskar
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u59c9sic
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-o73lt7v9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-kn_6jcuj
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_ms99ixz
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2uhwb38b
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i4qmmfl3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37209
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37209
distributed.worker - INFO -          dashboard at:        192.168.64.29:41755
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45865
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45865
distributed.worker - INFO -          dashboard at:        192.168.64.29:35095
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35147
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38379
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33291
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-z5gpzkzs
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44625
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35147
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38379
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40385
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33291
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44625
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45563
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:45173
distributed.worker - INFO -          dashboard at:        192.168.64.29:45699
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40385
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:43427
distributed.worker - INFO -          dashboard at:        192.168.64.29:42563
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          dashboard at:        192.168.64.29:38979
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41669
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fxw8zbhx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46377
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41669
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45563
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41417
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:42621
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:32879
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41417
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:32879
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          dashboard at:        192.168.64.29:43827
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:41383
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:45075
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46377
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-q6go33iy
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7rn4nawt
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:37175
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-we4polw_
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-100wdzqk
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39219
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7lv_1us8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39219
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qxi5uzsq
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hgf4gqj5
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8rarepnh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35947
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:42787
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35947
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -          dashboard at:        192.168.64.29:39277
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kmbno463
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38199
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38199
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-w4om5oqf
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33529
distributed.worker - INFO -          dashboard at:        192.168.64.29:33811
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33529
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:36573
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qbbaky_m
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3anksjy9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:43293
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7fevrvj8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-t3fljx0i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38955
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:45307
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38955
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:45307
distributed.worker - INFO -          dashboard at:        192.168.64.34:40281
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:34107
distributed.worker - INFO -          dashboard at:        192.168.64.34:34261
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:34107
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:38367
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39947
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:34647
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:34725
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:34725
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39947
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xgdr1p_8
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-t49mhuce
distributed.worker - INFO -          dashboard at:        192.168.64.34:39125
distributed.worker - INFO -          dashboard at:        192.168.64.34:33619
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:34647
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:46281
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0pb0j0bs
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:39211
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:46281
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40023
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:45827
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:34789
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40023
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:45827
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -          dashboard at:        192.168.64.34:35865
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:35381
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6yniaoio
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qff5dyqr
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6qf058lt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xrdml5kv
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ebbxanly
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39853
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39853
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.34:35005
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-x8797vx7
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ljrrr6zs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44889
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:33139
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:33139
distributed.worker - INFO -          dashboard at:        192.168.64.34:46251
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44889
distributed.worker - INFO -          dashboard at:        192.168.64.34:38805
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q9al4qlx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0bnvvwxg
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:43777
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39907
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39907
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:43777
distributed.worker - INFO -          dashboard at:        192.168.64.34:39531
distributed.worker - INFO -          dashboard at:        192.168.64.34:46593
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44131
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44131
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.34:35349
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:37173
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-93g1g6f6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:37173
distributed.worker - INFO -          dashboard at:        192.168.64.34:44183
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:35137
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-edohol1q
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-veha7zpr
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7l62u_jl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:43293
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:35137
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920327: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:13:43 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:13:47 2022
                            <40*lassen34>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:13:47 2022
Terminated at Sat Sep 17 20:14:42 2022
Results reported at Sat Sep 17 20:14:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:35137 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.62 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1233 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   55 sec.
    Turnaround time :                            59 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920328: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:13:43 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:13:47 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:13:47 2022
Terminated at Sat Sep 17 20:14:42 2022
Results reported at Sat Sep 17 20:14:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:43293 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.61 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   55 sec.
    Turnaround time :                            59 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920326: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:13:42 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:13:45 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:13:45 2022
Terminated at Sat Sep 17 20:14:42 2022
Results reported at Sat Sep 17 20:14:42 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:36041 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.54 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   57 sec.
    Turnaround time :                            60 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920325: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:13:42 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:13:45 2022
                            <40*lassen31>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:13:45 2022
Terminated at Sat Sep 17 20:14:43 2022
Results reported at Sat Sep 17 20:14:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:43293 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.80 sec.
    Max Memory :                                 58 MB
    Average Memory :                             58.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   722 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   57 sec.
    Turnaround time :                            61 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920324: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:13:42 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:13:45 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:13:45 2022
Terminated at Sat Sep 17 20:14:43 2022
Results reported at Sat Sep 17 20:14:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:35137 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.63 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   58 sec.
    Turnaround time :                            61 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920323: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:13:42 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:13:45 2022
                            <40*lassen33>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:13:45 2022
Terminated at Sat Sep 17 20:14:44 2022
Results reported at Sat Sep 17 20:14:44 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:36041 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.58 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   58 sec.
    Turnaround time :                            62 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:41217'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:36077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45739'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34101'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33741'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43519'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46001'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46113'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:40823'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39619'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:40917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:45493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:35725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43051'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37823'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:44099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44873'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:33877'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37325'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:39583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34657'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:36929'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:39773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34459'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:32883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:39009'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34201'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41547'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:33983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:33645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:39063'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:36221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:34975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:37151'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:42291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:43475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.34:34659'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34077'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41365'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:40507'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44639'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:33353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42309'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39881'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:34189'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:45559'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:35617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:40825'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:46657'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37141'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44807'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:34749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:46857'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:38273'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38971'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:40071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:35097'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:46547'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:35683'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:40149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38759'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44327'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:41071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:37797'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:45621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:38897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39537'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-qff5dyqr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xgdr1p_8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ebbxanly', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-apqoyvzr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0pb0j0bs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u59c9sic', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-x8797vx7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-r472eeia', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s0y450dk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-o8fe_9ar', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5yufsd8z', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tuar5yps', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-xoyzqt7e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-e1hp73ta', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-qypl8ovr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ljrrr6zs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-dnpotpx1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uqol73qv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f4x8dw3e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uicypzki', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2xxyz6lj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-q0pjme_x', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mx2pz2x1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-veha7zpr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-we4polw_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_ms99ixz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-e8ti53bv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-t3fljx0i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-93g1g6f6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xrdml5kv', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-100wdzqk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q9al4qlx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z4s3q2li', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7rn4nawt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-l13_k8wm', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-z5gpzkzs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8rarepnh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-txrwyvlx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z3qzzogg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-q6go33iy', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8sshskar', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qbbaky_m', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i4qmmfl3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-kmbno463', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6qf058lt', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-w4om5oqf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-t49mhuce', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-r5oc5aap', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7lv_1us8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i54d2nf8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qxi5uzsq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6yniaoio', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fxw8zbhx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0bnvvwxg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-7fevrvj8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-kn_6jcuj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3anksjy9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-edohol1q', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2uhwb38b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-hgf4gqj5', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-o73lt7v9', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9qn0od96', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7l62u_jl', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mdnoppen', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5qmvavqk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hjq4b5eu', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tz89bkse', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ayq_oxgn', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-p4mkcd5x', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-blq6xvio', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5aa418rk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5gm54e9o', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wbur6rur', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-69_o0gyu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8kj2a_3g', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rh_cf65a', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ihvukl_f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-u4z_31ef', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mkpj8l1s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ro3by81v', purging
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40439
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40439
distributed.worker - INFO -          dashboard at:        192.168.64.34:34981
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:34915
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:34915
distributed.worker - INFO -          dashboard at:        192.168.64.34:40797
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40319
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40319
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aikpc5kr
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38171
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:44607
distributed.worker - INFO -          dashboard at:        192.168.64.34:37979
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38171
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:44607
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:33043
distributed.worker - INFO -          dashboard at:        192.168.64.34:44235
distributed.worker - INFO -          dashboard at:        192.168.64.34:45869
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:33043
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-p75k_r7s
distributed.worker - INFO -          dashboard at:        192.168.64.34:42215
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:43267
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:42577
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38473
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:43267
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:42577
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38473
distributed.worker - INFO -          dashboard at:        192.168.64.34:46045
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-oa3jmcx7
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0vpod59z
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ukytpqt4
distributed.worker - INFO -          dashboard at:        192.168.64.34:45351
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:36773
distributed.worker - INFO -          dashboard at:        192.168.64.34:41575
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:36773
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-17ucxi8p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:35201
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.34:44935
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:35201
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:35601
distributed.worker - INFO -          dashboard at:        192.168.64.34:39959
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:39103
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:35601
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_7noxmnh
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:39103
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bjxh11q4
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2q6lu0rs
distributed.worker - INFO -          dashboard at:        192.168.64.34:45845
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.34:38241
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-kuqhb4ut
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2djmdjax
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-dvtm082h
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-92lgx6f3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:38837
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:38837
distributed.worker - INFO -          dashboard at:        192.168.64.34:37299
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-j8vkxko2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:33379
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:33379
distributed.worker - INFO -          dashboard at:        192.168.64.34:42179
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ijupnxlr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42989
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42989
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:34481
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:34481
distributed.worker - INFO -          dashboard at:        192.168.64.31:46217
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:39889
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -          dashboard at:        192.168.64.31:36195
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36067
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:43711
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:39889
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36067
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:43711
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:40443
distributed.worker - INFO -          dashboard at:        192.168.64.31:45615
distributed.worker - INFO -          dashboard at:        192.168.64.31:39083
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i9pr2g9t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38069
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wtc0xq39
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38069
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:33399
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.31:34753
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42637
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:37449
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0hzyv6wb
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36871
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:33399
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42637
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45741
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:37449
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36871
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nzu0rarg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:46655
distributed.worker - INFO -          dashboard at:        192.168.64.31:37849
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45741
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.31:33495
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:43273
distributed.worker - INFO -          dashboard at:        192.168.64.31:39249
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -          dashboard at:        192.168.64.31:41247
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:43273
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1gk5w3iu
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:40753
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:33369
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ojs1f5op
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:33369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:35817
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-32vrgf32
distributed.worker - INFO -          dashboard at:        192.168.64.31:34235
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-r4n1o1vf
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngo3cg5i
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-08s5j40v
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:35817
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q_6wi656
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-rp0_gk7e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:39403
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0sdsff1g
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bn9s3jvb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:44891
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:44891
distributed.worker - INFO -          dashboard at:        192.168.64.31:38709
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41031
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41031
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:44353
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i5ghu9r1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9csdbqvk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.34:40359
distributed.worker - INFO -          Listening to:  tcp://192.168.64.34:40359
distributed.worker - INFO -          dashboard at:        192.168.64.34:44509
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-jb9yjwet
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:41855
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37903
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:41855
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37903
distributed.worker - INFO -          dashboard at:        192.168.64.30:43363
distributed.worker - INFO -          dashboard at:        192.168.64.30:44139
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-rh_cf65a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-mkpj8l1s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37359
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37359
distributed.worker - INFO -          dashboard at:        192.168.64.30:38501
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44257
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36485
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5gm54e9o
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36485
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44257
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:45727
distributed.worker - INFO -          dashboard at:        192.168.64.30:46801
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wbur6rur
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-p4mkcd5x
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36689
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34719
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36689
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34719
distributed.worker - INFO -          dashboard at:        192.168.64.30:36041
distributed.worker - INFO -          dashboard at:        192.168.64.30:39641
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:32795
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:36985
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:36985
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:32795
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:35017
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:41923
distributed.worker - INFO -          dashboard at:        192.168.64.30:41619
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:35017
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-u4z_31ef
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:41349
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-blq6xvio
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tz89bkse
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5qmvavqk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8kj2a_3g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45079
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45079
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:45697
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ro3by81v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43643
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:36911
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34521
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43643
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:36911
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34521
distributed.worker - INFO -          dashboard at:        192.168.64.29:43689
distributed.worker - INFO -          dashboard at:        192.168.64.29:37777
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38267
distributed.worker - INFO -          dashboard at:        192.168.64.29:39811
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38267
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:41331
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:44545
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:42243
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41435
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33025
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:44545
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:32957
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:39849
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:42243
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38263
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33025
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41435
distributed.worker - INFO -          dashboard at:        192.168.64.29:35625
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cvxkp9n6
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:32957
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2ln1nxo8
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:39849
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:32793
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:34847
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -          dashboard at:        192.168.64.29:42047
distributed.worker - INFO -          dashboard at:        192.168.64.29:46549
distributed.worker - INFO -          dashboard at:        192.168.64.29:44833
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:39685
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ouv3ww6d
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35249
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cavt8fro
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38263
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:39279
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:34847
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:34753
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:46313
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-c3u23u7v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cu8m04ip
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-1p7nae97
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rl9h0jj4
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-67z9hlac
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lkcfj8gw
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41221
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41221
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37269
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:38513
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37269
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-62lcc8t8
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qo_vbqyq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:44579
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:45811
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:45811
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.29:41239
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-21bcnffr
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-s21pcus4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-23r33bhh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cdrpa8vi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33091
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33091
distributed.worker - INFO -          dashboard at:        192.168.64.30:43841
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:38409
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:38409
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ayq_oxgn
distributed.worker - INFO -          dashboard at:        192.168.64.30:40409
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-hjq4b5eu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33379
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33379
distributed.worker - INFO -          dashboard at:        192.168.64.30:42859
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-69_o0gyu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33649
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33649
distributed.worker - INFO -          dashboard at:        192.168.64.30:45977
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5aa418rk
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42713
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42713
distributed.worker - INFO -          dashboard at:        192.168.64.30:37933
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ihvukl_f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:32803
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:32803
distributed.worker - INFO -          dashboard at:        192.168.64.32:45045
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36373
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:45243
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40169
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36373
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:45243
distributed.worker - INFO -          dashboard at:        192.168.64.32:36569
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42235
distributed.worker - INFO -          dashboard at:        192.168.64.32:38157
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44361
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42235
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40169
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-vmqtm2d_
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44361
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37799
distributed.worker - INFO -          dashboard at:        192.168.64.32:39623
distributed.worker - INFO -          dashboard at:        192.168.64.32:37969
distributed.worker - INFO -          dashboard at:        192.168.64.32:46841
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33119
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37799
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33119
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:37451
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:38589
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cjecaw22
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43607
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-g_3xp75v
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43607
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:38661
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-whwwwb0b
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-m56ij5es
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_mn9sn9_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6gax1nls
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:43939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:43939
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xd887q7e
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0xphhsvg
distributed.worker - INFO -          dashboard at:        192.168.64.32:40035
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:45069
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:45069
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42105
distributed.worker - INFO -          dashboard at:        192.168.64.32:40011
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42105
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:36161
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5asafjt0
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-iwdtns36
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_s473n1b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:41667
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:33407
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38957
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:33407
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38957
distributed.worker - INFO -          dashboard at:        192.168.64.32:36933
distributed.worker - INFO -          dashboard at:        192.168.64.32:39621
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:41667
distributed.worker - INFO -          dashboard at:        192.168.64.32:43105
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:46225
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:46225
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:35083
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:42369
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8oi_9017
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-avul1r3r
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-w06lofbs
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4_el_wgd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:46249
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:42369
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33075
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33075
distributed.worker - INFO -          dashboard at:        192.168.64.33:36229
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:36687
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:36687
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:35661
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gplugurn
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5z1tdavo
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40473
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:43967
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40473
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:43967
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40735
distributed.worker - INFO -          dashboard at:        192.168.64.33:36141
distributed.worker - INFO -          dashboard at:        192.168.64.33:42031
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:41857
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40735
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33833
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33833
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:41857
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:36297
distributed.worker - INFO -          dashboard at:        192.168.64.33:36305
distributed.worker - INFO -          dashboard at:        192.168.64.33:33663
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:35665
distributed.worker - INFO -          dashboard at:        192.168.64.33:34203
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:36297
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:35665
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.33:33333
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:37655
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0x91qgrv
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s0jrg08c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-4xybcz0x
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wa7_ie80
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33277
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jr4onrit
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33277
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.33:35791
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ys8y3tl5
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-pe9yc1dx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:43237
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:43237
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33099
distributed.worker - INFO -          dashboard at:        192.168.64.33:46389
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33099
distributed.worker - INFO -          dashboard at:        192.168.64.33:45343
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jz0wgcpi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t6e8zck0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-igzv21uw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44847
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44323
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44847
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44323
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:38211
distributed.worker - INFO -          dashboard at:        192.168.64.33:44255
distributed.worker - INFO -          dashboard at:        192.168.64.33:37085
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:38211
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:34003
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:36499
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:34003
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -          dashboard at:        192.168.64.33:41117
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:36075
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ok3t6ynw
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jk6stvex
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ezx4c36g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t9q4afbx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:36075
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920336: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:15:16 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:15:19 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:15:19 2022
Terminated at Sat Sep 17 20:16:47 2022
Results reported at Sat Sep 17 20:16:47 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:42369 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.65 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1361 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   87 sec.
    Turnaround time :                            91 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920335: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:15:16 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:15:19 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:15:19 2022
Terminated at Sat Sep 17 20:16:47 2022
Results reported at Sat Sep 17 20:16:47 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:36075 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   1.25 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   88 sec.
    Turnaround time :                            91 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920334: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:15:16 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:15:19 2022
                            <40*lassen31>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:15:19 2022
Terminated at Sat Sep 17 20:16:47 2022
Results reported at Sat Sep 17 20:16:47 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:46249 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.67 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   88 sec.
    Turnaround time :                            91 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920333: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:15:16 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:15:19 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:15:19 2022
Terminated at Sat Sep 17 20:16:54 2022
Results reported at Sat Sep 17 20:16:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:42369 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.66 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   88 sec.
    Turnaround time :                            98 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920332: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:15:16 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:15:19 2022
                            <40*lassen33>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:15:19 2022
Terminated at Sat Sep 17 20:16:54 2022
Results reported at Sat Sep 17 20:16:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:36075 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.63 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   88 sec.
    Turnaround time :                            98 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920331: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:15:16 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:15:19 2022
                            <40*lassen34>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:15:19 2022
Terminated at Sat Sep 17 20:16:54 2022
Results reported at Sat Sep 17 20:16:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:46249 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.58 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   88 sec.
    Turnaround time :                            98 sec.

The output (if any) is above this job summary.

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33871'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:38903'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43357'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:43197'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:37241'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:39195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:35487'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:33263'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:45535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:46743'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:34867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:42667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.32:44453'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:43895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:43719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:44197'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:40399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:46125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:45407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41341'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:41291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:34203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:35639'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:35475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:40781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.31:37377'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-iwdtns36', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-62lcc8t8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-m56ij5es', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-c3u23u7v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_mn9sn9_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ouv3ww6d', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cjecaw22', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8oi_9017', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-2ln1nxo8', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lkcfj8gw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-avul1r3r', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6gax1nls', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-1p7nae97', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cavt8fro', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0xphhsvg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-rl9h0jj4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xd887q7e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-23r33bhh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qo_vbqyq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-w06lofbs', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cdrpa8vi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-4_el_wgd', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5asafjt0', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cu8m04ip', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-g_3xp75v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-whwwwb0b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-s21pcus4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-vmqtm2d_', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-_s473n1b', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-cvxkp9n6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-67z9hlac', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-21bcnffr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-92lgx6f3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-oa3jmcx7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-r4n1o1vf', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-j8vkxko2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0sdsff1g', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bn9s3jvb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ojs1f5op', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ijupnxlr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-wtc0xq39', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-nzu0rarg', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2djmdjax', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-rp0_gk7e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-08s5j40v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ukytpqt4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bjxh11q4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ngo3cg5i', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-32vrgf32', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-kuqhb4ut', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q_6wi656', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0vpod59z', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-p75k_r7s', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i5ghu9r1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-i9pr2g9t', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-_7noxmnh', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aikpc5kr', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-1gk5w3iu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-dvtm082h', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-jb9yjwet', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-17ucxi8p', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9csdbqvk', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-0hzyv6wb', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-2q6lu0rs', purging
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:43299'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:44007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:43737'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39329'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:46211'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:42711'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:34839'
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:34485'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:40383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:46711'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:36167'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:34805'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:39405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.33:33705'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:43317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:45827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44399'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:44723'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38289'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:41839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:41103'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:46769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:36917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:37875'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:40157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:38693'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.28:42155'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37541'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37663'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42427'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39025'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39191'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:43657'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37507'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44671'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:41101'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42109'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:44415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37229'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:39823'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:34593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:37883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.29:42721'
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ezx4c36g', purging
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38505
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38505
distributed.worker - INFO -          dashboard at:        192.168.64.32:43797
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42995
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42995
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:43615
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36615
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36615
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:41651
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-smjevhcx
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:37725
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8x_j9j3m
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:37725
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:42537
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:39367
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:36613
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:42537
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-tuk83r3x
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:36613
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:35569
distributed.worker - INFO -          dashboard at:        192.168.64.32:34993
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:36707
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:35569
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:40275
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -          dashboard at:        192.168.64.32:37245
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:40275
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38629
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:46353
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yyl7hauj
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38629
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.32:42341
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ixxxounq
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bnfnqhp6
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xqnc363y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-97se24in
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bsds7z4f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39553
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39553
distributed.worker - INFO -          dashboard at:        192.168.64.32:46137
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:38143
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:32859
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:32859
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:38143
distributed.worker - INFO -          dashboard at:        192.168.64.32:34871
distributed.worker - INFO -          dashboard at:        192.168.64.32:43855
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39161
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39161
distributed.worker - INFO -          dashboard at:        192.168.64.32:43783
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-k8toje1k
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-muhgeplc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-va36zeeq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-g4r_navh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:44763
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:44763
distributed.worker - INFO -          dashboard at:        192.168.64.32:41621
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-213eujve
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-pe9yc1dx', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:39111
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:39111
distributed.worker - INFO -          dashboard at:        192.168.64.32:40487
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.32:34399
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0iter0_x
distributed.worker - INFO -          Listening to:  tcp://192.168.64.32:34399
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.32:34697
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-jpr63urc
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t9q4afbx', purging
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45253
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45253
distributed.worker - INFO -          dashboard at:        192.168.64.31:32861
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42399
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42399
distributed.worker - INFO -          dashboard at:        192.168.64.31:35507
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42385
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42385
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:45457
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:40257
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:36497
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:46557
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:40257
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-27ra4hfo
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:32801
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:36497
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:46557
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:32801
distributed.worker - INFO -          dashboard at:        192.168.64.31:34635
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45137
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:45171
distributed.worker - INFO -          dashboard at:        192.168.64.31:33417
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-38s978jj
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45137
distributed.worker - INFO -          dashboard at:        192.168.64.31:38993
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:45171
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -          dashboard at:        192.168.64.31:44945
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aesqbhk2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8mllf6d6
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z6e2s4cw
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6_dtgmis
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-m1vhbi2w
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:38547
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:38547
distributed.worker - INFO -          dashboard at:        192.168.64.31:46637
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:46039
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:46039
distributed.worker - INFO -          dashboard at:        192.168.64.31:35793
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -          dashboard at:        192.168.64.31:44541
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:39963
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:39963
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:37937
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-3qtv2dvw
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9g7grfnu
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41045
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.31:44585
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41045
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -          dashboard at:        192.168.64.31:35413
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mnvnohin
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:41313
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:41313
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.31:33155
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-m1ze4ucp
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u26zxlx7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-k03io07i
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xwdqol7e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:33265
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:33265
distributed.worker - INFO -          dashboard at:        192.168.64.31:40775
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.31:42313
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.31:42313
distributed.worker - INFO -          dashboard at:        192.168.64.31:33283
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-dchp7jda
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bxctuk6v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-0x91qgrv', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-gplugurn', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-t6e8zck0', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-xqnc363y', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jk6stvex', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bnfnqhp6', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ys8y3tl5', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-97se24in', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-tuk83r3x', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-k8toje1k', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ok3t6ynw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-ixxxounq', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-g4r_navh', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-muhgeplc', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jz0wgcpi', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-jpr63urc', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-213eujve', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-8x_j9j3m', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-va36zeeq', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jr4onrit', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bsds7z4f', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-0iter0_x', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-yyl7hauj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-smjevhcx', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-wa7_ie80', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-4xybcz0x', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-5z1tdavo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-s0jrg08c', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-igzv21uw', purging
/usr/tce/packages/python/python-3.7.2/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/lib/python3.7/site-packages/distributed/cli/dask_worker.py:318: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  FutureWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36271'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:34505'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43647'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:37071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:46737'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:41803'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:44549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:45593'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:39015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:35863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:36989'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:42401'
distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.64.30:43995'
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45125
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45125
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36275
distributed.worker - INFO -          dashboard at:        192.168.64.28:45633
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36275
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          dashboard at:        192.168.64.28:32889
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35819
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35819
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:41227
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43661
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:35487
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43781
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:35487
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43661
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:40609
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-20fb54bm
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43781
distributed.worker - INFO -          dashboard at:        192.168.64.28:33855
distributed.worker - INFO -          dashboard at:        192.168.64.28:43175
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:40609
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.28:43249
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-uy73d71q
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:37109
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:36947
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:41215
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:37145
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:36947
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-iwysw4xn
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:41215
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:37145
distributed.worker - INFO -          dashboard at:        192.168.64.28:34981
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:45103
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:34583
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          dashboard at:        192.168.64.28:35813
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:34583
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-1z1_8wuw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kmo7q2ma
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-b7xpqtuj
distributed.worker - INFO -          dashboard at:        192.168.64.28:45853
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-b8wkj94x
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:45761
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:45761
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.28:36175
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:38355
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ojjy22wp
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-x8bxzzj5
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:38355
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ocbw7tgv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kjqn0ohw
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.28:45079
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2ssid8wr
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ithjqlgn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:41335
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:43315
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:41335
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:43315
distributed.worker - INFO -          dashboard at:        192.168.64.28:43263
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          dashboard at:        192.168.64.28:39115
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-dpx4wtuf
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-7jzfgmcr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.28:42639
distributed.worker - INFO -          Listening to:  tcp://192.168.64.28:42639
distributed.worker - INFO -          dashboard at:        192.168.64.28:45293
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-3xevy44c
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-z6e2s4cw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u26zxlx7', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-9g7grfnu', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-bxctuk6v', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-27ra4hfo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-38s978jj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-aesqbhk2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-3qtv2dvw', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-dchp7jda', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xwdqol7e', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6_dtgmis', purging
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46045
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35797
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35797
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46045
distributed.worker - INFO -          dashboard at:        192.168.64.29:43553
distributed.worker - INFO -          dashboard at:        192.168.64.29:40097
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:41969
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:41969
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-k03io07i', purging
distributed.worker - INFO -          dashboard at:        192.168.64.29:41593
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-fi2injvb
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:46503
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:32833
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38121
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:32833
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38121
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:46503
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:33025
distributed.worker - INFO -          dashboard at:        192.168.64.29:46523
distributed.worker - INFO -          dashboard at:        192.168.64.29:37763
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37075
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:37001
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-3og9y4s5
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37075
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:37001
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.29:45413
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40515
distributed.worker - INFO -          dashboard at:        192.168.64.29:37899
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40515
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:39323
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-sexuow6m
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-6bvb62a9
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-mn2ao381
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-gpxk17ln
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35587
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-77mtkqrp
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-qzydf1e_
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-mvqfhncc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:33401
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43265
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35587
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:33401
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43265
distributed.worker - INFO -          dashboard at:        192.168.64.29:41041
distributed.worker - INFO -          dashboard at:        192.168.64.29:44577
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:43199
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:43199
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.29:34885
distributed.worker - INFO -          dashboard at:        192.168.64.29:34541
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-snl5qfhw
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-lm2dbqsf
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bsx12480
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:35413
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:35413
distributed.worker - INFO -          dashboard at:        192.168.64.29:42145
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-eytwecwj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:35009
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:35009
distributed.worker - INFO -          dashboard at:        192.168.64.33:35377
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-5ajsuv43
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33623
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33255
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33255
distributed.worker - INFO -          dashboard at:        192.168.64.33:33577
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33623
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:37981
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          dashboard at:        192.168.64.33:34655
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:37981
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-ylmtcem7
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          dashboard at:        192.168.64.33:38693
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:34837
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:34837
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:39847
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40701
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-kb1ms2wj
distributed.worker - INFO -          dashboard at:        192.168.64.33:42271
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:33475
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:39847
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-2zuflern
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:33475
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:39933
distributed.worker - INFO -          dashboard at:        192.168.64.33:35381
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-tkg5xbih
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:36797
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40701
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:38917
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:36797
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44969
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:38917
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:45613
distributed.worker - INFO -          dashboard at:        192.168.64.33:38657
distributed.worker - INFO -          dashboard at:        192.168.64.33:36023
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:43975
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44969
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:44327
distributed.worker - INFO -          dashboard at:        192.168.64.33:46649
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:43975
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.33:45671
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:45613
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:36159
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:44327
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-sfc5ndnm
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          dashboard at:        192.168.64.33:36095
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -          dashboard at:        192.168.64.33:45019
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-f1z4wbx7
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-y18ds9a2
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-oob8mir1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-msk152y5
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:40141
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-vsu7z_pb
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:40141
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-9nnv_dov
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-bdc3l4l8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.33:41865
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-8vfhsds_
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.33:43597
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.33:43597
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.33:42173
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-jhew1cax
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-_u1s7lqx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/tmid/dask-worker-space/worker-39g6umdk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:38587
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:38587
distributed.worker - INFO -          dashboard at:        192.168.64.29:46277
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-bi0b_96c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.29:40965
distributed.worker - INFO -          Listening to:  tcp://192.168.64.29:40965
distributed.worker - INFO -          dashboard at:        192.168.64.29:39325
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/file_id/dask-worker-space/worker-tgngdhoy
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-m1vhbi2w', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-8mllf6d6', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mnvnohin', purging
distributed.diskutils - INFO - Found stale lock file and directory '/p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-m1ze4ucp', purging
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:40347
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:37359
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44437
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44437
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39537
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39537
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:37215
distributed.worker - INFO -          dashboard at:        192.168.64.30:39195
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:37215
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:40597
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39073
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:42945
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39073
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:42945
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:34745
distributed.worker - INFO -          dashboard at:        192.168.64.30:40363
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:46015
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:34745
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:33613
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-5ed18ejh
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:32995
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:33613
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-q1qbs060
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -          dashboard at:        192.168.64.30:39865
distributed.worker - INFO -          dashboard at:        192.168.64.30:39665
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43567
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43567
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-g7yjcj2h
distributed.worker - INFO -          dashboard at:        192.168.64.30:39037
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-busoklxg
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-mi7igkvi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-xyaid56q
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-ez99wtsd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-obc5q0ey
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44967
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44967
distributed.worker - INFO -          dashboard at:        192.168.64.30:41315
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45341
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45341
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:        192.168.64.30:38659
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-rim6z5xg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-u8o5qby8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:40437
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:40437
distributed.worker - INFO -          dashboard at:        192.168.64.30:38487
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:44963
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:44963
distributed.worker - INFO -          dashboard at:        192.168.64.30:46277
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:43731
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:43731
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -          dashboard at:        192.168.64.30:45723
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45007
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:45511
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-6htyioli
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45007
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:45511
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:43781
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        192.168.64.30:35609
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-7archvxv
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-j8d6_45w
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-92wrx4hv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://192.168.64.30:39621
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-unc0oc2s
distributed.worker - INFO -          Listening to:  tcp://192.168.64.30:39621
distributed.worker - INFO -          dashboard at:        192.168.64.30:36519
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                  93.13 GiB
distributed.worker - INFO -       Local Directory: /p/gpfs1/izzet/temp/proc_id/dask-worker-space/worker-yzt9kwod
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to: tcp://192.168.64.232:38099
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 79.69 MiB from 204 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 121.69 MiB from 398 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.

------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920358: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:21:04 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:21:09 2022
                            <40*lassen28>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:21:09 2022
Terminated at Sat Sep 17 20:24:27 2022
Results reported at Sat Sep 17 20:24:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:40347 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.63 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1361 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   192 sec.
    Turnaround time :                            203 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920357: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:21:04 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:21:09 2022
                            <40*lassen29>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:21:09 2022
Terminated at Sat Sep 17 20:24:27 2022
Results reported at Sat Sep 17 20:24:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37359 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.81 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   198 sec.
    Turnaround time :                            203 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920356: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:21:04 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:21:09 2022
                            <40*lassen30>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:21:09 2022
Terminated at Sat Sep 17 20:24:27 2022
Results reported at Sat Sep 17 20:24:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:38099 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-0 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.60 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   198 sec.
    Turnaround time :                            203 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920354: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:21:04 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:21:06 2022
                            <40*lassen32>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:21:06 2022
Terminated at Sat Sep 17 20:24:27 2022
Results reported at Sat Sep 17 20:24:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:37359 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/file_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.60 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   200 sec.
    Turnaround time :                            203 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920353: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:21:04 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:21:06 2022
                            <40*lassen33>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:21:06 2022
Terminated at Sat Sep 17 20:24:27 2022
Results reported at Sat Sep 17 20:24:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:40347 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/tmid

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.68 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   200 sec.
    Turnaround time :                            203 sec.

The output (if any) is above this job summary.


------------------------------------------------------------
Sender: LSF System <lsfadmin@lassen710>
Subject: Job 3920355: <dask-worker> in cluster <lassen> Exited

Job <dask-worker> was submitted from host <lassen232> by user <izzet> in cluster <lassen> at Sat Sep 17 20:21:04 2022
Job was executed on host(s) <1*lassen710>, in queue <pdebug>, as user <izzet> in cluster <lassen> at Sat Sep 17 20:21:06 2022
                            <40*lassen31>
</g/g92/izzet> was used as the home directory.
</usr/workspace/iopp/projects/vani-analysis-tool/vani> was used as the working directory.
Started at Sat Sep 17 20:21:06 2022
Terminated at Sat Sep 17 20:24:27 2022
Results reported at Sat Sep 17 20:24:27 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/usr/bin/env bash

#BSUB -J dask-worker
#BSUB -G asccasc
#BSUB -q pdebug
#BSUB -W 120
#BSUB -o digio.worker.log
#BSUB -e digio.worker.log

/usr/workspace/iopp/projects/vani-analysis-tool/venv-lassen/bin/python -m distributed.cli.dask_worker tcp://192.168.64.232:38099 --nthreads 1 --nprocs 16 --memory-limit 93.13GiB --name LSFCluster-1 --nanny --death-timeout 7200 --local-directory /p/gpfs1/izzet/temp/proc_id

------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 255.

Resource usage summary:

    CPU time :                                   0.69 sec.
    Max Memory :                                 59 MB
    Average Memory :                             59.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   1425 MB
    Max Processes :                              4
    Max Threads :                                27
    Run time :                                   201 sec.
    Turnaround time :                            203 sec.

The output (if any) is above this job summary.

